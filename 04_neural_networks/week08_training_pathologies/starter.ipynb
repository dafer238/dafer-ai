{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a89a799",
   "metadata": {},
   "source": [
    "# Week 08 — Training Pathologies\n",
    "\n",
    "This notebook explores common failure modes and their fixes. You'll:\n",
    "- Diagnose vanishing/exploding gradients\n",
    "- Compare activation functions and their effects\n",
    "- Apply fixes: initialization, normalization, gradient clipping\n",
    "- Track and visualize gradient flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0063ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "print(\"Libraries imported!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7eb9b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, pickle\n",
    "\n",
    "CACHE_DIR = \"cache_week08\"\n",
    "os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "\n",
    "def save_result(key, obj):\n",
    "    with open(os.path.join(CACHE_DIR, f\"{key}.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(obj, f)\n",
    "\n",
    "def load_result(key):\n",
    "    path = os.path.join(CACHE_DIR, f\"{key}.pkl\")\n",
    "    if os.path.exists(path):\n",
    "        with open(path, \"rb\") as f:\n",
    "            return pickle.load(f)\n",
    "    return None\n",
    "\n",
    "def cached(key, compute_fn):\n",
    "    result = load_result(key)\n",
    "    if result is not None:\n",
    "        print(f\"[cache] loaded '{key}'\")\n",
    "        return result\n",
    "    print(f\"[cache] computing '{key}'...\")\n",
    "    result = compute_fn()\n",
    "    save_result(key, result)\n",
    "    return result\n",
    "\n",
    "print(\"Cache utilities ready. Results will be stored in:\", CACHE_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eff6978",
   "metadata": {},
   "source": [
    "## 1. Track Gradient Norms Across Layers\n",
    "\n",
    "Build a deep network and monitor gradient magnitudes to detect vanishing/exploding gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3600fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class DeepNet(nn.Module):\n",
    "    def __init__(self, input_size=10, hidden_size=50, n_layers=10, activation='sigmoid'):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(nn.Linear(input_size, hidden_size))\n",
    "        for _ in range(n_layers - 2):\n",
    "            self.layers.append(nn.Linear(hidden_size, hidden_size))\n",
    "        self.layers.append(nn.Linear(hidden_size, 1))\n",
    "        if activation == 'sigmoid':   self.activation = nn.Sigmoid()\n",
    "        elif activation == 'tanh':    self.activation = nn.Tanh()\n",
    "        elif activation == 'relu':    self.activation = nn.ReLU()\n",
    "        else: raise ValueError(f\"Unknown activation: {activation}\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers[:-1]:\n",
    "            x = self.activation(layer(x))\n",
    "        return self.layers[-1](x)\n",
    "\n",
    "def compute_gradient_norms(model):\n",
    "    return [layer.weight.grad.norm().item()\n",
    "            if hasattr(layer, 'weight') and layer.weight.grad is not None else 0.0\n",
    "            for layer in model.layers]\n",
    "\n",
    "X = torch.randn(100, 10)\n",
    "y = torch.randn(100, 1)\n",
    "\n",
    "def _gradient_norm_analysis():\n",
    "    norms_dict = {}\n",
    "    for act in ['sigmoid', 'tanh', 'relu']:\n",
    "        model = DeepNet(input_size=10, hidden_size=50, n_layers=10, activation=act)\n",
    "        outputs = model(X)\n",
    "        loss = nn.MSELoss()(outputs, y)\n",
    "        loss.backward()\n",
    "        norms_dict[act] = compute_gradient_norms(model)\n",
    "        print(f\"{act:8s}: {norms_dict[act][:5]} ...\")\n",
    "    return norms_dict\n",
    "\n",
    "gradient_norms = cached(\"gradient_norm_analysis_10layers\", _gradient_norm_analysis)\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "for act, norms in gradient_norms.items():\n",
    "    plt.plot(range(len(norms)), norms, 'o-', label=act, linewidth=2, markersize=6)\n",
    "plt.xlabel('Layer Index'); plt.ylabel('Gradient L2 Norm')\n",
    "plt.title('Gradient Norms Across Layers (10-layer deep network)')\n",
    "plt.legend(); plt.grid(alpha=0.3); plt.yscale('log'); plt.show()\n",
    "print(\"\\n→ Sigmoid/Tanh show vanishing gradients in early layers!\")\n",
    "print(\"→ ReLU maintains better gradient flow.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6471f4c8",
   "metadata": {},
   "source": [
    "## 2. Activation Function Comparison\n",
    "\n",
    "Train networks with different activations and compare learning dynamics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab9fc53",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_model(model, X, y, n_epochs=200, lr=0.01):\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "    criterion = nn.MSELoss()\n",
    "    losses = []\n",
    "    for _ in range(n_epochs):\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(model(X), y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "    return losses\n",
    "\n",
    "torch.manual_seed(42)\n",
    "X_train = torch.randn(500, 10)\n",
    "y_train = torch.randn(500, 1)\n",
    "\n",
    "def _activation_comparison():\n",
    "    results = {}\n",
    "    for act in ['sigmoid', 'tanh', 'relu']:\n",
    "        print(f\"  Training {act}...\")\n",
    "        model = DeepNet(input_size=10, hidden_size=50, n_layers=6, activation=act)\n",
    "        losses = train_model(model, X_train, y_train, n_epochs=200, lr=0.01)\n",
    "        results[act] = losses\n",
    "        print(f\"    Final loss: {losses[-1]:.6f}\")\n",
    "    return results\n",
    "\n",
    "activation_results = cached(\"activation_comparison_200epochs\", _activation_comparison)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "for act, losses in activation_results.items():\n",
    "    plt.plot(losses, label=act, linewidth=2)\n",
    "plt.xlabel('Epoch'); plt.ylabel('Loss')\n",
    "plt.title('Training Dynamics: Activation Function Comparison')\n",
    "plt.legend(); plt.grid(alpha=0.3); plt.yscale('log'); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12d4672",
   "metadata": {},
   "source": [
    "## 3. Initialization Strategies\n",
    "\n",
    "Compare Xavier/Glorot and He initialization to solve gradient problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6f9c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def initialize_model(model, init_type='xavier'):\n",
    "    for layer in model.layers:\n",
    "        if hasattr(layer, 'weight'):\n",
    "            if init_type == 'xavier': nn.init.xavier_uniform_(layer.weight)\n",
    "            elif init_type == 'he':   nn.init.kaiming_uniform_(layer.weight, nonlinearity='relu')\n",
    "            elif init_type == 'small': nn.init.normal_(layer.weight, mean=0.0, std=0.01)\n",
    "            if hasattr(layer, 'bias') and layer.bias is not None:\n",
    "                nn.init.zeros_(layer.bias)\n",
    "\n",
    "def _init_comparison():\n",
    "    results = {}\n",
    "    for init_type in ['small', 'xavier', 'he']:\n",
    "        print(f\"  Training with {init_type} init...\")\n",
    "        model = DeepNet(input_size=10, hidden_size=50, n_layers=8, activation='relu')\n",
    "        initialize_model(model, init_type)\n",
    "        losses = train_model(model, X_train, y_train, n_epochs=200, lr=0.01)\n",
    "        results[init_type] = losses\n",
    "        print(f\"    Final loss: {losses[-1]:.6f}\")\n",
    "    return results\n",
    "\n",
    "init_results = cached(\"init_comparison_relu_200epochs\", _init_comparison)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "for init_type, losses in init_results.items():\n",
    "    plt.plot(losses, label=init_type, linewidth=2)\n",
    "plt.xlabel('Epoch'); plt.ylabel('Loss')\n",
    "plt.title('Initialization Comparison (ReLU activation)')\n",
    "plt.legend(); plt.grid(alpha=0.3); plt.yscale('log'); plt.show()\n",
    "print(\"\\n→ He initialization works best with ReLU!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a6f09c",
   "metadata": {},
   "source": [
    "## 4. Batch Normalization\n",
    "\n",
    "Add BatchNorm to stabilize training and improve gradient flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cefd22e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DeepNetWithBatchNorm(nn.Module):\n",
    "    def __init__(self, input_size=10, hidden_size=50, n_layers=10):\n",
    "        super().__init__()\n",
    "        self.layers     = nn.ModuleList()\n",
    "        self.batch_norms = nn.ModuleList()\n",
    "        self.layers.append(nn.Linear(input_size, hidden_size))\n",
    "        self.batch_norms.append(nn.BatchNorm1d(hidden_size))\n",
    "        for _ in range(n_layers - 2):\n",
    "            self.layers.append(nn.Linear(hidden_size, hidden_size))\n",
    "            self.batch_norms.append(nn.BatchNorm1d(hidden_size))\n",
    "        self.layers.append(nn.Linear(hidden_size, 1))\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i in range(len(self.layers) - 1):\n",
    "            x = self.activation(self.batch_norms[i](self.layers[i](x)))\n",
    "        return self.layers[-1](x)\n",
    "\n",
    "def _batchnorm_comparison():\n",
    "    print(\"  Training WITHOUT BatchNorm...\")\n",
    "    losses_no_bn = train_model(DeepNet(10, 50, 10, 'relu'), X_train, y_train, 200, 0.01)\n",
    "    print(f\"    Final loss: {losses_no_bn[-1]:.6f}\")\n",
    "    print(\"  Training WITH BatchNorm...\")\n",
    "    losses_bn = train_model(DeepNetWithBatchNorm(10, 50, 10), X_train, y_train, 200, 0.01)\n",
    "    print(f\"    Final loss: {losses_bn[-1]:.6f}\")\n",
    "    return losses_no_bn, losses_bn\n",
    "\n",
    "losses_no_bn, losses_bn = cached(\"batchnorm_comparison_200epochs\", _batchnorm_comparison)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(losses_no_bn, label='Without BatchNorm', linewidth=2)\n",
    "plt.plot(losses_bn,    label='With BatchNorm',    linewidth=2)\n",
    "plt.xlabel('Epoch'); plt.ylabel('Loss')\n",
    "plt.title('Effect of Batch Normalization (10-layer network)')\n",
    "plt.legend(); plt.grid(alpha=0.3); plt.yscale('log'); plt.show()\n",
    "print(f\"Final loss WITHOUT BatchNorm: {losses_no_bn[-1]:.6f}\")\n",
    "print(f\"Final loss WITH BatchNorm:    {losses_bn[-1]:.6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ea58e1",
   "metadata": {},
   "source": [
    "## 5. Gradient Clipping\n",
    "\n",
    "Use gradient clipping to prevent exploding gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e99b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_with_clipping(model, X, y, n_epochs=100, lr=0.01, clip_value=None):\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "    criterion = nn.MSELoss()\n",
    "    losses, grad_norms = [], []\n",
    "    for _ in range(n_epochs):\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(model(X), y)\n",
    "        loss.backward()\n",
    "        total_norm = sum(p.grad.data.norm(2).item() ** 2 for p in model.parameters() if p.grad is not None) ** 0.5\n",
    "        grad_norms.append(total_norm)\n",
    "        if clip_value is not None:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip_value)\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "    return losses, grad_norms\n",
    "\n",
    "def _gradclip_comparison():\n",
    "    print(\"  Training WITHOUT gradient clipping...\")\n",
    "    l_nc, g_nc = train_with_clipping(DeepNet(10, 50, 8, 'relu'), X_train, y_train, 100, 0.1, None)\n",
    "    print(\"  Training WITH gradient clipping...\")\n",
    "    l_c,  g_c  = train_with_clipping(DeepNet(10, 50, 8, 'relu'), X_train, y_train, 100, 0.1, 1.0)\n",
    "    return l_nc, g_nc, l_c, g_c\n",
    "\n",
    "losses_no_clip, grads_no_clip, losses_clip, grads_clip = cached(\n",
    "    \"gradclip_comparison_100epochs\", _gradclip_comparison)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "axes[0].plot(losses_no_clip, label='No clipping',              linewidth=2)\n",
    "axes[0].plot(losses_clip,    label='With clipping (max=1.0)', linewidth=2)\n",
    "axes[0].set(xlabel='Epoch', ylabel='Loss', title='Training Loss')\n",
    "axes[0].legend(); axes[0].grid(alpha=0.3); axes[0].set_yscale('log')\n",
    "axes[1].plot(grads_no_clip, label='No clipping',              linewidth=2)\n",
    "axes[1].plot(grads_clip,    label='With clipping (max=1.0)', linewidth=2)\n",
    "axes[1].set(xlabel='Epoch', ylabel='Gradient Norm', title='Gradient Magnitude')\n",
    "axes[1].legend(); axes[1].grid(alpha=0.3); axes[1].set_yscale('log')\n",
    "plt.tight_layout(); plt.show()\n",
    "print(\"\\n→ Gradient clipping prevents explosions and stabilizes training!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c3704a",
   "metadata": {},
   "source": [
    "## Exercises for Further Practice\n",
    "\n",
    "1. **Layer Normalization**: Implement and compare LayerNorm to BatchNorm\n",
    "2. **Residual Connections**: Add skip connections to improve gradient flow\n",
    "3. **Different Depths**: Test very deep networks (20+ layers) with and without fixes\n",
    "4. **Learning Rate Analysis**: Study how LR interacts with initialization and normalization\n",
    "5. **Real Data**: Apply these techniques to MNIST or CIFAR-10\n",
    "\n",
    "## Deliverables Checklist\n",
    "\n",
    "- [ ] Gradient norm tracking across layers with visualizations\n",
    "- [ ] Activation function comparison experiments\n",
    "- [ ] Initialization and BatchNorm experiments\n",
    "- [ ] Gradient clipping demonstration\n",
    "- [ ] Short write-up on which fixes work best for which pathologies\n",
    "\n",
    "## Recommended Resources\n",
    "\n",
    "- Glorot & Bengio (2010): \"Understanding the difficulty of training deep feedforward neural networks\"\n",
    "- Ioffe & Szegedy (2015): \"Batch Normalization\"\n",
    "- He et al. (2015): \"Delving Deep into Rectifiers\" (He initialization)\n",
    "- PyTorch documentation on initialization and normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc264f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# EXERCISE 1 — Layer Normalization vs Batch Normalization\n",
    "# Goal: implement a DeepNetWithLayerNorm class using nn.LayerNorm instead of nn.BatchNorm1d.\n",
    "# Compare training loss curves: BatchNorm vs LayerNorm vs no normalization.\n",
    "# Use cached(\"layernorm_vs_batchnorm_200epochs\", ...) for the experiment.\n",
    "# Expected insight: LayerNorm works on batch size 1, BatchNorm needs a batch of size > 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1a9a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# EXERCISE 2 — Residual Connections\n",
    "# Goal: implement DeepNetResidual where each hidden layer has a skip connection:\n",
    "# output = activation(layer(x)) + x (requires equal input/output size).\n",
    "# Compare gradient norms across layers to the baseline DeepNet.\n",
    "# Use cached(\"residual_gradient_norms_10layers\", ...) to save the norms.\n",
    "# Expected insight: skip connections prevent vanishing gradients by providing a\n",
    "# gradient \"highway\" directly to early layers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417f929c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# EXERCISE 4 — LR × Initialization Interaction\n",
    "# Goal: create a 3×3 grid: init ∈ {small, xavier, he} × lr ∈ {0.001, 0.01, 0.1}.\n",
    "# For each combo, train for 200 epochs and record final loss.\n",
    "# Display final losses as a heatmap using plt.imshow or seaborn.\n",
    "# Use cached(f\"lr_init_grid_{init}_{lr}\", ...) per cell.\n",
    "# Expected insight: some (init, lr) pairs diverge or plateau — the interaction matters.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
