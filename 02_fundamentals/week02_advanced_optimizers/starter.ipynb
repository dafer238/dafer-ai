{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c414c8cb",
   "metadata": {},
   "source": [
    "# Week 02 — Advanced Optimization\n",
    "\n",
    "This notebook explores advanced optimizers (Momentum, RMSProp, Adam) and learning rate schedules. You'll:\n",
    "- Implement and compare modern optimizers\n",
    "- Experiment with LR schedules (step, cosine, warmup)\n",
    "- Run LR range tests and hyperparameter sweeps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8ebbfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(42)\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df0c30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, pickle\n",
    "\n",
    "CACHE_DIR = \"cache_week02\"\n",
    "os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "\n",
    "def save_result(key, obj):\n",
    "    with open(os.path.join(CACHE_DIR, f\"{key}.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(obj, f)\n",
    "\n",
    "def load_result(key):\n",
    "    path = os.path.join(CACHE_DIR, f\"{key}.pkl\")\n",
    "    if os.path.exists(path):\n",
    "        with open(path, \"rb\") as f:\n",
    "            return pickle.load(f)\n",
    "    return None\n",
    "\n",
    "def cached(key, compute_fn):\n",
    "    result = load_result(key)\n",
    "    if result is not None:\n",
    "        print(f\"[cache] loaded '{key}'\")\n",
    "        return result\n",
    "    print(f\"[cache] computing '{key}'...\")\n",
    "    result = compute_fn()\n",
    "    save_result(key, result)\n",
    "    return result\n",
    "\n",
    "print(\"Cache utilities ready. Results will be stored in:\", CACHE_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae960f3",
   "metadata": {},
   "source": [
    "## 1. Implement Advanced Optimizers\n",
    "\n",
    "We'll implement Momentum, RMSProp, and Adam from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0784257f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer:\n",
    "    \"\"\"Base optimizer class\"\"\"\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "    \n",
    "    def step(self, params, grads):\n",
    "        raise NotImplementedError\n",
    "\n",
    "class MomentumOptimizer(Optimizer):\n",
    "    \"\"\"SGD with Momentum\"\"\"\n",
    "    def __init__(self, lr=0.01, momentum=0.9):\n",
    "        super().__init__(lr)\n",
    "        self.momentum = momentum\n",
    "        self.velocity = None\n",
    "    \n",
    "    def step(self, params, grads):\n",
    "        if self.velocity is None:\n",
    "            self.velocity = np.zeros_like(params)\n",
    "        \n",
    "        self.velocity = self.momentum * self.velocity - self.lr * grads\n",
    "        params += self.velocity\n",
    "        return params\n",
    "\n",
    "class RMSPropOptimizer(Optimizer):\n",
    "    \"\"\"RMSProp optimizer\"\"\"\n",
    "    def __init__(self, lr=0.01, beta=0.9, eps=1e-8):\n",
    "        super().__init__(lr)\n",
    "        self.beta = beta\n",
    "        self.eps = eps\n",
    "        self.cache = None\n",
    "    \n",
    "    def step(self, params, grads):\n",
    "        if self.cache is None:\n",
    "            self.cache = np.zeros_like(params)\n",
    "        \n",
    "        self.cache = self.beta * self.cache + (1 - self.beta) * (grads ** 2)\n",
    "        params -= self.lr * grads / (np.sqrt(self.cache) + self.eps)\n",
    "        return params\n",
    "\n",
    "class AdamOptimizer(Optimizer):\n",
    "    \"\"\"Adam optimizer with bias correction\"\"\"\n",
    "    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999, eps=1e-8):\n",
    "        super().__init__(lr)\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.eps = eps\n",
    "        self.m = None\n",
    "        self.v = None\n",
    "        self.t = 0\n",
    "    \n",
    "    def step(self, params, grads):\n",
    "        if self.m is None:\n",
    "            self.m = np.zeros_like(params)\n",
    "            self.v = np.zeros_like(params)\n",
    "        \n",
    "        self.t += 1\n",
    "        \n",
    "        # Update biased first and second moments\n",
    "        self.m = self.beta1 * self.m + (1 - self.beta1) * grads\n",
    "        self.v = self.beta2 * self.v + (1 - self.beta2) * (grads ** 2)\n",
    "        \n",
    "        # Bias correction\n",
    "        m_hat = self.m / (1 - self.beta1 ** self.t)\n",
    "        v_hat = self.v / (1 - self.beta2 ** self.t)\n",
    "        \n",
    "        # Update parameters\n",
    "        params -= self.lr * m_hat / (np.sqrt(v_hat) + self.eps)\n",
    "        return params\n",
    "\n",
    "print(\"Optimizers implemented: Momentum, RMSProp, Adam\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578ff802",
   "metadata": {},
   "source": [
    "## 2. Compare Optimizers on a Test Problem\n",
    "\n",
    "Let's compare all optimizers on a simple 2D quadratic loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c52dcee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define a simple 2D quadratic loss function\n",
    "def loss_fn(params):\n",
    "    x, y = params\n",
    "    return x**2 + 5*y**2\n",
    "\n",
    "def grad_fn(params):\n",
    "    x, y = params\n",
    "    return np.array([2*x, 10*y])\n",
    "\n",
    "n_steps = 100\n",
    "initial_params = np.array([3.0, -2.0])\n",
    "\n",
    "def _run_optimizer(name, opt):\n",
    "    params = initial_params.copy()\n",
    "    trajectory = [params.copy()]\n",
    "    for _ in range(n_steps):\n",
    "        grads = grad_fn(params)\n",
    "        params = opt.step(params, grads)\n",
    "        trajectory.append(params.copy())\n",
    "    return np.array(trajectory)\n",
    "\n",
    "optimizer_configs = {\n",
    "    'Momentum': lambda: MomentumOptimizer(lr=0.1, momentum=0.9),\n",
    "    'RMSProp': lambda: RMSPropOptimizer(lr=0.1, beta=0.9),\n",
    "    'Adam':    lambda: AdamOptimizer(lr=0.1, beta1=0.9, beta2=0.999),\n",
    "}\n",
    "\n",
    "trajectories = {\n",
    "    name: cached(f\"optimizer_{name.lower()}_{n_steps}steps\",\n",
    "                 lambda n=name, c=cfg: _run_optimizer(n, c()))\n",
    "    for name, cfg in optimizer_configs.items()\n",
    "}\n",
    "\n",
    "# Plot trajectories\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "x_range = np.linspace(-4, 4, 100)\n",
    "y_range = np.linspace(-3, 3, 100)\n",
    "X, Y = np.meshgrid(x_range, y_range)\n",
    "Z = X**2 + 5*Y**2\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.contour(X, Y, Z, levels=20, cmap='viridis', alpha=0.5)\n",
    "for name, traj in trajectories.items():\n",
    "    plt.plot(traj[:, 0], traj[:, 1], '-o', markersize=2, label=name, alpha=0.7)\n",
    "plt.plot(initial_params[0], initial_params[1], 'ko', markersize=10, label='Start')\n",
    "plt.xlabel('x'); plt.ylabel('y')\n",
    "plt.title('Optimizer Trajectories'); plt.legend(); plt.grid(alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "for name, traj in trajectories.items():\n",
    "    losses = [loss_fn(p) for p in traj]\n",
    "    plt.semilogy(losses, label=name, linewidth=2)\n",
    "plt.xlabel('Step'); plt.ylabel('Loss (log scale)')\n",
    "plt.title('Convergence Comparison'); plt.legend(); plt.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "for name, traj in trajectories.items():\n",
    "    print(f\"{name:12s} final loss: {loss_fn(traj[-1]):.6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557bcca0",
   "metadata": {},
   "source": [
    "## 3. Learning Rate Schedules\n",
    "\n",
    "Experiment with different LR schedules: step decay, cosine annealing, and warmup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74cf3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate schedulers\n",
    "def step_decay(epoch, lr_initial, drop_rate=0.5, epochs_drop=10):\n",
    "    \"\"\"Step decay LR schedule\"\"\"\n",
    "    return lr_initial * (drop_rate ** (epoch // epochs_drop))\n",
    "\n",
    "def cosine_annealing(epoch, lr_initial, T_max):\n",
    "    \"\"\"Cosine annealing LR schedule\"\"\"\n",
    "    return lr_initial * 0.5 * (1 + np.cos(np.pi * epoch / T_max))\n",
    "\n",
    "def warmup_cosine(epoch, lr_initial, warmup_epochs=5, T_max=50):\n",
    "    \"\"\"Warmup + cosine annealing\"\"\"\n",
    "    if epoch < warmup_epochs:\n",
    "        return lr_initial * (epoch + 1) / warmup_epochs\n",
    "    else:\n",
    "        return cosine_annealing(epoch - warmup_epochs, lr_initial, T_max - warmup_epochs)\n",
    "\n",
    "# Visualize schedules\n",
    "epochs = np.arange(0, 50)\n",
    "lr_initial = 0.1\n",
    "\n",
    "schedules = {\n",
    "    'Constant': [lr_initial] * len(epochs),\n",
    "    'Step Decay': [step_decay(e, lr_initial) for e in epochs],\n",
    "    'Cosine': [cosine_annealing(e, lr_initial, T_max=50) for e in epochs],\n",
    "    'Warmup+Cosine': [warmup_cosine(e, lr_initial) for e in epochs]\n",
    "}\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "for name, schedule in schedules.items():\n",
    "    plt.plot(epochs, schedule, linewidth=2, label=name)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.title('Learning Rate Schedules')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae3752e",
   "metadata": {},
   "source": [
    "## Exercises for Further Practice\n",
    "\n",
    "1. **LR Range Test**: Implement Leslie Smith's LR range test to find optimal LR\n",
    "2. **Hyperparameter Sensitivity**: Grid search over beta1/beta2 for Adam and plot effects\n",
    "3. **Non-Convex Losses**: Test optimizers on Rosenbrock function or other challenging landscapes\n",
    "4. **Adaptive vs Fixed LR**: Compare Adam with fixed LR vs Adam with cosine schedule\n",
    "5. **Implement AdaGrad**: Add AdaGrad optimizer and compare to Adam/RMSProp\n",
    "\n",
    "## Deliverables Checklist\n",
    "\n",
    "- [ ] Optimizer implementations (Momentum, RMSProp, Adam)\n",
    "- [ ] Comparison plots on at least two problems (convex + non-convex)\n",
    "- [ ] LR schedule experiments with visualizations  \n",
    "- [ ] Short write-up explaining which optimizer works best for which problem type\n",
    "\n",
    "## Recommended Resources\n",
    "\n",
    "- Kingma & Ba (2014): \"Adam: A Method for Stochastic Optimization\"\n",
    "- Ruder (2016): \"An overview of gradient descent optimization algorithms\"\n",
    "- Smith (2018): \"A disciplined approach to neural network hyper-parameters\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9b1fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# EXERCISE 1 — LR Range Test\n",
    "# Goal: implement Leslie Smith's range test: run Adam for 50 steps while\n",
    "# linearly increasing LR from 1e-4 to 1.0. Plot loss vs LR.\n",
    "# The \"good\" LR is just before the loss starts rising. Use cached() for the sweep.\n",
    "# Expected insight: there is a clear \"sweet spot\" LR region for every problem.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d9c07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# EXERCISE 2 — Hyperparameter Sensitivity (beta1/beta2 grid for Adam)\n",
    "# Goal: create a 5x5 grid of beta1 in [0.5, 0.99] and beta2 in [0.9, 0.9999].\n",
    "# For each pair, run Adam for 100 steps and record final loss.\n",
    "# Plot as a heatmap. Use cached(f\"adam_beta1_{b1}_beta2_{b2}\", ...) per combo.\n",
    "# Expected insight: Adam is robust within a wide range but breaks at extremes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8d96d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# EXERCISE 3 — Non-Convex Loss: Rosenbrock Function\n",
    "# Goal: test all three optimizers on f(x,y) = (1-x)^2 + 100*(y-x^2)^2.\n",
    "# Start from (-1, 1). Run 5000 steps. Plot trajectories on a contour plot.\n",
    "# Use cached(f\"rosenbrock_{name}_5000steps\", ...) per optimizer.\n",
    "# Expected insight: Adam and Momentum often struggle here — Rosenbrock is notoriously hard.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc617cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# EXERCISE 5 — Implement AdaGrad\n",
    "# Goal: add an AdaGradOptimizer class that accumulates squared gradients (no decay).\n",
    "# Compare it to RMSProp on both the quadratic loss and Rosenbrock.\n",
    "# Expected insight: AdaGrad's learning rate shrinks to near-zero over time,\n",
    "# while RMSProp's exponential moving average keeps it alive.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
