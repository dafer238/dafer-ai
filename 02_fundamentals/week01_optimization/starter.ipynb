{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67151a4d",
   "metadata": {},
   "source": [
    "# Week 01 — Optimization Intuition (Loss as Energy)\n",
    "\n",
    "This notebook guides you through building physical intuition for loss landscapes and optimization dynamics. You'll:\n",
    "- Visualize loss landscapes and understand gradient-based optimization\n",
    "- Implement gradient descent, SGD, and momentum\n",
    "- Experiment with learning rates and hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e3dc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf6711e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ─── Result Cache Utility ────────────────────────────────────────────────────\n",
    "# All computed results (trajectories, sweeps) are saved to disk so re-running\n",
    "# the notebook skips expensive computation and loads from cache instead.\n",
    "import os, pickle\n",
    "\n",
    "CACHE_DIR = \"cache_week01\"\n",
    "os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "\n",
    "def save_result(key, obj):\n",
    "    \"\"\"Pickle obj to CACHE_DIR/<key>.pkl\"\"\"\n",
    "    with open(os.path.join(CACHE_DIR, f\"{key}.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(obj, f)\n",
    "    print(f\"[cache] saved '{key}'\")\n",
    "\n",
    "def load_result(key):\n",
    "    \"\"\"Return unpickled object or None if not cached.\"\"\"\n",
    "    path = os.path.join(CACHE_DIR, f\"{key}.pkl\")\n",
    "    if os.path.exists(path):\n",
    "        with open(path, \"rb\") as f:\n",
    "            obj = pickle.load(f)\n",
    "        print(f\"[cache] loaded '{key}' from {path}\")\n",
    "        return obj\n",
    "    return None\n",
    "\n",
    "def cached(key, compute_fn):\n",
    "    \"\"\"Return cached result if available; otherwise compute, save, and return.\"\"\"\n",
    "    result = load_result(key)\n",
    "    if result is None:\n",
    "        result = compute_fn()\n",
    "        save_result(key, result)\n",
    "    return result\n",
    "\n",
    "print(\"Cache utility ready. Results will be stored in:\", CACHE_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9175560e",
   "metadata": {},
   "source": [
    "## 1. Visualize Simple Loss Landscapes\n",
    "\n",
    "Create 2D grids and plot loss contours for quadratic and multimodal functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5548dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss functions\n",
    "def quadratic(x, y):\n",
    "    \"\"\"Simple quadratic loss\"\"\"\n",
    "    return x**2 + 3*y**2\n",
    "\n",
    "def multimodal(x, y):\n",
    "    \"\"\"Multimodal loss with multiple local minima\"\"\"\n",
    "    return np.sin(x) * np.cos(y) + 0.1*(x**2 + y**2)\n",
    "\n",
    "# Create grid\n",
    "x_range = np.linspace(-3, 3, 100)\n",
    "y_range = np.linspace(-3, 3, 100)\n",
    "X, Y = np.meshgrid(x_range, y_range)\n",
    "\n",
    "# Compute losses\n",
    "Z_quad = quadratic(X, Y)\n",
    "Z_multi = multimodal(X, Y)\n",
    "\n",
    "# Plot contours\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Quadratic\n",
    "contour1 = axes[0].contour(X, Y, Z_quad, levels=20, cmap='viridis')\n",
    "axes[0].set_title('Quadratic Loss Landscape')\n",
    "axes[0].set_xlabel('x')\n",
    "axes[0].set_ylabel('y')\n",
    "plt.colorbar(contour1, ax=axes[0])\n",
    "\n",
    "# Multimodal\n",
    "contour2 = axes[1].contour(X, Y, Z_multi, levels=20, cmap='viridis')\n",
    "axes[1].set_title('Multimodal Loss Landscape')\n",
    "axes[1].set_xlabel('x')\n",
    "axes[1].set_ylabel('y')\n",
    "plt.colorbar(contour2, ax=axes[1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe327623",
   "metadata": {},
   "source": [
    "## 2. Gradient Descent Dynamics\n",
    "\n",
    "Implement vanilla gradient descent and plot parameter trajectories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902ee1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Gradient descent implementation\n",
    "def gradient_descent(x0, y0, grad_fn, lr=0.1, n_steps=50):\n",
    "    trajectory = [(x0, y0)]\n",
    "    x, y = x0, y0\n",
    "    for _ in range(n_steps):\n",
    "        gx, gy = grad_fn(x, y)\n",
    "        x -= lr * gx\n",
    "        y -= lr * gy\n",
    "        trajectory.append((x, y))\n",
    "    return trajectory\n",
    "\n",
    "def quad_gradient(x, y):\n",
    "    return (2*x, 6*y)\n",
    "\n",
    "# ── Cached GD trajectory ──────────────────────────────────────────────────────\n",
    "trajectory_gd = cached(\n",
    "    \"gd_trajectory_lr0.1\",\n",
    "    lambda: gradient_descent(-2.0, 2.0, quad_gradient, lr=0.1, n_steps=50)\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.contour(X, Y, Z_quad, levels=20, cmap='viridis', alpha=0.6)\n",
    "traj_x = [p[0] for p in trajectory_gd]\n",
    "traj_y = [p[1] for p in trajectory_gd]\n",
    "plt.plot(traj_x, traj_y, 'r-o', markersize=4, linewidth=2, label='GD trajectory')\n",
    "plt.plot(traj_x[0], traj_y[0], 'go', markersize=10, label='Start')\n",
    "plt.plot(traj_x[-1], traj_y[-1], 'ro', markersize=10, label='End')\n",
    "plt.title('Gradient Descent on Quadratic Loss')\n",
    "plt.xlabel('x'); plt.ylabel('y')\n",
    "plt.legend(); plt.grid(alpha=0.3); plt.show()\n",
    "print(f\"Final position: ({traj_x[-1]:.4f}, {traj_y[-1]:.4f})\")\n",
    "print(f\"Final loss: {quadratic(traj_x[-1], traj_y[-1]):.6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d15bc8",
   "metadata": {},
   "source": [
    "## 3. Momentum and Learning Rate Sweeps\n",
    "\n",
    "Implement momentum and experiment with different learning rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9f44fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def gradient_descent_momentum(x0, y0, grad_fn, lr=0.1, momentum=0.9, n_steps=50):\n",
    "    trajectory = [(x0, y0)]\n",
    "    x, y = x0, y0\n",
    "    vx, vy = 0.0, 0.0\n",
    "    for _ in range(n_steps):\n",
    "        gx, gy = grad_fn(x, y)\n",
    "        vx = momentum * vx - lr * gx\n",
    "        vy = momentum * vy - lr * gy\n",
    "        x += vx; y += vy\n",
    "        trajectory.append((x, y))\n",
    "    return trajectory\n",
    "\n",
    "# ── Cached momentum trajectory ────────────────────────────────────────────────\n",
    "trajectory_momentum = cached(\n",
    "    \"momentum_trajectory_lr0.05_m0.5\",\n",
    "    lambda: gradient_descent_momentum(2.0, -1.0, quad_gradient, lr=0.05, momentum=0.5, n_steps=50)\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.contour(X, Y, Z_quad, levels=20, cmap='viridis', alpha=0.6)\n",
    "plt.plot(traj_x, traj_y, 'b-o', markersize=3, linewidth=1.5, label='Vanilla GD', alpha=0.7)\n",
    "mom_x = [p[0] for p in trajectory_momentum]\n",
    "mom_y = [p[1] for p in trajectory_momentum]\n",
    "plt.plot(mom_x, mom_y, 'r-o', markersize=3, linewidth=1.5, label='GD + Momentum', alpha=0.7)\n",
    "plt.plot(2.0, -1.0, 'go', markersize=10, label='Start')\n",
    "plt.title('Gradient Descent: Vanilla vs Momentum')\n",
    "plt.xlabel('x'); plt.ylabel('y')\n",
    "plt.legend(); plt.grid(alpha=0.3); plt.show()\n",
    "print(f\"GD final loss: {quadratic(traj_x[-1], traj_y[-1]):.6f}\")\n",
    "print(f\"Momentum final loss: {quadratic(mom_x[-1], mom_y[-1]):.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176274eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ── Cached LR sweep ───────────────────────────────────────────────────────────\n",
    "learning_rates = [0.01, 0.05, 0.1, 0.2, 0.25]\n",
    "\n",
    "def _compute_lr_sweep():\n",
    "    return {lr: gradient_descent(2.0, -1.0, quad_gradient, lr=lr, n_steps=50)\n",
    "            for lr in learning_rates}\n",
    "\n",
    "lr_sweep_results = cached(\"lr_sweep_quadratic\", _compute_lr_sweep)\n",
    "\n",
    "final_losses = []\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.contour(X, Y, Z_quad, levels=20, cmap='viridis', alpha=0.3)\n",
    "for lr in learning_rates:\n",
    "    traj = lr_sweep_results[lr]\n",
    "    tx = [p[0] for p in traj]; ty = [p[1] for p in traj]\n",
    "    final_losses.append(quadratic(tx[-1], ty[-1]))\n",
    "    plt.plot(tx, ty, '-o', markersize=2, label=f'LR={lr}', alpha=0.7)\n",
    "plt.title('Learning Rate Sweep on Quadratic Loss')\n",
    "plt.xlabel('x'); plt.ylabel('y'); plt.legend(); plt.grid(alpha=0.3); plt.show()\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(learning_rates, final_losses, 'o-', linewidth=2, markersize=8)\n",
    "plt.xlabel('Learning Rate'); plt.ylabel('Final Loss')\n",
    "plt.title('Final Loss vs Learning Rate'); plt.grid(alpha=0.3); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45573a8",
   "metadata": {},
   "source": [
    "## Exercises for Further Practice\n",
    "\n",
    "1. **Explore SGD**: Implement stochastic gradient descent with mini-batches on a simple regression dataset\n",
    "2. **Saddle Points**: Create a saddle-point loss function and visualize optimizer behavior\n",
    "3. **Cyclical Learning Rates**: Implement Leslie Smith's cyclical LR and compare to constant LR\n",
    "4. **3D Visualization**: Create 3D surface plots of the loss landscapes\n",
    "5. **Divergence Analysis**: Find learning rates that cause divergence and explain why"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfef155",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# EXERCISE 1 — Stochastic Gradient Descent (SGD)\n",
    "# Goal: implement SGD where each update uses only a single random sample\n",
    "# from a small synthetic regression dataset (X shape (100,1), y = 3x + noise).\n",
    "# Compare the noisy SGD trajectory vs full-batch GD on the same loss contour.\n",
    "# Expected insight: SGD oscillates but converges to a similar minimum with\n",
    "# the right learning rate. Use cached() to store each run you want to revisit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bebdf53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# EXERCISE 2 — Saddle Points\n",
    "# Goal: define the function f(x, y) = x^2 - y^2 (classic saddle).\n",
    "# Visualize the surface and contours. Run GD from (0.01, 0.01) — does it\n",
    "# escape the saddle? Try adding small random noise to gradients (as in SGD).\n",
    "# Expected insight: saddle points are common in non-convex losses; pure GD\n",
    "# can stall there but noise helps escape.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86645a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# EXERCISE 3 — Cyclical Learning Rates (CLR)\n",
    "# Goal: implement Leslie Smith's triangular LR policy: LR oscillates between\n",
    "# base_lr=0.001 and max_lr=0.1 over a cycle of 10 steps.\n",
    "# Apply it to GD on the quadratic loss and plot the trajectory alongside\n",
    "# constant-LR GD on the same contour.\n",
    "# Use cached() to store the CLR trajectory.\n",
    "# Expected insight: CLR escapes local basins faster than constant LR.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d31234b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# EXERCISE 4 — 3D Loss Surface Visualization\n",
    "# Goal: create a 3D surface plot (use Axes3D / plot_surface) for the\n",
    "# multimodal loss used in Exercise 2 (or invent your own).\n",
    "# Overlay a GD trajectory by plotting each (x, y, loss(x,y)) point as\n",
    "# a 3D scatter. Rotate the view to find the most revealing angle.\n",
    "# Expected insight: 3D views surface structure invisible in 2D contours.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dea2d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# EXERCISE 5 — Divergence Analysis\n",
    "# Goal: run GD on the quadratic loss with LR values [0.3, 0.4, 0.5, 0.6].\n",
    "# For each, compute loss at every step and plot on a log-scale.\n",
    "# Find the exact threshold where divergence starts and explain WHY using\n",
    "# the update rule: for f = x^2 + 3y^2, the condition is lr < 1/max_eigenvalue.\n",
    "# Deliverable: a table of LR → \"converges / diverges\" with numerical evidence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57476fb",
   "metadata": {},
   "source": [
    "## Deliverables Checklist\n",
    "\n",
    "- [ ] Loss landscape visualizations (quadratic + multimodal)\n",
    "- [ ] GD/SGD/momentum implementations with trajectory plots\n",
    "- [ ] Learning rate sweep analysis\n",
    "- [ ] Short conclusions about convergence behavior and hyperparameter sensitivity\n",
    "\n",
    "## Recommended Next Steps\n",
    "\n",
    "- Review optimization sections in Goodfellow et al. \"Deep Learning\"\n",
    "- Explore Leslie Smith's LR range test and cyclical LR papers\n",
    "- Try implementing Adam optimizer (Week 02 preview)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
