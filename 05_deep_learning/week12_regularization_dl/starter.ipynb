{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1df4413c",
   "metadata": {},
   "source": [
    "# Week 12 — Regularization at Scale\n",
    "\n",
    "This notebook covers regularization techniques for deep models. You'll:\n",
    "- Implement and compare dropout, weight decay, and data augmentation\n",
    "- Design rigorous ablation experiments\n",
    "- Build ensemble models\n",
    "- Measure generalization improvements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe78aaae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "torch.manual_seed(42)\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72b45ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, pickle\n",
    "import torch\n",
    "\n",
    "CACHE_DIR = \"cache_week12\"\n",
    "os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "\n",
    "def save_result(key, obj):\n",
    "    with open(os.path.join(CACHE_DIR, f\"{key}.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(obj, f)\n",
    "\n",
    "def load_result(key):\n",
    "    path = os.path.join(CACHE_DIR, f\"{key}.pkl\")\n",
    "    if os.path.exists(path):\n",
    "        with open(path, \"rb\") as f:\n",
    "            return pickle.load(f)\n",
    "    return None\n",
    "\n",
    "def cached(key, compute_fn):\n",
    "    result = load_result(key)\n",
    "    if result is not None:\n",
    "        print(f\"[cache] loaded '{key}'\")\n",
    "        return result\n",
    "    print(f\"[cache] computing '{key}'...\")\n",
    "    result = compute_fn()\n",
    "    save_result(key, result)\n",
    "    return result\n",
    "\n",
    "def save_model(model, name):\n",
    "    torch.save(model.state_dict(), os.path.join(CACHE_DIR, f\"{name}.pth\"))\n",
    "\n",
    "def load_model_state(name):\n",
    "    path = os.path.join(CACHE_DIR, f\"{name}.pth\")\n",
    "    return torch.load(path, weights_only=True) if os.path.exists(path) else None\n",
    "\n",
    "print(\"Cache utilities ready. Results will be stored in:\", CACHE_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f63428",
   "metadata": {},
   "source": [
    "## 1. Dropout Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbda9268",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "class MLPWithDropout(nn.Module):\n",
    "    def __init__(self, input_size=784, hidden_size=256, num_classes=10, dropout_p=0.5):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.dropout1 = nn.Dropout(dropout_p)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.dropout2 = nn.Dropout(dropout_p)\n",
    "        self.fc3 = nn.Linear(hidden_size, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.relu(self.fc1(x));  x = self.dropout1(x)\n",
    "        x = self.relu(self.fc2(x));  x = self.dropout2(x)\n",
    "        return self.fc3(x)\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "train_dataset = datasets.MNIST('./data', train=True,  download=True, transform=transform)\n",
    "test_dataset  = datasets.MNIST('./data', train=False,               transform=transform)\n",
    "train_loader  = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader   = DataLoader(test_dataset,  batch_size=64, shuffle=False)\n",
    "\n",
    "def train_and_evaluate(model, train_loader, test_loader, n_epochs=5):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    train_losses, test_accs = [], []\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        epoch_loss = sum(\n",
    "            (lambda: (optimizer.zero_grad(), (loss := criterion(model(d), t)),\n",
    "                      loss.backward(), optimizer.step(), loss.item())[-1])()\n",
    "            for d, t in train_loader)\n",
    "        train_losses.append(epoch_loss / len(train_loader))\n",
    "        model.eval()\n",
    "        correct = sum(model(d).argmax(dim=1).eq(t).sum().item()\n",
    "                      for d, t in (torch.no_grad(), test_loader)[1])\n",
    "        test_accs.append(100. * correct / len(test_dataset))\n",
    "        print(f\"Epoch {epoch+1}: Train Loss={train_losses[-1]:.4f}, Test Acc={test_accs[-1]:.2f}%\")\n",
    "    return train_losses, test_accs\n",
    "\n",
    "# --- cleaner version (same logic, more readable) ---\n",
    "def _train_eval(model, n_epochs=5):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    tr_losses, te_accs = [], []\n",
    "    for ep in range(n_epochs):\n",
    "        model.train()\n",
    "        ep_loss = 0.0\n",
    "        for d, t in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(model(d), t)\n",
    "            loss.backward(); optimizer.step()\n",
    "            ep_loss += loss.item()\n",
    "        tr_losses.append(ep_loss / len(train_loader))\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        with torch.no_grad():\n",
    "            for d, t in test_loader:\n",
    "                correct += model(d).argmax(1).eq(t).sum().item()\n",
    "        te_accs.append(100. * correct / len(test_dataset))\n",
    "        print(f\"  Epoch {ep+1}: Loss={tr_losses[-1]:.4f}, Acc={te_accs[-1]:.2f}%\")\n",
    "    return tr_losses, te_accs\n",
    "\n",
    "def _dropout_comparison():\n",
    "    print(\"Training WITHOUT dropout...\")\n",
    "    m0 = MLPWithDropout(dropout_p=0.0)\n",
    "    l0, a0 = _train_eval(m0)\n",
    "    print(\"\\nTraining WITH dropout (p=0.5)...\")\n",
    "    m1 = MLPWithDropout(dropout_p=0.5)\n",
    "    l1, a1 = _train_eval(m1)\n",
    "    return l0, a0, l1, a1\n",
    "\n",
    "losses_no_drop, accs_no_drop, losses_drop, accs_drop = cached(\n",
    "    \"dropout_comparison_5epochs\", _dropout_comparison)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "axes[0].plot(losses_no_drop, label='No dropout', linewidth=2)\n",
    "axes[0].plot(losses_drop,    label='Dropout p=0.5', linewidth=2)\n",
    "axes[0].set(xlabel='Epoch', ylabel='Training Loss', title='Training Loss')\n",
    "axes[0].legend(); axes[0].grid(alpha=0.3)\n",
    "axes[1].plot(accs_no_drop, label='No dropout', linewidth=2)\n",
    "axes[1].plot(accs_drop,    label='Dropout p=0.5', linewidth=2)\n",
    "axes[1].set(xlabel='Epoch', ylabel='Test Accuracy (%)', title='Test Accuracy')\n",
    "axes[1].legend(); axes[1].grid(alpha=0.3)\n",
    "plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abaca942",
   "metadata": {},
   "source": [
    "## 2. Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5595c157",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "transform_augmented = transforms.Compose([\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "def _augmentation_experiment():\n",
    "    train_aug = DataLoader(\n",
    "        datasets.MNIST('./data', train=True, download=True, transform=transform_augmented),\n",
    "        batch_size=64, shuffle=True)\n",
    "    print(\"Training WITH data augmentation...\")\n",
    "    m = MLPWithDropout(dropout_p=0.0)\n",
    "    losses, accs = _train_eval(m, n_epochs=5)\n",
    "    return losses, accs\n",
    "\n",
    "losses_aug, accs_aug = cached(\"augmentation_experiment_5epochs\", _augmentation_experiment)\n",
    "\n",
    "print(f\"Final test accuracy (no augmentation): {accs_no_drop[-1]:.2f}%\")\n",
    "print(f\"Final test accuracy (with augmentation): {accs_aug[-1]:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57911a5",
   "metadata": {},
   "source": [
    "## 3. Weight Decay (L2 Regularization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23edd7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "weight_decays = [0.0, 1e-4, 1e-3, 1e-2]\n",
    "\n",
    "def _wd_sweep():\n",
    "    results = {}\n",
    "    for wd in weight_decays:\n",
    "        print(f\"  Training with weight_decay={wd}...\")\n",
    "        m = MLPWithDropout(dropout_p=0.0)\n",
    "        crit = nn.CrossEntropyLoss()\n",
    "        opt  = optim.Adam(m.parameters(), lr=0.001, weight_decay=wd)\n",
    "        for _ in range(5):\n",
    "            m.train()\n",
    "            for d, t in train_loader:\n",
    "                opt.zero_grad()\n",
    "                loss = crit(m(d), t)\n",
    "                loss.backward(); opt.step()\n",
    "        m.eval()\n",
    "        correct = sum(m(d).argmax(1).eq(t).sum().item() for d, t in test_loader)\n",
    "        acc = 100. * correct / len(test_dataset)\n",
    "        results[wd] = acc\n",
    "        print(f\"    Test accuracy: {acc:.2f}%\")\n",
    "    return results\n",
    "\n",
    "wd_results = cached(\"weight_decay_sweep_5epochs\", _wd_sweep)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(list(wd_results.keys()), list(wd_results.values()), 'o-', linewidth=2, markersize=8)\n",
    "plt.xlabel('Weight Decay'); plt.ylabel('Test Accuracy (%)')\n",
    "plt.title('Effect of Weight Decay'); plt.xscale('log'); plt.grid(alpha=0.3); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd6a754",
   "metadata": {},
   "source": [
    "## 4. Ensemble Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb92f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n_models = 5\n",
    "\n",
    "def _train_ensemble():\n",
    "    models_state = []\n",
    "    for i in range(n_models):\n",
    "        torch.manual_seed(i)\n",
    "        m = MLPWithDropout(dropout_p=0.3)\n",
    "        crit = nn.CrossEntropyLoss()\n",
    "        opt  = optim.Adam(m.parameters(), lr=0.001)\n",
    "        for _ in range(3):\n",
    "            m.train()\n",
    "            for d, t in train_loader:\n",
    "                opt.zero_grad()\n",
    "                loss = crit(m(d), t)\n",
    "                loss.backward(); opt.step()\n",
    "        models_state.append(m.state_dict())\n",
    "        print(f\"  Model {i+1} trained\")\n",
    "    return models_state\n",
    "\n",
    "ensemble_states = cached(\"ensemble_5models_3epochs\", _train_ensemble)\n",
    "\n",
    "ensemble_models = []\n",
    "for state in ensemble_states:\n",
    "    m = MLPWithDropout(dropout_p=0.3)\n",
    "    m.load_state_dict(state); m.eval()\n",
    "    ensemble_models.append(m)\n",
    "\n",
    "def ensemble_predict(models, data):\n",
    "    with torch.no_grad():\n",
    "        avg = torch.stack([m(data) for m in models]).mean(0)\n",
    "    return avg.argmax(1)\n",
    "\n",
    "correct_ens, correct_single = 0, 0\n",
    "for d, t in test_loader:\n",
    "    correct_ens    += ensemble_predict(ensemble_models, d).eq(t).sum().item()\n",
    "    correct_single += ensemble_models[0](d).argmax(1).eq(t).sum().item()\n",
    "\n",
    "acc_ens    = 100. * correct_ens    / len(test_dataset)\n",
    "acc_single = 100. * correct_single / len(test_dataset)\n",
    "print(f\"Single model accuracy: {acc_single:.2f}%\")\n",
    "print(f\"Ensemble accuracy:     {acc_ens:.2f}%\")\n",
    "print(f\"Improvement:           {acc_ens - acc_single:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c623719a",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1. **BatchNorm + Dropout**: Combine BatchNorm and Dropout and compare\n",
    "2. **Ablation Tables**: Create comprehensive ablation tables for all regularization methods\n",
    "3. **Augmentation Study**: Try different augmentation strategies and measure effects\n",
    "4. **Real Dataset**: Apply to CIFAR-10 or similar\n",
    "\n",
    "## Deliverables\n",
    "\n",
    "- [ ] Dropout experiments with validation curves\n",
    "- [ ] Data augmentation study\n",
    "- [ ] Weight decay comparison\n",
    "- [ ] Ensemble model implementation\n",
    "- [ ] Ablation table with clear recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710ab2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# EXERCISE 1 — BatchNorm + Dropout Interaction\n",
    "# Goal: add BatchNorm1d after each hidden layer in MLPWithDropout.\n",
    "# Test all 4 combos: (BN off/on) × (Dropout off/on). Report test accuracy.\n",
    "# Use cached(f\"bn{bn}_drop{dp}_5epochs\", ...) for each combo.\n",
    "# Expected insight: BatchNorm + Dropout can conflict — the recommended order matters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786e15da",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# EXERCISE 3 — Comprehensive Ablation Table\n",
    "# Goal: train 9 models (all combos of dropout∈{0,0.3,0.5} × wd∈{0,1e-4,1e-3})\n",
    "# for 5 epochs on MNIST. Record test accuracy in a 3×3 table (use pandas DataFrame).\n",
    "# Use cached(f\"ablation_dp{dp}_wd{wd}\", ...) per cell.\n",
    "# Deliverable: print the DataFrame and identify the best regularization combo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684bdbe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# EXERCISE 4 — Early Stopping as Implicit Regularization\n",
    "# Goal: implement a training loop with early stopping (patience=5, monitor val_loss).\n",
    "# Train a model without explicit dropout/weight-decay; stop when val_loss stops improving.\n",
    "# Use cached(\"early_stopping_run\", ...) to store (best_epoch, best_val_acc, history).\n",
    "# Compare stopped epoch vs. full-run epoch and explain the regularization effect.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
