{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "645fcc9f",
   "metadata": {},
   "source": [
    "# Week 10 — Efficient Training (Training at Scale)\n",
    "\n",
    "This notebook focuses on building efficient training pipelines. You'll:\n",
    "- Implement efficient DataLoaders with prefetching\n",
    "- Profile and optimize data loading bottlenecks\n",
    "- Use learning rate schedulers\n",
    "- Build robust checkpoint systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4350925f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from pathlib import Path\n",
    "%matplotlib inline\n",
    "\n",
    "torch.manual_seed(42)\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b12c1d",
   "metadata": {},
   "source": [
    "## 1. Custom Dataset with Efficient Loading\n",
    "\n",
    "Implement a custom dataset with proper data loading patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8be3320",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom dataset with simulated I/O delay\n",
    "class SyntheticDataset(Dataset):\n",
    "    def __init__(self, n_samples=1000, n_features=100, delay_ms=0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            n_samples: Number of samples\n",
    "            n_features: Number of features\n",
    "            delay_ms: Simulated I/O delay in milliseconds\n",
    "        \"\"\"\n",
    "        self.n_samples = n_samples\n",
    "        self.n_features = n_features\n",
    "        self.delay_ms = delay_ms\n",
    "        \n",
    "        # Pre-generate data (in practice, you'd load from disk)\n",
    "        self.data = torch.randn(n_samples, n_features)\n",
    "        self.labels = torch.randint(0, 10, (n_samples,))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Simulate I/O delay\n",
    "        if self.delay_ms > 0:\n",
    "            time.sleep(self.delay_ms / 1000.0)\n",
    "        \n",
    "        return self.data[idx], self.labels[idx]\n",
    "\n",
    "# Create datasets with different delays\n",
    "dataset_fast = SyntheticDataset(n_samples=1000, delay_ms=0)\n",
    "dataset_slow = SyntheticDataset(n_samples=1000, delay_ms=1)\n",
    "\n",
    "print(f\"Dataset size: {len(dataset_fast)}\")\n",
    "sample_x, sample_y = dataset_fast[0]\n",
    "print(f\"Sample shape: {sample_x.shape}, label: {sample_y}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6026fae",
   "metadata": {},
   "source": [
    "## 2. DataLoader Performance Comparison\n",
    "\n",
    "Compare different DataLoader configurations and measure throughput."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720a085f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark function\n",
    "def benchmark_dataloader(dataset, batch_size=32, num_workers=0, pin_memory=False, n_batches=50):\n",
    "    \"\"\"\n",
    "    Benchmark DataLoader throughput\n",
    "    \"\"\"\n",
    "    loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin_memory,\n",
    "        shuffle=True\n",
    "    )\n",
    "    \n",
    "    start_time = time.time()\n",
    "    for i, (batch_x, batch_y) in enumerate(loader):\n",
    "        if i >= n_batches:\n",
    "            break\n",
    "        # Simulate processing\n",
    "        _ = batch_x.mean()\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    throughput = n_batches / elapsed\n",
    "    \n",
    "    return elapsed, throughput\n",
    "\n",
    "# Compare configurations\n",
    "configs = [\n",
    "    {'num_workers': 0, 'pin_memory': False, 'name': '0 workers'},\n",
    "    {'num_workers': 2, 'pin_memory': False, 'name': '2 workers'},\n",
    "    {'num_workers': 4, 'pin_memory': False, 'name': '4 workers'},\n",
    "    {'num_workers': 4, 'pin_memory': True, 'name': '4 workers + pin_memory'},\n",
    "]\n",
    "\n",
    "print(\"Benchmarking DataLoader configurations...\\n\")\n",
    "results = []\n",
    "\n",
    "for config in configs:\n",
    "    elapsed, throughput = benchmark_dataloader(\n",
    "        dataset_slow,\n",
    "        batch_size=32,\n",
    "        num_workers=config['num_workers'],\n",
    "        pin_memory=config['pin_memory'],\n",
    "        n_batches=30\n",
    "    )\n",
    "    results.append({'name': config['name'], 'elapsed': elapsed, 'throughput': throughput})\n",
    "    print(f\"{config['name']:25s}: {elapsed:.2f}s ({throughput:.1f} batches/s)\")\n",
    "\n",
    "# Plot results\n",
    "names = [r['name'] for r in results]\n",
    "throughputs = [r['throughput'] for r in results]\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.barh(names, throughputs, color='skyblue')\n",
    "plt.xlabel('Throughput (batches/second)')\n",
    "plt.title('DataLoader Performance Comparison')\n",
    "plt.grid(axis='x', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n→ Using multiple workers significantly improves throughput!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee4d822",
   "metadata": {},
   "source": [
    "## 3. Learning Rate Schedulers\n",
    "\n",
    "Experiment with different LR scheduling strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b9bbe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple model for demonstration\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(100, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 10)\n",
    ")\n",
    "\n",
    "# Create different schedulers\n",
    "def visualize_schedulers(initial_lr=0.1, n_epochs=100):\n",
    "    schedulers_to_test = {\n",
    "        'StepLR': optim.lr_scheduler.StepLR(\n",
    "            optim.SGD(model.parameters(), lr=initial_lr),\n",
    "            step_size=30,\n",
    "            gamma=0.1\n",
    "        ),\n",
    "        'ExponentialLR': optim.lr_scheduler.ExponentialLR(\n",
    "            optim.SGD(model.parameters(), lr=initial_lr),\n",
    "            gamma=0.95\n",
    "        ),\n",
    "        'CosineAnnealingLR': optim.lr_scheduler.CosineAnnealingLR(\n",
    "            optim.SGD(model.parameters(), lr=initial_lr),\n",
    "            T_max=n_epochs\n",
    "        ),\n",
    "        'ReduceLROnPlateau': optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optim.SGD(model.parameters(), lr=initial_lr),\n",
    "            mode='min',\n",
    "            factor=0.5,\n",
    "            patience=10\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    lr_histories = {name: [] for name in schedulers_to_test.keys()}\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        for name, scheduler in schedulers_to_test.items():\n",
    "            current_lr = scheduler.optimizer.param_groups[0]['lr']\n",
    "            lr_histories[name].append(current_lr)\n",
    "            \n",
    "            # Step scheduler\n",
    "            if name == 'ReduceLROnPlateau':\n",
    "                # Simulate a metric (e.g., loss)\n",
    "                fake_loss = 1.0 / (epoch + 1) + np.random.randn() * 0.01\n",
    "                scheduler.step(fake_loss)\n",
    "            else:\n",
    "                scheduler.step()\n",
    "    \n",
    "    return lr_histories\n",
    "\n",
    "# Visualize\n",
    "lr_histories = visualize_schedulers(initial_lr=0.1, n_epochs=100)\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "for name, lrs in lr_histories.items():\n",
    "    plt.plot(lrs, label=name, linewidth=2)\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.title('Learning Rate Schedulers Comparison')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.yscale('log')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b97322",
   "metadata": {},
   "source": [
    "## 4. Robust Checkpointing System\n",
    "\n",
    "Build a comprehensive checkpoint system with metadata and resumption support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f593333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpoint manager\n",
    "class CheckpointManager:\n",
    "    def __init__(self, checkpoint_dir='checkpoints', keep_last_n=3):\n",
    "        self.checkpoint_dir = Path(checkpoint_dir)\n",
    "        self.checkpoint_dir.mkdir(exist_ok=True)\n",
    "        self.keep_last_n = keep_last_n\n",
    "    \n",
    "    def save(self, model, optimizer, scheduler, epoch, metrics, is_best=False):\n",
    "        \"\"\"\n",
    "        Save checkpoint with metadata\n",
    "        \n",
    "        Args:\n",
    "            model: PyTorch model\n",
    "            optimizer: Optimizer state\n",
    "            scheduler: LR scheduler state\n",
    "            epoch: Current epoch\n",
    "            metrics: Dict of metrics (e.g., {'train_loss': 0.5, 'val_acc': 0.9})\n",
    "            is_best: Whether this is the best model so far\n",
    "        \"\"\"\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict() if scheduler else None,\n",
    "            'metrics': metrics,\n",
    "        }\n",
    "        \n",
    "        # Save regular checkpoint\n",
    "        checkpoint_path = self.checkpoint_dir / f'checkpoint_epoch_{epoch}.pth'\n",
    "        torch.save(checkpoint, checkpoint_path)\n",
    "        print(f\"Checkpoint saved: {checkpoint_path}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if is_best:\n",
    "            best_path = self.checkpoint_dir / 'best_model.pth'\n",
    "            torch.save(checkpoint, best_path)\n",
    "            print(f\"Best model saved: {best_path}\")\n",
    "        \n",
    "        # Clean up old checkpoints\n",
    "        self._cleanup_old_checkpoints()\n",
    "    \n",
    "    def load(self, checkpoint_path, model, optimizer=None, scheduler=None):\n",
    "        \"\"\"\n",
    "        Load checkpoint and restore state\n",
    "        \"\"\"\n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    "        \n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        if optimizer:\n",
    "            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        if scheduler and checkpoint['scheduler_state_dict']:\n",
    "            scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "        \n",
    "        return checkpoint['epoch'], checkpoint['metrics']\n",
    "    \n",
    "    def _cleanup_old_checkpoints(self):\n",
    "        \"\"\"Keep only the last N checkpoints\"\"\"\n",
    "        checkpoints = sorted(self.checkpoint_dir.glob('checkpoint_epoch_*.pth'))\n",
    "        if len(checkpoints) > self.keep_last_n:\n",
    "            for ckpt in checkpoints[:-self.keep_last_n]:\n",
    "                ckpt.unlink()\n",
    "                print(f\"Removed old checkpoint: {ckpt}\")\n",
    "\n",
    "# Example usage\n",
    "ckpt_manager = CheckpointManager(checkpoint_dir='./checkpoints', keep_last_n=3)\n",
    "\n",
    "# Simulate training and saving checkpoints\n",
    "model = nn.Linear(10, 2)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10)\n",
    "\n",
    "best_val_acc = 0.0\n",
    "for epoch in range(1, 6):\n",
    "    # Simulate training\n",
    "    train_loss = 1.0 / epoch + np.random.rand() * 0.1\n",
    "    val_acc = min(0.95, 0.5 + epoch * 0.1 + np.random.rand() * 0.05)\n",
    "    \n",
    "    metrics = {'train_loss': train_loss, 'val_acc': val_acc}\n",
    "    is_best = val_acc > best_val_acc\n",
    "    \n",
    "    if is_best:\n",
    "        best_val_acc = val_acc\n",
    "    \n",
    "    ckpt_manager.save(model, optimizer, scheduler, epoch, metrics, is_best=is_best)\n",
    "    scheduler.step()\n",
    "    print()\n",
    "\n",
    "# Load best model\n",
    "print(\"\\nLoading best model...\")\n",
    "best_epoch, best_metrics = ckpt_manager.load('./checkpoints/best_model.pth', model, optimizer, scheduler)\n",
    "print(f\"Loaded best model from epoch {best_epoch}\")\n",
    "print(f\"Metrics: {best_metrics}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab42cc69",
   "metadata": {},
   "source": [
    "## Exercises for Further Practice\n",
    "\n",
    "1. **Mixed Precision Training**: Use `torch.cuda.amp` for faster training\n",
    "2. **Gradient Accumulation**: Implement gradient accumulation for larger effective batch sizes\n",
    "3. **Profiling**: Use PyTorch profiler to identify bottlenecks\n",
    "4. **Distributed Training**: Explore `torch.nn.DataParallel` or `DistributedDataParallel`\n",
    "5. **Real Dataset**: Apply these techniques to a large dataset (e.g., ImageNet subset)\n",
    "\n",
    "## Deliverables Checklist\n",
    "\n",
    "- [ ] Custom dataset with efficient data loading\n",
    "- [ ] DataLoader performance benchmarks\n",
    "- [ ] Learning rate scheduler experiments\n",
    "- [ ] Robust checkpoint system implementation\n",
    "- [ ] Short report on throughput optimizations\n",
    "\n",
    "## Recommended Resources\n",
    "\n",
    "- PyTorch documentation on data loading and DataLoader\n",
    "- PyTorch performance tuning guide\n",
    "- Papers on distributed training and mixed precision"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
