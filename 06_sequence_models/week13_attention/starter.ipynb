{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3acc5066",
   "metadata": {},
   "source": [
    "# Week 13: Attention Mechanisms\n",
    "\n",
    "This notebook introduces attention mechanisms, a key building block for modern sequence-to-sequence models and transformers. You'll implement scaled dot-product attention from scratch and visualize how attention weights focus on relevant parts of input sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724791c8",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf0b40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import seaborn as sns\n",
    "\n",
    "# Reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Display configuration\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8d1404",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "# ── Cache helpers ─────────────────────────────────────────────────────────────\n",
    "CACHE_DIR = \"cache_week13\"\n",
    "os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "\n",
    "def save_result(key, obj):\n",
    "    with open(os.path.join(CACHE_DIR, f\"{key}.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(obj, f)\n",
    "\n",
    "def load_result(key):\n",
    "    path = os.path.join(CACHE_DIR, f\"{key}.pkl\")\n",
    "    if os.path.exists(path):\n",
    "        with open(path, \"rb\") as f:\n",
    "            return pickle.load(f)\n",
    "    return None\n",
    "\n",
    "def cached(key, compute_fn):\n",
    "    result = load_result(key)\n",
    "    if result is not None:\n",
    "        print(f\"[cache] Loaded '{key}'\")\n",
    "        return result\n",
    "    print(f\"[cache] Computing '{key}' …\")\n",
    "    result = compute_fn()\n",
    "    save_result(key, result)\n",
    "    return result\n",
    "\n",
    "def save_model(model, name):\n",
    "    path = os.path.join(CACHE_DIR, f\"{name}.pth\")\n",
    "    torch.save(model.state_dict(), path)\n",
    "    print(f\"[cache] Model saved → {path}\")\n",
    "\n",
    "def load_model_state(name):\n",
    "    path = os.path.join(CACHE_DIR, f\"{name}.pth\")\n",
    "    if os.path.exists(path):\n",
    "        print(f\"[cache] Model loaded ← {path}\")\n",
    "        return torch.load(path, weights_only=True)\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a0e9b6",
   "metadata": {},
   "source": [
    "## Exercise 1: Scaled Dot-Product Attention from Scratch\n",
    "\n",
    "Implement the core attention mechanism used in transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de60bec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(Q, K, V, mask=None):\n",
    "    \"\"\"\n",
    "    Compute scaled dot-product attention.\n",
    "    \n",
    "    Args:\n",
    "        Q: Query matrix (batch_size, seq_len_q, d_k)\n",
    "        K: Key matrix (batch_size, seq_len_k, d_k)\n",
    "        V: Value matrix (batch_size, seq_len_v, d_v)\n",
    "        mask: Optional mask (batch_size, seq_len_q, seq_len_k)\n",
    "    \n",
    "    Returns:\n",
    "        output: Attention output (batch_size, seq_len_q, d_v)\n",
    "        attention_weights: Attention weights (batch_size, seq_len_q, seq_len_k)\n",
    "    \"\"\"\n",
    "    d_k = Q.shape[-1]\n",
    "    \n",
    "    # Compute attention scores: Q @ K^T / sqrt(d_k)\n",
    "    scores = torch.matmul(Q, K.transpose(-2, -1)) / np.sqrt(d_k)\n",
    "    \n",
    "    # Apply mask if provided (set masked positions to -inf)\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "    \n",
    "    # Apply softmax to get attention weights\n",
    "    attention_weights = F.softmax(scores, dim=-1)\n",
    "    \n",
    "    # Compute weighted sum of values\n",
    "    output = torch.matmul(attention_weights, V)\n",
    "    \n",
    "    return output, attention_weights\n",
    "\n",
    "# Test with simple example\n",
    "batch_size, seq_len, d_model = 2, 4, 8\n",
    "Q = torch.randn(batch_size, seq_len, d_model)\n",
    "K = torch.randn(batch_size, seq_len, d_model)\n",
    "V = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "output, attn_weights = scaled_dot_product_attention(Q, K, V)\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Attention weights shape: {attn_weights.shape}\")\n",
    "print(f\"Attention weights sum (should be ~1.0): {attn_weights[0].sum(dim=-1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdae0475",
   "metadata": {},
   "source": [
    "## Exercise 2: Visualize Attention Weights\n",
    "\n",
    "Create a heatmap to understand where the model is \"looking\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5643480c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_attention(attention_weights, query_labels=None, key_labels=None):\n",
    "    \"\"\"\n",
    "    Visualize attention weights as a heatmap.\n",
    "    \n",
    "    Args:\n",
    "        attention_weights: (seq_len_q, seq_len_k) tensor\n",
    "        query_labels: Optional labels for query positions\n",
    "        key_labels: Optional labels for key positions\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(\n",
    "        attention_weights.detach().cpu().numpy(),\n",
    "        cmap='viridis',\n",
    "        annot=True,\n",
    "        fmt='.2f',\n",
    "        xticklabels=key_labels if key_labels else range(attention_weights.shape[1]),\n",
    "        yticklabels=query_labels if query_labels else range(attention_weights.shape[0]),\n",
    "        cbar_kws={'label': 'Attention Weight'}\n",
    "    )\n",
    "    plt.xlabel('Key Position')\n",
    "    plt.ylabel('Query Position')\n",
    "    plt.title('Attention Weights Heatmap')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Create a simple attention example with meaningful labels\n",
    "sentence = ['The', 'cat', 'sat', 'on', 'the', 'mat']\n",
    "seq_len = len(sentence)\n",
    "d_k = 16\n",
    "\n",
    "# Create queries and keys (one per word)\n",
    "Q_words = torch.randn(1, seq_len, d_k)\n",
    "K_words = torch.randn(1, seq_len, d_k)\n",
    "V_words = torch.randn(1, seq_len, d_k)\n",
    "\n",
    "_, attn_weights_words = scaled_dot_product_attention(Q_words, K_words, V_words)\n",
    "\n",
    "# Visualize attention for the first sequence\n",
    "visualize_attention(attn_weights_words[0], query_labels=sentence, key_labels=sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b25d384",
   "metadata": {},
   "source": [
    "## Exercise 3: Multi-Head Attention\n",
    "\n",
    "Implement multi-head attention to capture different types of relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d5677f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        \n",
    "        # Linear projections for Q, K, V\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        # Output projection\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "    \n",
    "    def split_heads(self, x):\n",
    "        \"\"\"Split last dimension into (num_heads, d_k)\"\"\"\n",
    "        batch_size, seq_len, d_model = x.shape\n",
    "        return x.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "    \n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        batch_size = Q.shape[0]\n",
    "        \n",
    "        # Linear projections\n",
    "        Q = self.W_q(Q)\n",
    "        K = self.W_k(K)\n",
    "        V = self.W_v(V)\n",
    "        \n",
    "        # Split into multiple heads: (batch_size, num_heads, seq_len, d_k)\n",
    "        Q = self.split_heads(Q)\n",
    "        K = self.split_heads(K)\n",
    "        V = self.split_heads(V)\n",
    "        \n",
    "        # Apply attention on each head\n",
    "        attn_output, attn_weights = scaled_dot_product_attention(Q, K, V, mask)\n",
    "        \n",
    "        # Concatenate heads: (batch_size, seq_len, d_model)\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
    "        \n",
    "        # Apply output projection\n",
    "        output = self.W_o(attn_output)\n",
    "        \n",
    "        return output, attn_weights\n",
    "\n",
    "# Test multi-head attention\n",
    "d_model = 64\n",
    "num_heads = 8\n",
    "batch_size, seq_len = 2, 10\n",
    "\n",
    "mha = MultiHeadAttention(d_model, num_heads)\n",
    "x = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "output, attn_weights = mha(x, x, x)\n",
    "print(f\"Multi-head attention output shape: {output.shape}\")\n",
    "print(f\"Attention weights shape: {attn_weights.shape}\")\n",
    "print(f\"Number of parameters: {sum(p.numel() for p in mha.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2b09c1",
   "metadata": {},
   "source": [
    "## Exercise 4: Simple Seq2Seq with Attention\n",
    "\n",
    "Build a sequence-to-sequence model with attention for a toy task (e.g., reversing sequences)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed39855",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqWithAttention(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # Encoder\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.encoder = nn.GRU(embed_dim, hidden_dim, batch_first=True)\n",
    "        \n",
    "        # Attention\n",
    "        self.attention = nn.Linear(hidden_dim * 2, 1)\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = nn.GRU(embed_dim + hidden_dim, hidden_dim, batch_first=True)\n",
    "        self.output_layer = nn.Linear(hidden_dim, vocab_size)\n",
    "    \n",
    "    def forward(self, src, tgt):\n",
    "        # Encode source sequence\n",
    "        src_embed = self.embedding(src)\n",
    "        encoder_outputs, hidden = self.encoder(src_embed)\n",
    "        \n",
    "        # Decode with attention\n",
    "        tgt_embed = self.embedding(tgt)\n",
    "        batch_size, tgt_len, _ = tgt_embed.shape\n",
    "        \n",
    "        outputs = []\n",
    "        for t in range(tgt_len):\n",
    "            # Compute attention weights\n",
    "            hidden_expanded = hidden.transpose(0, 1).expand(-1, encoder_outputs.shape[1], -1)\n",
    "            attn_input = torch.cat([hidden_expanded, encoder_outputs], dim=-1)\n",
    "            attn_scores = self.attention(attn_input).squeeze(-1)\n",
    "            attn_weights = F.softmax(attn_scores, dim=-1)\n",
    "            \n",
    "            # Compute context vector\n",
    "            context = torch.bmm(attn_weights.unsqueeze(1), encoder_outputs).squeeze(1)\n",
    "            \n",
    "            # Decoder step\n",
    "            decoder_input = torch.cat([tgt_embed[:, t:t+1, :], context.unsqueeze(1)], dim=-1)\n",
    "            decoder_output, hidden = self.decoder(decoder_input, hidden)\n",
    "            \n",
    "            # Predict output\n",
    "            output = self.output_layer(decoder_output)\n",
    "            outputs.append(output)\n",
    "        \n",
    "        return torch.cat(outputs, dim=1)\n",
    "\n",
    "# Create toy dataset: reverse sequences\n",
    "vocab_size = 20\n",
    "seq_len = 8\n",
    "num_samples = 1000\n",
    "\n",
    "# Generate random sequences and their reverses\n",
    "src_data = torch.randint(1, vocab_size, (num_samples, seq_len))\n",
    "tgt_data = torch.flip(src_data, dims=[1])\n",
    "\n",
    "CACHE_KEY = \"seq2seq_attention_5epochs\"\n",
    "\n",
    "_state = load_model_state(CACHE_KEY)\n",
    "if _state is not None:\n",
    "    model = Seq2SeqWithAttention(vocab_size, embed_dim=32, hidden_dim=64)\n",
    "    model.load_state_dict(_state)\n",
    "    history = load_result(CACHE_KEY + \"_history\")\n",
    "    print(\"[cache] Seq2Seq model and history loaded — skipping training.\")\n",
    "else:\n",
    "    model = Seq2SeqWithAttention(vocab_size, embed_dim=32, hidden_dim=64)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    dataset = TensorDataset(src_data, tgt_data)\n",
    "    dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "    history = []\n",
    "    model.train()\n",
    "    for epoch in range(5):\n",
    "        total_loss = 0\n",
    "        for src_batch, tgt_batch in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(src_batch, tgt_batch)\n",
    "            loss = criterion(output.view(-1, vocab_size), tgt_batch.view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        history.append(avg_loss)\n",
    "        print(f\"Epoch {epoch+1}/5, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    save_model(model, CACHE_KEY)\n",
    "    save_result(CACHE_KEY + \"_history\", history)\n",
    "\n",
    "# Test on a few examples\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_src = src_data[:3]\n",
    "    test_tgt = tgt_data[:3]\n",
    "    predictions = model(test_src, test_tgt)\n",
    "    predicted_tokens = predictions.argmax(dim=-1)\n",
    "    \n",
    "    for i in range(3):\n",
    "        print(f\"\\nSource:    {test_src[i].tolist()}\")\n",
    "        print(f\"Target:    {test_tgt[i].tolist()}\")\n",
    "        print(f\"Predicted: {predicted_tokens[i].tolist()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9b19d7",
   "metadata": {},
   "source": [
    "## Further Practice\n",
    "\n",
    "1. **Causal Masking**: Implement a causal mask for autoregressive generation (prevent attending to future tokens)\n",
    "2. **Cross-Attention**: Modify the implementation to support cross-attention (different Q vs K,V sources)\n",
    "3. **Attention Patterns**: Visualize attention weights across different heads in multi-head attention\n",
    "4. **Complexity Analysis**: Measure time/memory for different sequence lengths and attention dimensionalities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01443f42",
   "metadata": {},
   "source": [
    "## Deliverables\n",
    "\n",
    "- [ ] Implement scaled dot-product attention from scratch\n",
    "- [ ] Visualize attention weights as heatmaps\n",
    "- [ ] Build multi-head attention module\n",
    "- [ ] Train seq2seq model with attention on toy task\n",
    "- [ ] Compare attention vs non-attention seq2seq performance\n",
    "- [ ] Analyze attention patterns on several examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7add0589",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# EXERCISE 1 — Causal (Auto-Regressive) Mask\n",
    "# Goal: implement a causal mask so each position can only attend to itself and earlier positions.\n",
    "# Apply it to scaled_dot_product_attention and verify by checking that the upper triangle\n",
    "# of attn_weights is zero (no future leakage).\n",
    "# Use cached(\"causal_attention_demo\", ...) to store the resulting weight matrix for quick reload.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24269a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# EXERCISE 2 — Cross-Attention (Encoder–Decoder)\n",
    "# Goal: build a cross-attention layer where Q comes from the decoder hidden state\n",
    "# and K, V come from the encoder output (different source from Q).\n",
    "# Verify shapes for sequences of different lengths (src_len ≠ tgt_len).\n",
    "# Visualize cross-attention weights as a heatmap with tokens on both axes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442422f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# EXERCISE 3 — Multi-Head Attention Pattern Analysis\n",
    "# Goal: run multi-head attention on a real sentence (tokenized as integer IDs, length ~12).\n",
    "# For each of the 8 heads, plot the attention heatmap using visualize_attention().\n",
    "# Identify which heads attend locally (nearby tokens) vs globally (all positions equally).\n",
    "# Use cached(\"multihead_patterns_8heads\", ...) to store the list of per-head weight matrices.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0074d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# EXERCISE 4 — Attention vs No-Attention Seq2Seq Comparison\n",
    "# Goal: build a vanilla GRU Seq2Seq (no attention) and train it on the same reverse-sequence task.\n",
    "# Compare validation accuracy against the attention model after 5 epochs.\n",
    "# Use cached(\"seq2seq_no_attention_5epochs\", ...) and cached(\"seq2seq_attention_5epochs_history\", ...)\n",
    "# to store each training history, then plot both loss curves side-by-side.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a5ed86",
   "metadata": {},
   "source": [
    "## Recommended Resources\n",
    "\n",
    "- [Attention Is All You Need](https://arxiv.org/abs/1706.03762) (original Transformer paper)\n",
    "- [The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/) (visual guide)\n",
    "- [Attention and Augmented Recurrent Neural Networks](https://distill.pub/2016/augmented-rnns/) (Distill article)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
