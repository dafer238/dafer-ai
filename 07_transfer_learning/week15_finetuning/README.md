Week 15 â€” Fine-Tuning & Transfer Learning

Overview
Study transfer learning strategies, when to fine-tune vs use feature extractors, and lightweight methods like adapters.

Study
- Transfer learning paradigms; domain gap and covariate shift
- Adapters and parameter-efficient fine-tuning

Practical libraries & tools
- HuggingFace Transformers for NLP; PyTorch for vision transfer

Datasets & examples
- Small target-domain datasets; use pretrained checkpoints and adapt

Exercises
1) Fine-tune a pretrained model on a small downstream task and report metrics.

2) Compare feature-extraction vs full fine-tuning across limited-data regimes.

3) Try adapter or partial-freeze strategies and compare parameter efficiency.

Deliverable
- Notebook with fine-tuning experiments, lessons learned, and suggested workflow for your capstone.
