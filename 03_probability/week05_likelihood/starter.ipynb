{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "482f003d",
   "metadata": {},
   "source": [
    "# Week 05 — Probability & Noise (Likelihood)\n",
    "\n",
    "This notebook ties loss-based ML to probabilistic modeling through maximum likelihood estimation. You'll:\n",
    "- Understand how likelihood assumptions lead to common loss functions\n",
    "- Implement MLE for different noise models\n",
    "- Explore robust alternatives to Gaussian assumptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aadb6b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from scipy.optimize import minimize\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(42)\n",
    "sns.set_style('whitegrid')\n",
    "print(\"Libraries imported!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1030b489",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, pickle\n",
    "\n",
    "CACHE_DIR = \"cache_week05\"\n",
    "os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "\n",
    "def save_result(key, obj):\n",
    "    with open(os.path.join(CACHE_DIR, f\"{key}.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(obj, f)\n",
    "\n",
    "def load_result(key):\n",
    "    path = os.path.join(CACHE_DIR, f\"{key}.pkl\")\n",
    "    if os.path.exists(path):\n",
    "        with open(path, \"rb\") as f:\n",
    "            return pickle.load(f)\n",
    "    return None\n",
    "\n",
    "def cached(key, compute_fn):\n",
    "    result = load_result(key)\n",
    "    if result is not None:\n",
    "        print(f\"[cache] loaded '{key}'\")\n",
    "        return result\n",
    "    print(f\"[cache] computing '{key}'...\")\n",
    "    result = compute_fn()\n",
    "    save_result(key, result)\n",
    "    return result\n",
    "\n",
    "print(\"Cache utilities ready. Results will be stored in:\", CACHE_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f13a7b7c",
   "metadata": {},
   "source": [
    "## 1. Maximum Likelihood Estimation (MLE) for Gaussian Linear Regression\n",
    "\n",
    "Derive and implement MLE, showing the connection between negative log-likelihood and mean squared error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca24b2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from scipy.optimize import minimize\n",
    "from scipy import stats\n",
    "\n",
    "n_samples = 100\n",
    "X = np.random.randn(n_samples, 1) * 2\n",
    "w_true = np.array([2.0, 3.5])\n",
    "sigma_true = 1.0\n",
    "\n",
    "X_with_bias = np.hstack([np.ones((n_samples, 1)), X])\n",
    "y_true = X_with_bias @ w_true\n",
    "y = y_true + np.random.randn(n_samples) * sigma_true\n",
    "\n",
    "def neg_log_likelihood_gaussian(params, X, y):\n",
    "    n = len(y)\n",
    "    w = params[:-1]; sigma = params[-1]\n",
    "    if sigma <= 0: return 1e10\n",
    "    mu = X @ w; residuals = y - mu\n",
    "    return 0.5 * n * np.log(2 * np.pi * sigma**2) + np.sum(residuals**2) / (2 * sigma**2)\n",
    "\n",
    "def _fit_gaussian_mle():\n",
    "    init = np.array([0.0, 0.0, 1.0])\n",
    "    result = minimize(neg_log_likelihood_gaussian, init, args=(X_with_bias, y), method='L-BFGS-B')\n",
    "    return result\n",
    "\n",
    "result = cached(\"gaussian_mle_linear_regression\", _fit_gaussian_mle)\n",
    "w_mle = result.x[:-1]; sigma_mle = result.x[-1]\n",
    "\n",
    "print(\"True parameters: w0={:.2f}, w1={:.2f}, sigma={:.2f}\".format(*w_true, sigma_true))\n",
    "print(\"MLE parameters:  w0={:.2f}, w1={:.2f}, sigma={:.2f}\".format(*w_mle, sigma_mle))\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.scatter(X, y, alpha=0.6, label='Data')\n",
    "X_line = np.linspace(X.min(), X.max(), 100).reshape(-1, 1)\n",
    "X_line_bias = np.hstack([np.ones((100, 1)), X_line])\n",
    "y_pred_mle  = X_line_bias @ w_mle\n",
    "y_pred_true = X_line_bias @ w_true\n",
    "plt.plot(X_line, y_pred_true, 'g--', linewidth=2, label='True model')\n",
    "plt.plot(X_line, y_pred_mle,  'r-',  linewidth=2, label='MLE fit')\n",
    "plt.fill_between(X_line.ravel(), y_pred_mle - 2*sigma_mle, y_pred_mle + 2*sigma_mle,\n",
    "                 alpha=0.2, color='red', label='±2σ')\n",
    "plt.xlabel('x'); plt.ylabel('y'); plt.title('Maximum Likelihood Estimation')\n",
    "plt.legend(); plt.grid(alpha=0.3); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413123a0",
   "metadata": {},
   "source": [
    "## 2. Connection: NLL and MSE\n",
    "\n",
    "Show that minimizing negative log-likelihood with Gaussian noise is equivalent to minimizing MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a18a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Gaussian noise with fixed sigma, NLL ∝ MSE\n",
    "# Let's verify this by computing both\n",
    "\n",
    "def mse_loss(w, X, y):\n",
    "    \"\"\"Mean Squared Error\"\"\"\n",
    "    mu = X @ w\n",
    "    return np.mean((y - mu)**2)\n",
    "\n",
    "# Grid search over w1 (fixing w0 and sigma)\n",
    "w0_fixed = w_mle[0]\n",
    "sigma_fixed = sigma_mle\n",
    "w1_range = np.linspace(0, 6, 100)\n",
    "\n",
    "nll_values = []\n",
    "mse_values = []\n",
    "\n",
    "for w1 in w1_range:\n",
    "    params = np.array([w0_fixed, w1, sigma_fixed])\n",
    "    nll = neg_log_likelihood_gaussian(params, X_with_bias, y)\n",
    "    nll_values.append(nll)\n",
    "    \n",
    "    mse = mse_loss(np.array([w0_fixed, w1]), X_with_bias, y)\n",
    "    mse_values.append(mse)\n",
    "\n",
    "# Normalize for comparison\n",
    "nll_normalized = (nll_values - np.min(nll_values)) / (np.max(nll_values) - np.min(nll_values))\n",
    "mse_normalized = (mse_values - np.min(mse_values)) / (np.max(mse_values) - np.min(mse_values))\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(w1_range, nll_normalized, 'b-', linewidth=2, label='Normalized NLL')\n",
    "plt.plot(w1_range, mse_normalized, 'r--', linewidth=2, label='Normalized MSE')\n",
    "plt.axvline(w_true[1], color='g', linestyle=':', linewidth=2, label='True w1')\n",
    "plt.axvline(w_mle[1], color='orange', linestyle=':', linewidth=2, label='MLE w1')\n",
    "plt.xlabel('w1 (slope parameter)')\n",
    "plt.ylabel('Normalized Loss')\n",
    "plt.title('Negative Log-Likelihood vs MSE\\n(Curves overlap when noise is Gaussian!)')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"Correlation between NLL and MSE:\", np.corrcoef(nll_values, mse_values)[0, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040d9602",
   "metadata": {},
   "source": [
    "## 3. Robust Regression: Laplace and Student-t Noise\n",
    "\n",
    "Explore robust alternatives to Gaussian noise that are less sensitive to outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4bdebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "np.random.seed(42)\n",
    "X_robust = np.random.randn(80, 1) * 2\n",
    "X_robust_bias = np.hstack([np.ones((80, 1)), X_robust])\n",
    "y_robust = X_robust_bias @ w_true + np.random.randn(80) * 0.5\n",
    "outlier_idx = [10, 25, 40, 55, 70]\n",
    "y_robust[outlier_idx] += np.random.choice([-5, 5], size=len(outlier_idx))\n",
    "\n",
    "def neg_log_likelihood_laplace(params, X, y):\n",
    "    w = params[:-1]; b = params[-1]\n",
    "    if b <= 0: return 1e10\n",
    "    return len(y) * np.log(2 * b) + np.sum(np.abs(y - X @ w)) / b\n",
    "\n",
    "def neg_log_likelihood_t(params, X, y, df=3):\n",
    "    w = params[:-1]; scale = params[-1]\n",
    "    if scale <= 0: return 1e10\n",
    "    return -np.sum(stats.t.logpdf((y - X @ w) / scale, df=df)) + len(y) * np.log(scale)\n",
    "\n",
    "def _fit_robust():\n",
    "    init = np.array([0.0, 0.0, 1.0])\n",
    "    rg = minimize(neg_log_likelihood_gaussian, init, args=(X_robust_bias, y_robust), method='L-BFGS-B')\n",
    "    rl = minimize(neg_log_likelihood_laplace,  init, args=(X_robust_bias, y_robust), method='L-BFGS-B')\n",
    "    rt = minimize(neg_log_likelihood_t,        init, args=(X_robust_bias, y_robust), method='L-BFGS-B')\n",
    "    return rg, rl, rt\n",
    "\n",
    "result_gaussian, result_laplace, result_t = cached(\"robust_regression_comparison\", _fit_robust)\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.scatter(X_robust, y_robust, alpha=0.6, s=50, label='Data')\n",
    "plt.scatter(X_robust[outlier_idx], y_robust[outlier_idx], color='red', s=100,\n",
    "            marker='x', linewidths=3, label='Outliers')\n",
    "X_line = np.linspace(X_robust.min(), X_robust.max(), 100).reshape(-1, 1)\n",
    "X_line_bias = np.hstack([np.ones((100, 1)), X_line])\n",
    "plt.plot(X_line, X_line_bias @ w_true,                 'g--',    linewidth=2, label='True model',      alpha=0.7)\n",
    "plt.plot(X_line, X_line_bias @ result_gaussian.x[:-1], 'b-',     linewidth=2, label='Gaussian MLE')\n",
    "plt.plot(X_line, X_line_bias @ result_laplace.x[:-1],  '-',      linewidth=2, color='orange', label='Laplace MLE (L1)')\n",
    "plt.plot(X_line, X_line_bias @ result_t.x[:-1],        'purple', linewidth=2, label='Student-t MLE')\n",
    "plt.xlabel('x'); plt.ylabel('y')\n",
    "plt.title('Robust Regression: Gaussian vs Laplace vs Student-t')\n",
    "plt.legend(); plt.grid(alpha=0.3); plt.show()\n",
    "print(f\"Gaussian: w0={result_gaussian.x[0]:.2f}, w1={result_gaussian.x[1]:.2f}\")\n",
    "print(f\"Laplace:  w0={result_laplace.x[0]:.2f},  w1={result_laplace.x[1]:.2f}\")\n",
    "print(f\"Student-t: w0={result_t.x[0]:.2f}, w1={result_t.x[1]:.2f}\")\n",
    "print(f\"True:     w0={w_true[0]:.2f}, w1={w_true[1]:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60a00d7",
   "metadata": {},
   "source": [
    "## 4. Likelihood Surfaces and Multimodality\n",
    "\n",
    "Visualize likelihood as a function of parameters to understand optimization landscapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b302411d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _compute_nll_surface():\n",
    "    w0_range = np.linspace(0, 4, 50)\n",
    "    w1_range = np.linspace(1, 6, 50)\n",
    "    W0, W1 = np.meshgrid(w0_range, w1_range)\n",
    "    NLL = np.zeros_like(W0)\n",
    "    sigma_fixed = 1.0\n",
    "    for i in range(len(w0_range)):\n",
    "        for j in range(len(w1_range)):\n",
    "            params = np.array([W0[j, i], W1[j, i], sigma_fixed])\n",
    "            NLL[j, i] = neg_log_likelihood_gaussian(params, X_with_bias, y)\n",
    "    return W0, W1, NLL\n",
    "\n",
    "W0, W1, NLL = cached(\"nll_surface_50x50\", _compute_nll_surface)\n",
    "\n",
    "sigma_fixed = 1.0\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "contour = plt.contour(W0, W1, NLL, levels=30, cmap='viridis')\n",
    "plt.colorbar(contour, label='Negative Log-Likelihood')\n",
    "plt.plot(w_mle[0], w_mle[1], 'r*', markersize=20, label='MLE')\n",
    "plt.plot(w_true[0], w_true[1], 'g*', markersize=20, label='True')\n",
    "plt.xlabel('w0 (intercept)'); plt.ylabel('w1 (slope)')\n",
    "plt.title('Likelihood Surface (Contours)'); plt.legend(); plt.grid(alpha=0.3)\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "ax = plt.subplot(1, 2, 2, projection='3d')\n",
    "ax.plot_surface(W0, W1, NLL, cmap='viridis', alpha=0.6, edgecolor='none')\n",
    "ax.scatter([w_mle[0]], [w_mle[1]],\n",
    "           [neg_log_likelihood_gaussian(np.concatenate([w_mle, [sigma_fixed]]), X_with_bias, y)],\n",
    "           color='red', s=100)\n",
    "ax.set_xlabel('w0'); ax.set_ylabel('w1'); ax.set_zlabel('NLL')\n",
    "ax.set_title('Likelihood Surface (3D)')\n",
    "plt.tight_layout(); plt.show()\n",
    "print(\"The likelihood surface is convex for Gaussian linear regression.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6deb8b",
   "metadata": {},
   "source": [
    "## Exercises for Further Practice\n",
    "\n",
    "1. **Poisson Regression**: Implement MLE for count data using Poisson likelihood\n",
    "2. **Bayesian Linear Regression**: Add priors and compute MAP estimates\n",
    "3. **Heteroscedastic Noise**: Model variance as a function of inputs\n",
    "4. **AIC/BIC**: Implement model selection using information criteria\n",
    "5. **Real Data**: Apply different noise models to UCI regression datasets\n",
    "\n",
    "## Deliverables Checklist\n",
    "\n",
    "- [ ] MLE derivation and implementation for Gaussian linear regression\n",
    "- [ ] Demonstration of NLL ≈ MSE connection\n",
    "- [ ] Robust regression comparison (Gaussian vs Laplace vs Student-t)\n",
    "- [ ] Likelihood surface visualization with interpretation\n",
    "\n",
    "## Recommended Resources\n",
    "\n",
    "- Murphy, \"Machine Learning: A Probabilistic Perspective\" (MLE chapter)\n",
    "- Bishop, \"Pattern Recognition and Machine Learning\" (Probability chapter)\n",
    "- SciPy documentation on probability distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c295ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# EXERCISE 1 — Poisson Regression (Count Data)\n",
    "# Goal: generate synthetic count data y ~ Poisson(λ=exp(w0 + w1*x)).\n",
    "# Define the Poisson NLL and optimize it with scipy.minimize.\n",
    "# Use cached(\"poisson_regression_mle\", ...) to save the optimized parameters.\n",
    "# Expected insight: Poisson NLL is the foundation of log-linear models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97bd77b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# EXERCISE 2 — MAP Estimation (Bayesian Linear Regression)\n",
    "# Goal: add an L2 prior p(w) = N(0, σ_prior^2 I) to the Gaussian NLL.\n",
    "# The MAP objective is: NLL + λ||w||^2 (same as Ridge!).\n",
    "# Derive and implement this, then show that MAP with σ_prior=1 matches Ridge(alpha=σ^2/σ_prior^2).\n",
    "# Expected insight: Bayesian MAP = regularized MLE; the prior IS the regularizer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065e6eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# EXERCISE 4 — Heteroscedastic Noise\n",
    "# Goal: generate data where σ(x) = σ_0 * exp(0.3 * x), so variance grows with x.\n",
    "# Model both μ(x) = w0 + w1*x and log(σ(x)) = a0 + a1*x simultaneously using MLE.\n",
    "# Compare fit vs standard homoscedastic regression.\n",
    "# Expected insight: misspecifying the noise model leads to poor uncertainty estimates.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
