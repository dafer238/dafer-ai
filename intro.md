# AI & ML — From Foundations to Deployment

A 21-week self-study course that builds machine-learning expertise from first principles through to production deployment. Every week pairs a **theory page** (rigorous explanations, derivations, and notation) with a **hands-on notebook** (coding exercises that put the theory into practice).

---

## Part I · Introduction (Weeks 0a–0b)

| Week | Topic                  | Key ideas                                                                                                      |
| ---- | ---------------------- | -------------------------------------------------------------------------------------------------------------- |
| 0a   | **The AI Landscape**   | Supervised / unsupervised / reinforcement learning, the training loop, evaluation metrics, the modern AI stack |
| 0b   | **Mathematics & Data** | Linear algebra, calculus & gradients, probability & statistics, data representation, NumPy essentials          |

## Part II · Fundamentals (Weeks 1–6)

| Week | Topic                        | Key ideas                                                                              |
| ---- | ---------------------------- | -------------------------------------------------------------------------------------- |
| 1    | **Optimisation**             | Loss landscapes, gradient descent, learning rates, SGD, momentum                       |
| 2    | **Advanced Optimisers**      | EMA, AdaGrad, RMSProp, Adam / AdamW, learning-rate schedules                           |
| 3    | **Linear Models**            | Linear & logistic regression, closed-form vs. iterative solutions, decision boundaries |
| 4    | **Dimensionality Reduction** | PCA, eigenvalue spectrum, reconstruction error, t-SNE, UMAP                            |
| 5    | **Clustering**               | K-means, hierarchical clustering, DBSCAN, Gaussian mixtures, evaluation metrics        |
| 6    | **Regularisation**           | Bias–variance trade-off, L1 / L2 penalties, early stopping, cross-validation           |

## Part III · Probability (Weeks 7–10)

| Week | Topic                | Key ideas                                                                                  |
| ---- | -------------------- | ------------------------------------------------------------------------------------------ |
| 7    | **Likelihood**       | Maximum likelihood estimation, log-likelihood, MLE for Gaussian and Bernoulli models       |
| 8    | **Uncertainty**      | Aleatoric vs. epistemic uncertainty, Bayesian inference, posterior predictive, calibration |
| 9    | **Time Series**      | Trend, seasonality, stationarity, AR / MA / ARIMA, forecasting evaluation                  |
| 10   | **Surrogate Models** | Gaussian processes, acquisition functions, Bayesian optimisation                           |

## Part IV · Neural Networks (Weeks 11–12)

| Week | Topic                            | Key ideas                                                                                      |
| ---- | -------------------------------- | ---------------------------------------------------------------------------------------------- |
| 11   | **Neural Networks from Scratch** | Perceptron, multi-layer networks, activation functions, backpropagation, weight initialisation |
| 12   | **Training Pathologies**         | Vanishing / exploding gradients, dead neurons, batch normalisation, residual connections       |

## Part V · Deep Learning (Weeks 13–16)

| Week | Topic                            | Key ideas                                                                                     |
| ---- | -------------------------------- | --------------------------------------------------------------------------------------------- |
| 13   | **PyTorch Basics**               | Tensors, autograd, `nn.Module`, `DataLoader`, training loop in PyTorch                        |
| 14   | **Training at Scale**            | Mixed precision, gradient accumulation, distributed training, checkpointing                   |
| 15   | **CNN Representations**          | Convolutions, pooling, feature maps, receptive fields, classic architectures (LeNet → ResNet) |
| 16   | **Deep Learning Regularisation** | Dropout, data augmentation, weight decay in deep nets, batch / layer normalisation            |

## Part VI · Sequence Models (Weeks 17–18)

| Week | Topic            | Key ideas                                                                             |
| ---- | ---------------- | ------------------------------------------------------------------------------------- |
| 17   | **Attention**    | Seq2seq limitations, additive & dot-product attention, self-attention                 |
| 18   | **Transformers** | Positional encoding, multi-head attention, encoder–decoder architecture, pre-training |

## Part VII · Transfer Learning (Week 19)

| Week | Topic           | Key ideas                                                                            |
| ---- | --------------- | ------------------------------------------------------------------------------------ |
| 19   | **Fine-Tuning** | Feature extraction vs. full fine-tuning, learning-rate strategies, domain adaptation |

## Part VIII · Deployment (Week 20)

| Week | Topic          | Key ideas                                                                          |
| ---- | -------------- | ---------------------------------------------------------------------------------- |
| 20   | **Deployment** | Model serialisation (ONNX, TorchScript), serving APIs, monitoring, edge deployment |

---

## How to use this book

Use the **sidebar** to navigate between weeks, or the **← → arrow buttons** at the bottom of each page to move sequentially. Use the **search box** at the top of the sidebar to filter pages by name, or the **concept search** to look up any term across all content.
