{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90234632",
   "metadata": {},
   "source": [
    "# Week 14: Transformers\n",
    "\n",
    "This notebook builds a complete transformer encoder from scratch. You'll implement positional encodings, multi-head self-attention, feed-forward networks, and residual connections to create a production-quality transformer architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301c010b",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e49c734",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import time\n",
    "import seaborn as sns\n",
    "\n",
    "# Reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Display configuration\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9662d19a",
   "metadata": {},
   "source": [
    "## Exercise 1: Positional Encoding\n",
    "\n",
    "Implement sinusoidal positional encodings to inject sequence order information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963197a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Create matrix of shape (max_len, d_model)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
    "        \n",
    "        # Apply sin to even indices, cos to odd indices\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        # Add batch dimension: (1, max_len, d_model)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        \n",
    "        # Register as buffer (not a parameter, but part of state)\n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (batch_size, seq_len, d_model)\n",
    "        Returns:\n",
    "            x + positional encoding\n",
    "        \"\"\"\n",
    "        return x + self.pe[:, :x.size(1), :]\n",
    "\n",
    "# Visualize positional encodings\n",
    "d_model = 128\n",
    "max_len = 100\n",
    "pos_encoder = PositionalEncoding(d_model, max_len)\n",
    "\n",
    "# Get positional encodings for visualization\n",
    "dummy_input = torch.zeros(1, max_len, d_model)\n",
    "pe_values = pos_encoder(dummy_input)[0].numpy()\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.imshow(pe_values.T, cmap='RdBu', aspect='auto', vmin=-1, vmax=1)\n",
    "plt.colorbar(label='Encoding Value')\n",
    "plt.xlabel('Position')\n",
    "plt.ylabel('Dimension')\n",
    "plt.title('Positional Encoding Heatmap (Sinusoidal)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot a few dimensions over positions\n",
    "plt.figure(figsize=(14, 6))\n",
    "for dim in [0, 1, 10, 20, 50, 100]:\n",
    "    plt.plot(pe_values[:, dim], label=f'dim {dim}')\n",
    "plt.xlabel('Position')\n",
    "plt.ylabel('Encoding Value')\n",
    "plt.title('Positional Encoding: Selected Dimensions')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf49169",
   "metadata": {},
   "source": [
    "## Exercise 2: Transformer Encoder Layer\n",
    "\n",
    "Build a single transformer encoder layer with multi-head attention, feed-forward network, and residual connections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0184cbe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Multi-head self-attention\n",
    "        self.self_attn = nn.MultiheadAttention(d_model, num_heads, dropout=dropout, batch_first=True)\n",
    "        \n",
    "        # Feed-forward network\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_ff, d_model)\n",
    "        )\n",
    "        \n",
    "        # Layer normalization\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        # Multi-head attention with residual connection\n",
    "        attn_output, attn_weights = self.self_attn(x, x, x, attn_mask=mask)\n",
    "        x = self.norm1(x + self.dropout1(attn_output))\n",
    "        \n",
    "        # Feed-forward with residual connection\n",
    "        ffn_output = self.ffn(x)\n",
    "        x = self.norm2(x + self.dropout2(ffn_output))\n",
    "        \n",
    "        return x, attn_weights\n",
    "\n",
    "# Test the encoder layer\n",
    "d_model = 128\n",
    "num_heads = 8\n",
    "d_ff = 512\n",
    "batch_size, seq_len = 4, 20\n",
    "\n",
    "encoder_layer = TransformerEncoderLayer(d_model, num_heads, d_ff)\n",
    "x = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "output, attn_weights = encoder_layer(x)\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Attention weights shape: {attn_weights.shape}\")\n",
    "print(f\"Number of parameters: {sum(p.numel() for p in encoder_layer.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce95c1b0",
   "metadata": {},
   "source": [
    "## Exercise 3: Complete Transformer Encoder\n",
    "\n",
    "Stack multiple encoder layers with positional encoding and input embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30941dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, num_heads, d_ff, num_layers, max_len=5000, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # Input embedding\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        \n",
    "        # Positional encoding\n",
    "        self.pos_encoding = PositionalEncoding(d_model, max_len)\n",
    "        \n",
    "        # Stack of encoder layers\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerEncoderLayer(d_model, num_heads, d_ff, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # Dropout after embedding\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        # Embedding + positional encoding\n",
    "        x = self.embedding(x) * np.sqrt(self.d_model)  # Scale embeddings\n",
    "        x = self.pos_encoding(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Pass through encoder layers\n",
    "        attn_weights_list = []\n",
    "        for layer in self.layers:\n",
    "            x, attn_weights = layer(x, mask)\n",
    "            attn_weights_list.append(attn_weights)\n",
    "        \n",
    "        return x, attn_weights_list\n",
    "\n",
    "# Create a full transformer encoder\n",
    "vocab_size = 1000\n",
    "d_model = 128\n",
    "num_heads = 8\n",
    "d_ff = 512\n",
    "num_layers = 6\n",
    "\n",
    "transformer = TransformerEncoder(vocab_size, d_model, num_heads, d_ff, num_layers).to(device)\n",
    "\n",
    "# Test with random input\n",
    "batch_size, seq_len = 8, 25\n",
    "input_ids = torch.randint(0, vocab_size, (batch_size, seq_len)).to(device)\n",
    "\n",
    "output, attn_weights_list = transformer(input_ids)\n",
    "print(f\"Input IDs shape: {input_ids.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Number of attention weight tensors: {len(attn_weights_list)}\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in transformer.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349fd2ea",
   "metadata": {},
   "source": [
    "## Exercise 4: Sequence Classification Task\n",
    "\n",
    "Use the transformer encoder for a classification task (e.g., sentiment analysis proxy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3cdc1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, num_heads, d_ff, num_layers, num_classes, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.transformer = TransformerEncoder(vocab_size, d_model, num_heads, d_ff, num_layers, dropout=dropout)\n",
    "        \n",
    "        # Classification head\n",
    "        self.classifier = nn.Linear(d_model, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Get transformer output\n",
    "        encoder_output, _ = self.transformer(x)\n",
    "        \n",
    "        # Pool: use [CLS] token (first position) or mean pooling\n",
    "        pooled = encoder_output.mean(dim=1)  # Mean pooling\n",
    "        \n",
    "        # Classify\n",
    "        logits = self.classifier(pooled)\n",
    "        return logits\n",
    "\n",
    "# Create synthetic classification dataset\n",
    "num_samples = 5000\n",
    "seq_len = 30\n",
    "vocab_size = 500\n",
    "num_classes = 3\n",
    "\n",
    "# Random sequences and labels\n",
    "X = torch.randint(0, vocab_size, (num_samples, seq_len))\n",
    "y = torch.randint(0, num_classes, (num_samples,))\n",
    "\n",
    "# Split into train/val\n",
    "split = int(0.8 * num_samples)\n",
    "X_train, X_val = X[:split], X[split:]\n",
    "y_train, y_val = y[:split], y[split:]\n",
    "\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "val_dataset = TensorDataset(X_val, y_val)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64)\n",
    "\n",
    "# Create model\n",
    "model = TransformerClassifier(\n",
    "    vocab_size=vocab_size,\n",
    "    d_model=64,\n",
    "    num_heads=4,\n",
    "    d_ff=256,\n",
    "    num_layers=3,\n",
    "    num_classes=num_classes,\n",
    "    dropout=0.1\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 5\n",
    "train_losses, val_accuracies = [], []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Train\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        logits = model(batch_X)\n",
    "        loss = criterion(logits, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "    \n",
    "    # Validate\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_y in val_loader:\n",
    "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "            logits = model(batch_X)\n",
    "            preds = logits.argmax(dim=1)\n",
    "            correct += (preds == batch_y).sum().item()\n",
    "            total += batch_y.size(0)\n",
    "    \n",
    "    val_acc = correct / total\n",
    "    val_accuracies.append(val_acc)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - Train Loss: {avg_train_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "# Plot training curves\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "ax1.plot(train_losses, marker='o')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Train Loss')\n",
    "ax1.set_title('Training Loss')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "ax2.plot(val_accuracies, marker='o', color='green')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Validation Accuracy')\n",
    "ax2.set_title('Validation Accuracy')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efba5d47",
   "metadata": {},
   "source": [
    "## Exercise 5: Computational Profiling\n",
    "\n",
    "Measure memory usage and inference time for different transformer configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e6c3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def profile_transformer(d_model, num_heads, num_layers, seq_len, batch_size=16):\n",
    "    \"\"\"\n",
    "    Profile transformer performance.\n",
    "    \"\"\"\n",
    "    model = TransformerEncoder(\n",
    "        vocab_size=1000,\n",
    "        d_model=d_model,\n",
    "        num_heads=num_heads,\n",
    "        d_ff=d_model * 4,\n",
    "        num_layers=num_layers\n",
    "    ).to(device)\n",
    "    \n",
    "    # Count parameters\n",
    "    num_params = sum(p.numel() for p in model.parameters())\n",
    "    \n",
    "    # Create input\n",
    "    x = torch.randint(0, 1000, (batch_size, seq_len)).to(device)\n",
    "    \n",
    "    # Warm-up\n",
    "    for _ in range(3):\n",
    "        _ = model(x)\n",
    "    \n",
    "    # Measure inference time\n",
    "    if device.type == 'cuda':\n",
    "        torch.cuda.synchronize()\n",
    "    \n",
    "    start = time.time()\n",
    "    num_repeats = 50\n",
    "    for _ in range(num_repeats):\n",
    "        _ = model(x)\n",
    "    \n",
    "    if device.type == 'cuda':\n",
    "        torch.cuda.synchronize()\n",
    "    \n",
    "    elapsed = (time.time() - start) / num_repeats\n",
    "    \n",
    "    return {\n",
    "        'num_params': num_params,\n",
    "        'inference_time_ms': elapsed * 1000\n",
    "    }\n",
    "\n",
    "# Profile different configurations\n",
    "configs = [\n",
    "    {'d_model': 64, 'num_heads': 4, 'num_layers': 2, 'seq_len': 50},\n",
    "    {'d_model': 128, 'num_heads': 8, 'num_layers': 4, 'seq_len': 50},\n",
    "    {'d_model': 256, 'num_heads': 8, 'num_layers': 6, 'seq_len': 50},\n",
    "    {'d_model': 128, 'num_heads': 8, 'num_layers': 4, 'seq_len': 100},\n",
    "    {'d_model': 128, 'num_heads': 8, 'num_layers': 4, 'seq_len': 200},\n",
    "]\n",
    "\n",
    "results = []\n",
    "for config in configs:\n",
    "    print(f\"Profiling: {config}\")\n",
    "    result = profile_transformer(**config)\n",
    "    result.update(config)\n",
    "    results.append(result)\n",
    "    print(f\"  Params: {result['num_params']:,}, Time: {result['inference_time_ms']:.2f}ms\\n\")\n",
    "\n",
    "# Visualize scaling\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Parameters vs time\n",
    "ax1.scatter(df['num_params'] / 1e6, df['inference_time_ms'], s=100, alpha=0.7)\n",
    "ax1.set_xlabel('Number of Parameters (Millions)')\n",
    "ax1.set_ylabel('Inference Time (ms)')\n",
    "ax1.set_title('Model Size vs Inference Time')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Sequence length vs time (for same model size)\n",
    "same_model = df[(df['d_model'] == 128) & (df['num_heads'] == 8) & (df['num_layers'] == 4)]\n",
    "ax2.plot(same_model['seq_len'], same_model['inference_time_ms'], marker='o', linewidth=2)\n",
    "ax2.set_xlabel('Sequence Length')\n",
    "ax2.set_ylabel('Inference Time (ms)')\n",
    "ax2.set_title('Sequence Length vs Inference Time (d=128, h=8, L=4)')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120f945d",
   "metadata": {},
   "source": [
    "## Further Practice\n",
    "\n",
    "1. **Decoder**: Implement a transformer decoder with causal masking\n",
    "2. **Pre-LN vs Post-LN**: Compare pre-layer norm and post-layer norm architectures\n",
    "3. **Learned Positional Encodings**: Replace sinusoidal encodings with learned embeddings\n",
    "4. **Relative Positional Encodings**: Implement relative positional attention (T5-style)\n",
    "5. **Flash Attention**: Explore efficient attention implementations for long sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df030794",
   "metadata": {},
   "source": [
    "## Deliverables\n",
    "\n",
    "- [ ] Implement and visualize positional encodings\n",
    "- [ ] Build transformer encoder layer from scratch\n",
    "- [ ] Stack multiple layers into full transformer encoder\n",
    "- [ ] Train transformer on classification task\n",
    "- [ ] Profile computational costs (params, time, memory)\n",
    "- [ ] Compare transformer with RNN/LSTM baselines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99bcf3c",
   "metadata": {},
   "source": [
    "## Recommended Resources\n",
    "\n",
    "- [Attention Is All You Need](https://arxiv.org/abs/1706.03762) (original paper)\n",
    "- [The Annotated Transformer](http://nlp.seas.harvard.edu/2018/04/03/attention.html) (line-by-line implementation)\n",
    "- [Illustrated Guide to Transformers](https://jalammar.github.io/illustrated-transformer/)\n",
    "- [Formal Algorithms for Transformers](https://arxiv.org/abs/2207.09238) (comprehensive reference)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
