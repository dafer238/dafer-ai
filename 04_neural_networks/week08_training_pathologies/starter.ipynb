{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a89a799",
   "metadata": {},
   "source": [
    "# Week 08 — Training Pathologies\n",
    "\n",
    "This notebook explores common failure modes and their fixes. You'll:\n",
    "- Diagnose vanishing/exploding gradients\n",
    "- Compare activation functions and their effects\n",
    "- Apply fixes: initialization, normalization, gradient clipping\n",
    "- Track and visualize gradient flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0063ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "print(\"Libraries imported!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eff6978",
   "metadata": {},
   "source": [
    "## 1. Track Gradient Norms Across Layers\n",
    "\n",
    "Build a deep network and monitor gradient magnitudes to detect vanishing/exploding gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3600fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep network for gradient tracking\n",
    "class DeepNet(nn.Module):\n",
    "    def __init__(self, input_size=10, hidden_size=50, n_layers=10, activation='sigmoid'):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        # First layer\n",
    "        self.layers.append(nn.Linear(input_size, hidden_size))\n",
    "        \n",
    "        # Hidden layers\n",
    "        for _ in range(n_layers - 2):\n",
    "            self.layers.append(nn.Linear(hidden_size, hidden_size))\n",
    "        \n",
    "        # Output layer\n",
    "        self.layers.append(nn.Linear(hidden_size, 1))\n",
    "        \n",
    "        # Activation\n",
    "        if activation == 'sigmoid':\n",
    "            self.activation = nn.Sigmoid()\n",
    "        elif activation == 'tanh':\n",
    "            self.activation = nn.Tanh()\n",
    "        elif activation == 'relu':\n",
    "            self.activation = nn.ReLU()\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown activation: {activation}\")\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for i, layer in enumerate(self.layers[:-1]):\n",
    "            x = self.activation(layer(x))\n",
    "        x = self.layers[-1](x)  # No activation on output\n",
    "        return x\n",
    "\n",
    "# Function to compute gradient norms\n",
    "def compute_gradient_norms(model, loss):\n",
    "    \"\"\"Compute L2 norm of gradients for each layer\"\"\"\n",
    "    norms = []\n",
    "    for i, layer in enumerate(model.layers):\n",
    "        if hasattr(layer, 'weight') and layer.weight.grad is not None:\n",
    "            norm = layer.weight.grad.norm().item()\n",
    "            norms.append(norm)\n",
    "        else:\n",
    "            norms.append(0.0)\n",
    "    return norms\n",
    "\n",
    "# Create toy data\n",
    "X = torch.randn(100, 10)\n",
    "y = torch.randn(100, 1)\n",
    "\n",
    "# Test different activations\n",
    "activations = ['sigmoid', 'tanh', 'relu']\n",
    "gradient_norms = {}\n",
    "\n",
    "for act in activations:\n",
    "    model = DeepNet(input_size=10, hidden_size=50, n_layers=10, activation=act)\n",
    "    \n",
    "    # Forward and backward\n",
    "    outputs = model(X)\n",
    "    loss = nn.MSELoss()(outputs, y)\n",
    "    loss.backward()\n",
    "    \n",
    "    # Get gradient norms\n",
    "    norms = compute_gradient_norms(model, loss)\n",
    "    gradient_norms[act] = norms\n",
    "    \n",
    "    print(f\"{act:8s} - Gradient norms: {norms[:5]} ... {norms[-2:]}\")\n",
    "\n",
    "# Plot gradient norms\n",
    "plt.figure(figsize=(12, 5))\n",
    "for act, norms in gradient_norms.items():\n",
    "    plt.plot(range(len(norms)), norms, 'o-', label=act, linewidth=2, markersize=6)\n",
    "\n",
    "plt.xlabel('Layer Index')\n",
    "plt.ylabel('Gradient L2 Norm')\n",
    "plt.title('Gradient Norms Across Layers (10-layer deep network)')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.yscale('log')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n→ Sigmoid/Tanh show vanishing gradients in early layers!\")\n",
    "print(\"→ ReLU maintains better gradient flow.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6471f4c8",
   "metadata": {},
   "source": [
    "## 2. Activation Function Comparison\n",
    "\n",
    "Train networks with different activations and compare learning dynamics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab9fc53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train_model(model, X, y, n_epochs=100, lr=0.01):\n",
    "    \"\"\"Train model and return loss history\"\"\"\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "    criterion = nn.MSELoss()\n",
    "    losses = []\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X)\n",
    "        loss = criterion(outputs, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "    \n",
    "    return losses\n",
    "\n",
    "# Generate larger dataset\n",
    "torch.manual_seed(42)\n",
    "X_train = torch.randn(500, 10)\n",
    "y_train = torch.randn(500, 1)\n",
    "\n",
    "# Compare activations\n",
    "results = {}\n",
    "for act in ['sigmoid', 'tanh', 'relu']:\n",
    "    print(f\"Training with {act}...\")\n",
    "    model = DeepNet(input_size=10, hidden_size=50, n_layers=6, activation=act)\n",
    "    losses = train_model(model, X_train, y_train, n_epochs=200, lr=0.01)\n",
    "    results[act] = losses\n",
    "    print(f\"  Final loss: {losses[-1]:.6f}\")\n",
    "\n",
    "# Plot training curves\n",
    "plt.figure(figsize=(10, 5))\n",
    "for act, losses in results.items():\n",
    "    plt.plot(losses, label=act, linewidth=2)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Dynamics: Activation Function Comparison')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12d4672",
   "metadata": {},
   "source": [
    "## 3. Initialization Strategies\n",
    "\n",
    "Compare Xavier/Glorot and He initialization to solve gradient problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6f9c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply different initializations\n",
    "def initialize_model(model, init_type='xavier'):\n",
    "    \"\"\"Initialize model weights\"\"\"\n",
    "    for layer in model.layers:\n",
    "        if hasattr(layer, 'weight'):\n",
    "            if init_type == 'xavier':\n",
    "                nn.init.xavier_uniform_(layer.weight)\n",
    "            elif init_type == 'he':\n",
    "                nn.init.kaiming_uniform_(layer.weight, nonlinearity='relu')\n",
    "            elif init_type == 'small':\n",
    "                nn.init.normal_(layer.weight, mean=0.0, std=0.01)\n",
    "            \n",
    "            if hasattr(layer, 'bias') and layer.bias is not None:\n",
    "                nn.init.zeros_(layer.bias)\n",
    "\n",
    "# Test different initializations with ReLU\n",
    "init_types = ['small', 'xavier', 'he']\n",
    "init_results = {}\n",
    "\n",
    "for init_type in init_types:\n",
    "    print(f\"Training with {init_type} initialization...\")\n",
    "    model = DeepNet(input_size=10, hidden_size=50, n_layers=8, activation='relu')\n",
    "    initialize_model(model, init_type)\n",
    "    losses = train_model(model, X_train, y_train, n_epochs=200, lr=0.01)\n",
    "    init_results[init_type] = losses\n",
    "    print(f\"  Final loss: {losses[-1]:.6f}\")\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 5))\n",
    "for init_type, losses in init_results.items():\n",
    "    plt.plot(losses, label=init_type, linewidth=2)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Initialization Comparison (ReLU activation)')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.yscale('log')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n→ He initialization works best with ReLU!\")\n",
    "print(\"→ Xavier works better with tanh/sigmoid.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a6f09c",
   "metadata": {},
   "source": [
    "## 4. Batch Normalization\n",
    "\n",
    "Add BatchNorm to stabilize training and improve gradient flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cefd22e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network with BatchNorm\n",
    "class DeepNetWithBatchNorm(nn.Module):\n",
    "    def __init__(self, input_size=10, hidden_size=50, n_layers=10):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.batch_norms = nn.ModuleList()\n",
    "        \n",
    "        # First layer\n",
    "        self.layers.append(nn.Linear(input_size, hidden_size))\n",
    "        self.batch_norms.append(nn.BatchNorm1d(hidden_size))\n",
    "        \n",
    "        # Hidden layers\n",
    "        for _ in range(n_layers - 2):\n",
    "            self.layers.append(nn.Linear(hidden_size, hidden_size))\n",
    "            self.batch_norms.append(nn.BatchNorm1d(hidden_size))\n",
    "        \n",
    "        # Output layer\n",
    "        self.layers.append(nn.Linear(hidden_size, 1))\n",
    "        self.activation = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for i in range(len(self.layers) - 1):\n",
    "            x = self.layers[i](x)\n",
    "            x = self.batch_norms[i](x)\n",
    "            x = self.activation(x)\n",
    "        x = self.layers[-1](x)\n",
    "        return x\n",
    "\n",
    "# Compare with and without BatchNorm\n",
    "print(\"Training WITHOUT BatchNorm...\")\n",
    "model_no_bn = DeepNet(input_size=10, hidden_size=50, n_layers=10, activation='relu')\n",
    "losses_no_bn = train_model(model_no_bn, X_train, y_train, n_epochs=200, lr=0.01)\n",
    "\n",
    "print(\"Training WITH BatchNorm...\")\n",
    "model_bn = DeepNetWithBatchNorm(input_size=10, hidden_size=50, n_layers=10)\n",
    "losses_bn = train_model(model_bn, X_train, y_train, n_epochs=200, lr=0.01)\n",
    "\n",
    "# Plot comparison\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(losses_no_bn, label='Without BatchNorm', linewidth=2)\n",
    "plt.plot(losses_bn, label='With BatchNorm', linewidth=2)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Effect of Batch Normalization (10-layer network)')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.yscale('log')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nFinal loss WITHOUT BatchNorm: {losses_no_bn[-1]:.6f}\")\n",
    "print(f\"Final loss WITH BatchNorm: {losses_bn[-1]:.6f}\")\n",
    "print(\"\\n→ BatchNorm significantly stabilizes deep network training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ea58e1",
   "metadata": {},
   "source": [
    "## 5. Gradient Clipping\n",
    "\n",
    "Use gradient clipping to prevent exploding gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e99b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training with gradient clipping\n",
    "def train_with_clipping(model, X, y, n_epochs=100, lr=0.01, clip_value=None):\n",
    "    \"\"\"Train with optional gradient clipping\"\"\"\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "    criterion = nn.MSELoss()\n",
    "    losses = []\n",
    "    grad_norms = []\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X)\n",
    "        loss = criterion(outputs, y)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Compute total gradient norm\n",
    "        total_norm = 0.0\n",
    "        for p in model.parameters():\n",
    "            if p.grad is not None:\n",
    "                total_norm += p.grad.data.norm(2).item() ** 2\n",
    "        total_norm = total_norm ** 0.5\n",
    "        grad_norms.append(total_norm)\n",
    "        \n",
    "        # Clip gradients\n",
    "        if clip_value is not None:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip_value)\n",
    "        \n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "    \n",
    "    return losses, grad_norms\n",
    "\n",
    "# Create a problem that might have exploding gradients (high LR)\n",
    "print(\"Training WITHOUT gradient clipping...\")\n",
    "model_no_clip = DeepNet(input_size=10, hidden_size=50, n_layers=8, activation='relu')\n",
    "losses_no_clip, grads_no_clip = train_with_clipping(model_no_clip, X_train, y_train, \n",
    "                                                     n_epochs=100, lr=0.1, clip_value=None)\n",
    "\n",
    "print(\"Training WITH gradient clipping...\")\n",
    "model_clip = DeepNet(input_size=10, hidden_size=50, n_layers=8, activation='relu')\n",
    "losses_clip, grads_clip = train_with_clipping(model_clip, X_train, y_train, \n",
    "                                              n_epochs=100, lr=0.1, clip_value=1.0)\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].plot(losses_no_clip, label='No clipping', linewidth=2)\n",
    "axes[0].plot(losses_clip, label='With clipping (max=1.0)', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "axes[0].set_yscale('log')\n",
    "\n",
    "axes[1].plot(grads_no_clip, label='No clipping', linewidth=2)\n",
    "axes[1].plot(grads_clip, label='With clipping (max=1.0)', linewidth=2)\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Gradient Norm')\n",
    "axes[1].set_title('Gradient Magnitude')\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "axes[1].set_yscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n→ Gradient clipping prevents explosions and stabilizes training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c3704a",
   "metadata": {},
   "source": [
    "## Exercises for Further Practice\n",
    "\n",
    "1. **Layer Normalization**: Implement and compare LayerNorm to BatchNorm\n",
    "2. **Residual Connections**: Add skip connections to improve gradient flow\n",
    "3. **Different Depths**: Test very deep networks (20+ layers) with and without fixes\n",
    "4. **Learning Rate Analysis**: Study how LR interacts with initialization and normalization\n",
    "5. **Real Data**: Apply these techniques to MNIST or CIFAR-10\n",
    "\n",
    "## Deliverables Checklist\n",
    "\n",
    "- [ ] Gradient norm tracking across layers with visualizations\n",
    "- [ ] Activation function comparison experiments\n",
    "- [ ] Initialization and BatchNorm experiments\n",
    "- [ ] Gradient clipping demonstration\n",
    "- [ ] Short write-up on which fixes work best for which pathologies\n",
    "\n",
    "## Recommended Resources\n",
    "\n",
    "- Glorot & Bengio (2010): \"Understanding the difficulty of training deep feedforward neural networks\"\n",
    "- Ioffe & Szegedy (2015): \"Batch Normalization\"\n",
    "- He et al. (2015): \"Delving Deep into Rectifiers\" (He initialization)\n",
    "- PyTorch documentation on initialization and normalization"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
