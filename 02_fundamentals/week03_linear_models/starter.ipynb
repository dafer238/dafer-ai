{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea2d748b",
   "metadata": {},
   "source": [
    "# Week 03 â€” Classical ML Foundations\n",
    "\n",
    "This notebook covers linear and logistic regression, connecting statistical foundations to ML practice. You'll:\n",
    "- Implement closed-form solutions (normal equations) and iterative methods\n",
    "- Understand bias-variance tradeoff empirically\n",
    "- Build and evaluate classical models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf8b1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(42)\n",
    "print(\"Libraries imported!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d9aadd",
   "metadata": {},
   "source": [
    "## 1. Linear Regression: Closed-Form Solution\n",
    "\n",
    "Derive and implement the normal equations for linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f62bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic linear data\n",
    "n_samples = 100\n",
    "X = np.random.randn(n_samples, 1) * 2\n",
    "y_true = 3 * X.squeeze() + 2\n",
    "y = y_true + np.random.randn(n_samples) * 0.5\n",
    "\n",
    "# Add bias term (column of ones)\n",
    "X_with_bias = np.hstack([np.ones((n_samples, 1)), X])\n",
    "\n",
    "# Closed-form solution: w = (X^T X)^{-1} X^T y\n",
    "def normal_equations(X, y):\n",
    "    \"\"\"Solve linear regression using normal equations\"\"\"\n",
    "    XTX = X.T @ X\n",
    "    XTy = X.T @ y\n",
    "    w = np.linalg.solve(XTX, XTy)\n",
    "    return w\n",
    "\n",
    "# Fit model\n",
    "w = normal_equations(X_with_bias, y)\n",
    "print(f\"Fitted parameters: w0={w[0]:.3f}, w1={w[1]:.3f}\")\n",
    "print(f\"True parameters: w0=2.000, w1=3.000\")\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.scatter(X, y, alpha=0.6, label='Data')\n",
    "X_line = np.linspace(X.min(), X.max(), 100).reshape(-1, 1)\n",
    "X_line_bias = np.hstack([np.ones((100, 1)), X_line])\n",
    "y_pred = X_line_bias @ w\n",
    "plt.plot(X_line, y_pred, 'r-', linewidth=2, label='Fitted line')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Linear Regression: Normal Equations')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136ccac5",
   "metadata": {},
   "source": [
    "## 2. Compare to scikit-learn\n",
    "\n",
    "Verify our implementation matches sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c880619c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit with sklearn\n",
    "sklearn_model = LinearRegression()\n",
    "sklearn_model.fit(X, y)\n",
    "\n",
    "print(\"Our implementation:\")\n",
    "print(f\"  Intercept: {w[0]:.6f}, Slope: {w[1]:.6f}\")\n",
    "print(\"\\nsklearn:\")\n",
    "print(f\"  Intercept: {sklearn_model.intercept_:.6f}, Slope: {sklearn_model.coef_[0]:.6f}\")\n",
    "print(\"\\nMatch:\", np.allclose([w[0], w[1]], [sklearn_model.intercept_, sklearn_model.coef_[0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7b3bf7",
   "metadata": {},
   "source": [
    "## 3. Logistic Regression\n",
    "\n",
    "Implement and visualize logistic regression for binary classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777abdef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate binary classification data\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "X_clf, y_clf = make_classification(n_samples=200, n_features=2, n_redundant=0,\n",
    "                                   n_informative=2, n_clusters_per_class=1,\n",
    "                                   class_sep=1.5, random_state=42)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_clf, y_clf, test_size=0.3, random_state=42)\n",
    "\n",
    "# Fit logistic regression\n",
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "train_acc = log_reg.score(X_train, y_train)\n",
    "test_acc = log_reg.score(X_test, y_test)\n",
    "print(f\"Train accuracy: {train_acc:.3f}\")\n",
    "print(f\"Test accuracy: {test_acc:.3f}\")\n",
    "\n",
    "# Visualize decision boundary\n",
    "def plot_decision_boundary(model, X, y):\n",
    "    h = 0.02\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    plt.contourf(xx, yy, Z, alpha=0.3, cmap='RdYlBu')\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap='RdYlBu', edgecolors='k', s=50)\n",
    "    plt.xlabel('Feature 1')\n",
    "    plt.ylabel('Feature 2')\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plot_decision_boundary(log_reg, X_train, y_train)\n",
    "plt.title('Training Set')\n",
    "plt.subplot(1, 2, 2)\n",
    "plot_decision_boundary(log_reg, X_test, y_test)\n",
    "plt.title('Test Set')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243e2c7d",
   "metadata": {},
   "source": [
    "## 4. Bias-Variance Tradeoff\n",
    "\n",
    "Explore the bias-variance tradeoff by varying model complexity using polynomial features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219e0991",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate non-linear data\n",
    "np.random.seed(42)\n",
    "X_poly = np.linspace(0, 1, 30).reshape(-1, 1)\n",
    "y_poly = np.sin(2 * np.pi * X_poly).ravel() + np.random.randn(30) * 0.1\n",
    "\n",
    "# Split data\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(X_poly, y_poly, test_size=0.3, random_state=42)\n",
    "\n",
    "# Try different polynomial degrees\n",
    "degrees = [1, 2, 3, 5, 10, 15]\n",
    "train_errors = []\n",
    "test_errors = []\n",
    "\n",
    "for degree in degrees:\n",
    "    poly = PolynomialFeatures(degree=degree)\n",
    "    X_tr_poly = poly.fit_transform(X_tr)\n",
    "    X_te_poly = poly.transform(X_te)\n",
    "    \n",
    "    model = LinearRegression()\n",
    "    model.fit(X_tr_poly, y_tr)\n",
    "    \n",
    "    train_pred = model.predict(X_tr_poly)\n",
    "    test_pred = model.predict(X_te_poly)\n",
    "    \n",
    "    train_errors.append(mean_squared_error(y_tr, train_pred))\n",
    "    test_errors.append(mean_squared_error(y_te, test_pred))\n",
    "\n",
    "# Plot bias-variance tradeoff\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(degrees, train_errors, 'o-', label='Training Error', linewidth=2)\n",
    "plt.plot(degrees, test_errors, 's-', label='Test Error', linewidth=2)\n",
    "plt.xlabel('Polynomial Degree')\n",
    "plt.ylabel('Mean Squared Error')\n",
    "plt.title('Bias-Variance Tradeoff')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nOptimal degree minimizing test error:\", degrees[np.argmin(test_errors)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620c3e6b",
   "metadata": {},
   "source": [
    "## Exercises for Further Practice\n",
    "\n",
    "1. **Regularized Regression**: Implement Ridge regression from scratch using the modified normal equations\n",
    "2. **Gradient-Based Fit**: Implement linear regression using gradient descent and compare to closed-form\n",
    "3. **Multi-class Classification**: Use LogisticRegression with multi-class data (iris dataset)\n",
    "4. **Cross-Validation**: Implement k-fold CV to select polynomial degree\n",
    "5. **Real Dataset**: Apply linear/logistic models to a UCI dataset of your choice\n",
    "\n",
    "## Deliverables Checklist\n",
    "\n",
    "- [ ] Normal equations implementation and comparison to sklearn\n",
    "- [ ] Logistic regression with decision boundary visualization\n",
    "- [ ] Bias-variance experiment with plots\n",
    "- [ ] Mini-project applying models to real dataset\n",
    "\n",
    "## Recommended Resources\n",
    "\n",
    "- Bishop, \"Pattern Recognition and Machine Learning\" (Chapter 3)\n",
    "- scikit-learn documentation on linear models\n",
    "- Stanford CS229 lecture notes on supervised learning"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
