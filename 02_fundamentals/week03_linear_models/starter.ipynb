{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea2d748b",
   "metadata": {},
   "source": [
    "# Week 03 — Classical ML Foundations\n",
    "\n",
    "This notebook covers linear and logistic regression, connecting statistical foundations to ML practice. You'll:\n",
    "- Implement closed-form solutions (normal equations) and iterative methods\n",
    "- Understand bias-variance tradeoff empirically\n",
    "- Build and evaluate classical models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf8b1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(42)\n",
    "print(\"Libraries imported!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1221b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, pickle\n",
    "\n",
    "CACHE_DIR = \"cache_week03\"\n",
    "os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "\n",
    "def save_result(key, obj):\n",
    "    with open(os.path.join(CACHE_DIR, f\"{key}.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(obj, f)\n",
    "\n",
    "def load_result(key):\n",
    "    path = os.path.join(CACHE_DIR, f\"{key}.pkl\")\n",
    "    if os.path.exists(path):\n",
    "        with open(path, \"rb\") as f:\n",
    "            return pickle.load(f)\n",
    "    return None\n",
    "\n",
    "def cached(key, compute_fn):\n",
    "    result = load_result(key)\n",
    "    if result is not None:\n",
    "        print(f\"[cache] loaded '{key}'\")\n",
    "        return result\n",
    "    print(f\"[cache] computing '{key}'...\")\n",
    "    result = compute_fn()\n",
    "    save_result(key, result)\n",
    "    return result\n",
    "\n",
    "print(\"Cache utilities ready. Results will be stored in:\", CACHE_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d9aadd",
   "metadata": {},
   "source": [
    "## 1. Linear Regression: Closed-Form Solution\n",
    "\n",
    "Derive and implement the normal equations for linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f62bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic linear data\n",
    "n_samples = 100\n",
    "X = np.random.randn(n_samples, 1) * 2\n",
    "y_true = 3 * X.squeeze() + 2\n",
    "y = y_true + np.random.randn(n_samples) * 0.5\n",
    "\n",
    "# Add bias term (column of ones)\n",
    "X_with_bias = np.hstack([np.ones((n_samples, 1)), X])\n",
    "\n",
    "# Closed-form solution: w = (X^T X)^{-1} X^T y\n",
    "def normal_equations(X, y):\n",
    "    \"\"\"Solve linear regression using normal equations\"\"\"\n",
    "    XTX = X.T @ X\n",
    "    XTy = X.T @ y\n",
    "    w = np.linalg.solve(XTX, XTy)\n",
    "    return w\n",
    "\n",
    "# Fit model\n",
    "w = normal_equations(X_with_bias, y)\n",
    "print(f\"Fitted parameters: w0={w[0]:.3f}, w1={w[1]:.3f}\")\n",
    "print(f\"True parameters: w0=2.000, w1=3.000\")\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.scatter(X, y, alpha=0.6, label='Data')\n",
    "X_line = np.linspace(X.min(), X.max(), 100).reshape(-1, 1)\n",
    "X_line_bias = np.hstack([np.ones((100, 1)), X_line])\n",
    "y_pred = X_line_bias @ w\n",
    "plt.plot(X_line, y_pred, 'r-', linewidth=2, label='Fitted line')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Linear Regression: Normal Equations')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136ccac5",
   "metadata": {},
   "source": [
    "## 2. Compare to scikit-learn\n",
    "\n",
    "Verify our implementation matches sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c880619c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit with sklearn\n",
    "sklearn_model = LinearRegression()\n",
    "sklearn_model.fit(X, y)\n",
    "\n",
    "print(\"Our implementation:\")\n",
    "print(f\"  Intercept: {w[0]:.6f}, Slope: {w[1]:.6f}\")\n",
    "print(\"\\nsklearn:\")\n",
    "print(f\"  Intercept: {sklearn_model.intercept_:.6f}, Slope: {sklearn_model.coef_[0]:.6f}\")\n",
    "print(\"\\nMatch:\", np.allclose([w[0], w[1]], [sklearn_model.intercept_, sklearn_model.coef_[0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7b3bf7",
   "metadata": {},
   "source": [
    "## 3. Logistic Regression\n",
    "\n",
    "Implement and visualize logistic regression for binary classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777abdef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "X_clf, y_clf = make_classification(n_samples=200, n_features=2, n_redundant=0,\n",
    "                                   n_informative=2, n_clusters_per_class=1,\n",
    "                                   class_sep=1.5, random_state=42)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_clf, y_clf, test_size=0.3, random_state=42)\n",
    "\n",
    "def _fit_logistic():\n",
    "    model = LogisticRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    return model\n",
    "\n",
    "log_reg = cached(\"logistic_regression_clf_sep1.5\", _fit_logistic)\n",
    "\n",
    "train_acc = log_reg.score(X_train, y_train)\n",
    "test_acc  = log_reg.score(X_test,  y_test)\n",
    "print(f\"Train accuracy: {train_acc:.3f}\")\n",
    "print(f\"Test accuracy:  {test_acc:.3f}\")\n",
    "\n",
    "def plot_decision_boundary(model, X, y):\n",
    "    h = 0.02\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "    plt.contourf(xx, yy, Z, alpha=0.3, cmap='RdYlBu')\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap='RdYlBu', edgecolors='k', s=50)\n",
    "    plt.xlabel('Feature 1'); plt.ylabel('Feature 2')\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1); plot_decision_boundary(log_reg, X_train, y_train); plt.title('Training Set')\n",
    "plt.subplot(1, 2, 2); plot_decision_boundary(log_reg, X_test,  y_test);  plt.title('Test Set')\n",
    "plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243e2c7d",
   "metadata": {},
   "source": [
    "## 4. Bias-Variance Tradeoff\n",
    "\n",
    "Explore the bias-variance tradeoff by varying model complexity using polynomial features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219e0991",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "np.random.seed(42)\n",
    "X_poly = np.linspace(0, 1, 30).reshape(-1, 1)\n",
    "y_poly = np.sin(2 * np.pi * X_poly).ravel() + np.random.randn(30) * 0.1\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(X_poly, y_poly, test_size=0.3, random_state=42)\n",
    "\n",
    "degrees = [1, 2, 3, 5, 10, 15]\n",
    "\n",
    "def _compute_bias_variance():\n",
    "    train_errors, test_errors = [], []\n",
    "    for degree in degrees:\n",
    "        poly = PolynomialFeatures(degree=degree)\n",
    "        X_tr_poly = poly.fit_transform(X_tr)\n",
    "        X_te_poly = poly.transform(X_te)\n",
    "        model = LinearRegression()\n",
    "        model.fit(X_tr_poly, y_tr)\n",
    "        train_errors.append(mean_squared_error(y_tr, model.predict(X_tr_poly)))\n",
    "        test_errors.append(mean_squared_error(y_te, model.predict(X_te_poly)))\n",
    "    return train_errors, test_errors\n",
    "\n",
    "train_errors, test_errors = cached(\"bias_variance_poly_degrees\", _compute_bias_variance)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(degrees, train_errors, 'o-', label='Training Error', linewidth=2)\n",
    "plt.plot(degrees, test_errors,  's-', label='Test Error',     linewidth=2)\n",
    "plt.xlabel('Polynomial Degree'); plt.ylabel('Mean Squared Error')\n",
    "plt.title('Bias-Variance Tradeoff'); plt.legend(); plt.grid(alpha=0.3)\n",
    "plt.show()\n",
    "print(\"\\nOptimal degree minimizing test error:\", degrees[np.argmin(test_errors)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620c3e6b",
   "metadata": {},
   "source": [
    "## Exercises for Further Practice\n",
    "\n",
    "1. **Regularized Regression**: Implement Ridge regression from scratch using the modified normal equations\n",
    "2. **Gradient-Based Fit**: Implement linear regression using gradient descent and compare to closed-form\n",
    "3. **Multi-class Classification**: Use LogisticRegression with multi-class data (iris dataset)\n",
    "4. **Cross-Validation**: Implement k-fold CV to select polynomial degree\n",
    "5. **Real Dataset**: Apply linear/logistic models to a UCI dataset of your choice\n",
    "\n",
    "## Deliverables Checklist\n",
    "\n",
    "- [ ] Normal equations implementation and comparison to sklearn\n",
    "- [ ] Logistic regression with decision boundary visualization\n",
    "- [ ] Bias-variance experiment with plots\n",
    "- [ ] Mini-project applying models to real dataset\n",
    "\n",
    "## Recommended Resources\n",
    "\n",
    "- Bishop, \"Pattern Recognition and Machine Learning\" (Chapter 3)\n",
    "- scikit-learn documentation on linear models\n",
    "- Stanford CS229 lecture notes on supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0efe52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# EXERCISE 1 — Ridge Regression from Scratch\n",
    "# Goal: derive the Ridge closed-form solution: w = (X^T X + λI)^{-1} X^T y.\n",
    "# Implement it, then compare its coefficients to sklearn Ridge(alpha=1.0).\n",
    "# Sweep λ in [0.001, 0.01, 0.1, 1, 10] and plot coefficient norm vs λ.\n",
    "# Expected insight: larger λ shrinks all weights toward zero (L2 regularization).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa29cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# EXERCISE 2 — Gradient-Descent Linear Regression\n",
    "# Goal: implement linear regression via GD (not the normal equations).\n",
    "# Use MSE loss and its gradient. Run for 500 steps with lr=0.01.\n",
    "# Plot loss curve and overlay the GD fit line vs the closed-form one.\n",
    "# Expected insight: GD converges to the same solution as normal equations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e5a081",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# EXERCISE 3 — Multi-Class Logistic Regression (Iris dataset)\n",
    "# Goal: load sklearn's Iris dataset (3 classes). Fit LogisticRegression(max_iter=200).\n",
    "# Report per-class precision/recall and draw a confusion matrix.\n",
    "# Use cached(\"logistic_regression_iris\", ...) to save the fitted model.\n",
    "# Expected insight: the \"setosa\" class is perfectly separable; the other two overlap.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6594482f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# EXERCISE 5 — Real Dataset Mini-Project\n",
    "# Goal: pick any tabular dataset (e.g. sklearn's California Housing or Diabetes).\n",
    "# Split 80/20. Fit LinearRegression, Ridge, and LogisticRegression (if classification).\n",
    "# Report R² (regression) or accuracy (classification) for train and test.\n",
    "# Deliverable: one paragraph interpreting the results and any over/underfitting signs.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
