{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c414c8cb",
   "metadata": {},
   "source": [
    "# Week 02 â€” Advanced Optimization\n",
    "\n",
    "This notebook explores advanced optimizers (Momentum, RMSProp, Adam) and learning rate schedules. You'll:\n",
    "- Implement and compare modern optimizers\n",
    "- Experiment with LR schedules (step, cosine, warmup)\n",
    "- Run LR range tests and hyperparameter sweeps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8ebbfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(42)\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae960f3",
   "metadata": {},
   "source": [
    "## 1. Implement Advanced Optimizers\n",
    "\n",
    "We'll implement Momentum, RMSProp, and Adam from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0784257f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer:\n",
    "    \"\"\"Base optimizer class\"\"\"\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "    \n",
    "    def step(self, params, grads):\n",
    "        raise NotImplementedError\n",
    "\n",
    "class MomentumOptimizer(Optimizer):\n",
    "    \"\"\"SGD with Momentum\"\"\"\n",
    "    def __init__(self, lr=0.01, momentum=0.9):\n",
    "        super().__init__(lr)\n",
    "        self.momentum = momentum\n",
    "        self.velocity = None\n",
    "    \n",
    "    def step(self, params, grads):\n",
    "        if self.velocity is None:\n",
    "            self.velocity = np.zeros_like(params)\n",
    "        \n",
    "        self.velocity = self.momentum * self.velocity - self.lr * grads\n",
    "        params += self.velocity\n",
    "        return params\n",
    "\n",
    "class RMSPropOptimizer(Optimizer):\n",
    "    \"\"\"RMSProp optimizer\"\"\"\n",
    "    def __init__(self, lr=0.01, beta=0.9, eps=1e-8):\n",
    "        super().__init__(lr)\n",
    "        self.beta = beta\n",
    "        self.eps = eps\n",
    "        self.cache = None\n",
    "    \n",
    "    def step(self, params, grads):\n",
    "        if self.cache is None:\n",
    "            self.cache = np.zeros_like(params)\n",
    "        \n",
    "        self.cache = self.beta * self.cache + (1 - self.beta) * (grads ** 2)\n",
    "        params -= self.lr * grads / (np.sqrt(self.cache) + self.eps)\n",
    "        return params\n",
    "\n",
    "class AdamOptimizer(Optimizer):\n",
    "    \"\"\"Adam optimizer with bias correction\"\"\"\n",
    "    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999, eps=1e-8):\n",
    "        super().__init__(lr)\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.eps = eps\n",
    "        self.m = None\n",
    "        self.v = None\n",
    "        self.t = 0\n",
    "    \n",
    "    def step(self, params, grads):\n",
    "        if self.m is None:\n",
    "            self.m = np.zeros_like(params)\n",
    "            self.v = np.zeros_like(params)\n",
    "        \n",
    "        self.t += 1\n",
    "        \n",
    "        # Update biased first and second moments\n",
    "        self.m = self.beta1 * self.m + (1 - self.beta1) * grads\n",
    "        self.v = self.beta2 * self.v + (1 - self.beta2) * (grads ** 2)\n",
    "        \n",
    "        # Bias correction\n",
    "        m_hat = self.m / (1 - self.beta1 ** self.t)\n",
    "        v_hat = self.v / (1 - self.beta2 ** self.t)\n",
    "        \n",
    "        # Update parameters\n",
    "        params -= self.lr * m_hat / (np.sqrt(v_hat) + self.eps)\n",
    "        return params\n",
    "\n",
    "print(\"Optimizers implemented: Momentum, RMSProp, Adam\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578ff802",
   "metadata": {},
   "source": [
    "## 2. Compare Optimizers on a Test Problem\n",
    "\n",
    "Let's compare all optimizers on a simple 2D quadratic loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c52dcee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple 2D quadratic loss function\n",
    "def loss_fn(params):\n",
    "    x, y = params\n",
    "    return x**2 + 5*y**2\n",
    "\n",
    "def grad_fn(params):\n",
    "    x, y = params\n",
    "    return np.array([2*x, 10*y])\n",
    "\n",
    "# Compare optimizers\n",
    "optimizers = {\n",
    "    'Momentum': MomentumOptimizer(lr=0.1, momentum=0.9),\n",
    "    'RMSProp': RMSPropOptimizer(lr=0.1, beta=0.9),\n",
    "    'Adam': AdamOptimizer(lr=0.1, beta1=0.9, beta2=0.999)\n",
    "}\n",
    "\n",
    "trajectories = {}\n",
    "n_steps = 100\n",
    "initial_params = np.array([3.0, -2.0])\n",
    "\n",
    "for name, opt in optimizers.items():\n",
    "    params = initial_params.copy()\n",
    "    trajectory = [params.copy()]\n",
    "    \n",
    "    for _ in range(n_steps):\n",
    "        grads = grad_fn(params)\n",
    "        params = opt.step(params, grads)\n",
    "        trajectory.append(params.copy())\n",
    "    \n",
    "    trajectories[name] = np.array(trajectory)\n",
    "\n",
    "# Plot trajectories\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Create contour plot\n",
    "x_range = np.linspace(-4, 4, 100)\n",
    "y_range = np.linspace(-3, 3, 100)\n",
    "X, Y = np.meshgrid(x_range, y_range)\n",
    "Z = X**2 + 5*Y**2\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.contour(X, Y, Z, levels=20, cmap='viridis', alpha=0.5)\n",
    "for name, traj in trajectories.items():\n",
    "    plt.plot(traj[:, 0], traj[:, 1], '-o', markersize=2, label=name, alpha=0.7)\n",
    "plt.plot(initial_params[0], initial_params[1], 'ko', markersize=10, label='Start')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Optimizer Trajectories')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "# Plot loss curves\n",
    "plt.subplot(1, 2, 2)\n",
    "for name, traj in trajectories.items():\n",
    "    losses = [loss_fn(p) for p in traj]\n",
    "    plt.semilogy(losses, label=name, linewidth=2)\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Loss (log scale)')\n",
    "plt.title('Convergence Comparison')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print final losses\n",
    "for name, traj in trajectories.items():\n",
    "    final_loss = loss_fn(traj[-1])\n",
    "    print(f\"{name:12s} final loss: {final_loss:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557bcca0",
   "metadata": {},
   "source": [
    "## 3. Learning Rate Schedules\n",
    "\n",
    "Experiment with different LR schedules: step decay, cosine annealing, and warmup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74cf3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate schedulers\n",
    "def step_decay(epoch, lr_initial, drop_rate=0.5, epochs_drop=10):\n",
    "    \"\"\"Step decay LR schedule\"\"\"\n",
    "    return lr_initial * (drop_rate ** (epoch // epochs_drop))\n",
    "\n",
    "def cosine_annealing(epoch, lr_initial, T_max):\n",
    "    \"\"\"Cosine annealing LR schedule\"\"\"\n",
    "    return lr_initial * 0.5 * (1 + np.cos(np.pi * epoch / T_max))\n",
    "\n",
    "def warmup_cosine(epoch, lr_initial, warmup_epochs=5, T_max=50):\n",
    "    \"\"\"Warmup + cosine annealing\"\"\"\n",
    "    if epoch < warmup_epochs:\n",
    "        return lr_initial * (epoch + 1) / warmup_epochs\n",
    "    else:\n",
    "        return cosine_annealing(epoch - warmup_epochs, lr_initial, T_max - warmup_epochs)\n",
    "\n",
    "# Visualize schedules\n",
    "epochs = np.arange(0, 50)\n",
    "lr_initial = 0.1\n",
    "\n",
    "schedules = {\n",
    "    'Constant': [lr_initial] * len(epochs),\n",
    "    'Step Decay': [step_decay(e, lr_initial) for e in epochs],\n",
    "    'Cosine': [cosine_annealing(e, lr_initial, T_max=50) for e in epochs],\n",
    "    'Warmup+Cosine': [warmup_cosine(e, lr_initial) for e in epochs]\n",
    "}\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "for name, schedule in schedules.items():\n",
    "    plt.plot(epochs, schedule, linewidth=2, label=name)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.title('Learning Rate Schedules')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae3752e",
   "metadata": {},
   "source": [
    "## Exercises for Further Practice\n",
    "\n",
    "1. **LR Range Test**: Implement Leslie Smith's LR range test to find optimal LR\n",
    "2. **Hyperparameter Sensitivity**: Grid search over beta1/beta2 for Adam and plot effects\n",
    "3. **Non-Convex Losses**: Test optimizers on Rosenbrock function or other challenging landscapes\n",
    "4. **Adaptive vs Fixed LR**: Compare Adam with fixed LR vs Adam with cosine schedule\n",
    "5. **Implement AdaGrad**: Add AdaGrad optimizer and compare to Adam/RMSProp\n",
    "\n",
    "## Deliverables Checklist\n",
    "\n",
    "- [ ] Optimizer implementations (Momentum, RMSProp, Adam)\n",
    "- [ ] Comparison plots on at least two problems (convex + non-convex)\n",
    "- [ ] LR schedule experiments with visualizations  \n",
    "- [ ] Short write-up explaining which optimizer works best for which problem type\n",
    "\n",
    "## Recommended Resources\n",
    "\n",
    "- Kingma & Ba (2014): \"Adam: A Method for Stochastic Optimization\"\n",
    "- Ruder (2016): \"An overview of gradient descent optimization algorithms\"\n",
    "- Smith (2018): \"A disciplined approach to neural network hyper-parameters\""
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
