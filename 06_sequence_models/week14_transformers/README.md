Week 14 â€” Transformers

Overview
Study the transformer architecture, positional encodings, multi-head attention, and practical scaling considerations.

Study
- Transformer blocks (self-attention, feed-forward), positional encodings
- Multi-head attention and parallelization

Practical libraries & tools
- PyTorch / HuggingFace Transformers for reference implementations

Datasets & examples
- Small language modeling tasks (toy text) or time-series forecasting examples

Exercises
1) Build a minimal transformer encoder (single layer) and train on a toy task.

2) Experiment with positional encodings (sinusoidal vs learned).

3) Profile memory and compute for different sequence lengths and head counts.

Reading
- "Attention Is All You Need" and accessible walkthrough blogs; HuggingFace tutorials.

Deliverable
- Notebook implementing a minimal transformer, experiments with positional encodings, and a short profiling report.
