{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f52c3a6a",
   "metadata": {},
   "source": [
    "# Week 07 — Neural Networks From Scratch\n",
    "\n",
    "This notebook implements neural networks from first principles to deeply understand forward/backprop. You'll:\n",
    "- Build a fully connected neural network in NumPy\n",
    "- Implement backpropagation manually\n",
    "- Perform gradient checking\n",
    "- Experiment with initializations and activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47ca8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_moons, make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(42)\n",
    "print(\"Libraries imported!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7841505",
   "metadata": {},
   "source": [
    "## 1. Build a Fully Connected Neural Network\n",
    "\n",
    "Implement forward pass, loss, backward pass, and training loop from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb66a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation functions\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_grad(x):\n",
    "    return (x > 0).astype(float)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n",
    "\n",
    "def sigmoid_grad(x):\n",
    "    s = sigmoid(x)\n",
    "    return s * (1 - s)\n",
    "\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "\n",
    "# Simple 2-layer neural network\n",
    "class TwoLayerNet:\n",
    "    def __init__(self, input_size, hidden_size, output_size, init_std=0.01):\n",
    "        \"\"\"Initialize network parameters\"\"\"\n",
    "        self.W1 = np.random.randn(input_size, hidden_size) * init_std\n",
    "        self.b1 = np.zeros((1, hidden_size))\n",
    "        self.W2 = np.random.randn(hidden_size, output_size) * init_std\n",
    "        self.b2 = np.zeros((1, output_size))\n",
    "        \n",
    "        # Cache for backprop\n",
    "        self.cache = {}\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"Forward pass\"\"\"\n",
    "        # Layer 1\n",
    "        z1 = X @ self.W1 + self.b1\n",
    "        a1 = relu(z1)\n",
    "        \n",
    "        # Layer 2\n",
    "        z2 = a1 @ self.W2 + self.b2\n",
    "        a2 = softmax(z2)\n",
    "        \n",
    "        # Cache for backward pass\n",
    "        self.cache = {'X': X, 'z1': z1, 'a1': a1, 'z2': z2, 'a2': a2}\n",
    "        return a2\n",
    "    \n",
    "    def backward(self, y_true):\n",
    "        \"\"\"Backward pass (backpropagation)\"\"\"\n",
    "        m = y_true.shape[0]\n",
    "        X = self.cache['X']\n",
    "        a1 = self.cache['a1']\n",
    "        a2 = self.cache['a2']\n",
    "        z1 = self.cache['z1']\n",
    "        \n",
    "        # Output layer gradient\n",
    "        dz2 = a2 - y_true  # Softmax + cross-entropy gradient\n",
    "        dW2 = a1.T @ dz2 / m\n",
    "        db2 = np.sum(dz2, axis=0, keepdims=True) / m\n",
    "        \n",
    "        # Hidden layer gradient\n",
    "        da1 = dz2 @ self.W2.T\n",
    "        dz1 = da1 * relu_grad(z1)\n",
    "        dW1 = X.T @ dz1 / m\n",
    "        db1 = np.sum(dz1, axis=0, keepdims=True) / m\n",
    "        \n",
    "        return {'dW1': dW1, 'db1': db1, 'dW2': dW2, 'db2': db2}\n",
    "    \n",
    "    def compute_loss(self, y_pred, y_true):\n",
    "        \"\"\"Cross-entropy loss\"\"\"\n",
    "        m = y_true.shape[0]\n",
    "        log_probs = -np.log(y_pred[range(m), y_true.argmax(axis=1)] + 1e-8)\n",
    "        return np.mean(log_probs)\n",
    "    \n",
    "    def train_step(self, X, y, lr=0.01):\n",
    "        \"\"\"One training step\"\"\"\n",
    "        # Forward\n",
    "        y_pred = self.forward(X)\n",
    "        loss = self.compute_loss(y_pred, y)\n",
    "        \n",
    "        # Backward\n",
    "        grads = self.backward(y)\n",
    "        \n",
    "        # Update parameters\n",
    "        self.W1 -= lr * grads['dW1']\n",
    "        self.b1 -= lr * grads['db1']\n",
    "        self.W2 -= lr * grads['dW2']\n",
    "        self.b2 -= lr * grads['db2']\n",
    "        \n",
    "        return loss\n",
    "\n",
    "print(\"Neural network class implemented!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2d5791",
   "metadata": {},
   "source": [
    "## 2. Train on a Toy Dataset\n",
    "\n",
    "Test the network on a simple classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec99f0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate toy dataset\n",
    "X, y = make_moons(n_samples=300, noise=0.2, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# One-hot encode labels\n",
    "def one_hot(y, n_classes=2):\n",
    "    return np.eye(n_classes)[y]\n",
    "\n",
    "y_train_oh = one_hot(y_train)\n",
    "y_test_oh = one_hot(y_test)\n",
    "\n",
    "# Initialize and train network\n",
    "net = TwoLayerNet(input_size=2, hidden_size=10, output_size=2, init_std=0.1)\n",
    "\n",
    "n_epochs = 1000\n",
    "lr = 0.5\n",
    "losses = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    loss = net.train_step(X_train, y_train_oh, lr=lr)\n",
    "    losses.append(loss)\n",
    "    \n",
    "    if (epoch + 1) % 200 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{n_epochs}, Loss: {loss:.4f}\")\n",
    "\n",
    "# Evaluate\n",
    "y_pred_train = net.forward(X_train)\n",
    "y_pred_test = net.forward(X_test)\n",
    "train_acc = np.mean(y_pred_train.argmax(axis=1) == y_train_oh.argmax(axis=1))\n",
    "test_acc = np.mean(y_pred_test.argmax(axis=1) == y_test_oh.argmax(axis=1))\n",
    "\n",
    "print(f\"\\nFinal Training Accuracy: {train_acc*100:.2f}%\")\n",
    "print(f\"Final Test Accuracy: {test_acc*100:.2f}%\")\n",
    "\n",
    "# Plot loss curve\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(losses, linewidth=2)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Curve')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb98908e",
   "metadata": {},
   "source": [
    "## 3. Gradient Checking\n",
    "\n",
    "Validate backprop implementation using numerical gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62590ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_check(net, X, y, epsilon=1e-5):\n",
    "    \"\"\"\n",
    "    Numerical gradient checking\n",
    "    \n",
    "    Compare analytic gradients from backprop to numerical gradients\n",
    "    from finite differences\n",
    "    \"\"\"\n",
    "    # Get analytic gradients\n",
    "    y_pred = net.forward(X)\n",
    "    grads_analytic = net.backward(y)\n",
    "    \n",
    "    # Check each parameter\n",
    "    params = {'W1': net.W1, 'b1': net.b1, 'W2': net.W2, 'b2': net.b2}\n",
    "    \n",
    "    for param_name, param in params.items():\n",
    "        grad_analytic = grads_analytic[f'd{param_name}']\n",
    "        grad_numerical = np.zeros_like(param)\n",
    "        \n",
    "        # Compute numerical gradient for a subset of parameters\n",
    "        it = np.nditer(param, flags=['multi_index'], op_flags=['readwrite'])\n",
    "        count = 0\n",
    "        while not it.finished and count < 10:  # Check only first 10 for speed\n",
    "            ix = it.multi_index\n",
    "            old_value = param[ix]\n",
    "            \n",
    "            # f(x + eps)\n",
    "            param[ix] = old_value + epsilon\n",
    "            y_plus = net.forward(X)\n",
    "            loss_plus = net.compute_loss(y_plus, y)\n",
    "            \n",
    "            # f(x - eps)\n",
    "            param[ix] = old_value - epsilon\n",
    "            y_minus = net.forward(X)\n",
    "            loss_minus = net.compute_loss(y_minus, y)\n",
    "            \n",
    "            # Numerical gradient\n",
    "            grad_numerical[ix] = (loss_plus - loss_minus) / (2 * epsilon)\n",
    "            \n",
    "            # Restore\n",
    "            param[ix] = old_value\n",
    "            it.iternext()\n",
    "            count += 1\n",
    "        \n",
    "        # Compare\n",
    "        diff = np.linalg.norm(grad_analytic.ravel()[:10] - grad_numerical.ravel()[:10])\n",
    "        rel_error = diff / (np.linalg.norm(grad_analytic.ravel()[:10]) + np.linalg.norm(grad_numerical.ravel()[:10]) + 1e-8)\n",
    "        \n",
    "        print(f\"{param_name}: relative error = {rel_error:.2e}\")\n",
    "        if rel_error < 1e-5:\n",
    "            print(f\"  ✓ Gradient check passed!\")\n",
    "        elif rel_error < 1e-3:\n",
    "            print(f\"  ~ Gradient check OK (might be acceptable)\")\n",
    "        else:\n",
    "            print(f\"  ✗ Gradient check FAILED!\")\n",
    "\n",
    "# Run gradient check on small batch\n",
    "print(\"Running gradient check...\\n\")\n",
    "net_check = TwoLayerNet(input_size=2, hidden_size=5, output_size=2, init_std=0.1)\n",
    "X_small = X_train[:10]\n",
    "y_small = y_train_oh[:10]\n",
    "gradient_check(net_check, X_small, y_small)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d033e40",
   "metadata": {},
   "source": [
    "## 4. Initialization Experiments\n",
    "\n",
    "Compare different initialization strategies: small random, Xavier/Glorot, He."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e4fa63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different initializations\n",
    "def train_with_init(init_type, n_epochs=500):\n",
    "    \"\"\"Train network with specific initialization\"\"\"\n",
    "    if init_type == 'small':\n",
    "        net = TwoLayerNet(2, 10, 2, init_std=0.01)\n",
    "    elif init_type == 'xavier':\n",
    "        net = TwoLayerNet(2, 10, 2, init_std=np.sqrt(2.0 / (2 + 10)))\n",
    "    elif init_type == 'he':\n",
    "        net = TwoLayerNet(2, 10, 2, init_std=np.sqrt(2.0 / 2))\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown init_type: {init_type}\")\n",
    "    \n",
    "    losses = []\n",
    "    for epoch in range(n_epochs):\n",
    "        loss = net.train_step(X_train, y_train_oh, lr=0.5)\n",
    "        losses.append(loss)\n",
    "    \n",
    "    return losses, net\n",
    "\n",
    "# Compare initializations\n",
    "init_types = ['small', 'xavier', 'he']\n",
    "results = {}\n",
    "\n",
    "for init_type in init_types:\n",
    "    losses, net = train_with_init(init_type, n_epochs=500)\n",
    "    results[init_type] = losses\n",
    "    y_pred = net.forward(X_test)\n",
    "    acc = np.mean(y_pred.argmax(axis=1) == y_test_oh.argmax(axis=1))\n",
    "    print(f\"{init_type:10s} - Final test accuracy: {acc*100:.2f}%\")\n",
    "\n",
    "# Plot comparison\n",
    "plt.figure(figsize=(12, 5))\n",
    "for init_type, losses in results.items():\n",
    "    plt.plot(losses, label=init_type, linewidth=2)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss: Initialization Comparison')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da69ba1b",
   "metadata": {},
   "source": [
    "## Exercises for Further Practice\n",
    "\n",
    "1. **Add More Layers**: Extend the network to 3+ layers\n",
    "2. **Different Activations**: Implement tanh, leaky ReLU, and compare\n",
    "3. **Batch Processing**: Add mini-batch training support\n",
    "4. **Regularization**: Add L2 regularization to the loss and gradients\n",
    "5. **Visualization**: Plot decision boundaries for different network configurations\n",
    "\n",
    "## Deliverables Checklist\n",
    "\n",
    "- [ ] Fully connected neural network implementation from scratch\n",
    "- [ ] Gradient checking validation\n",
    "- [ ] Initialization experiments with analysis\n",
    "- [ ] Training plots and performance metrics\n",
    "\n",
    "## Recommended Resources\n",
    "\n",
    "- CS231n lecture notes on backpropagation\n",
    "- Andrew Ng's notes on neural networks\n",
    "- \"Neural Networks and Deep Learning\" by Michael Nielsen (online book)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
