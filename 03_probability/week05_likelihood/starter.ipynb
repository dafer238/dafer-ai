{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "482f003d",
   "metadata": {},
   "source": [
    "# Week 05 — Probability & Noise (Likelihood)\n",
    "\n",
    "This notebook ties loss-based ML to probabilistic modeling through maximum likelihood estimation. You'll:\n",
    "- Understand how likelihood assumptions lead to common loss functions\n",
    "- Implement MLE for different noise models\n",
    "- Explore robust alternatives to Gaussian assumptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aadb6b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from scipy.optimize import minimize\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(42)\n",
    "sns.set_style('whitegrid')\n",
    "print(\"Libraries imported!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f13a7b7c",
   "metadata": {},
   "source": [
    "## 1. Maximum Likelihood Estimation (MLE) for Gaussian Linear Regression\n",
    "\n",
    "Derive and implement MLE, showing the connection between negative log-likelihood and mean squared error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca24b2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data\n",
    "n_samples = 100\n",
    "X = np.random.randn(n_samples, 1) * 2\n",
    "w_true = np.array([2.0, 3.5])  # [intercept, slope]\n",
    "sigma_true = 1.0\n",
    "\n",
    "# Add bias column\n",
    "X_with_bias = np.hstack([np.ones((n_samples, 1)), X])\n",
    "y_true = X_with_bias @ w_true\n",
    "y = y_true + np.random.randn(n_samples) * sigma_true\n",
    "\n",
    "# Define negative log-likelihood for Gaussian noise\n",
    "def neg_log_likelihood_gaussian(params, X, y):\n",
    "    \"\"\"\n",
    "    Negative log-likelihood for linear regression with Gaussian noise\n",
    "    \n",
    "    L(w, sigma | X, y) = -sum(log N(y_i | X_i @ w, sigma^2))\n",
    "                       = (n/2) log(2*pi*sigma^2) + sum((y_i - X_i @ w)^2) / (2*sigma^2)\n",
    "    \"\"\"\n",
    "    n = len(y)\n",
    "    w = params[:-1]\n",
    "    sigma = params[-1]\n",
    "    \n",
    "    if sigma <= 0:\n",
    "        return 1e10  # Invalid sigma\n",
    "    \n",
    "    mu = X @ w\n",
    "    residuals = y - mu\n",
    "    \n",
    "    # Negative log-likelihood\n",
    "    nll = 0.5 * n * np.log(2 * np.pi * sigma**2) + np.sum(residuals**2) / (2 * sigma**2)\n",
    "    return nll\n",
    "\n",
    "# Optimize to find MLE\n",
    "initial_params = np.array([0.0, 0.0, 1.0])  # [w0, w1, sigma]\n",
    "result = minimize(neg_log_likelihood_gaussian, initial_params, args=(X_with_bias, y), method='L-BFGS-B')\n",
    "\n",
    "w_mle = result.x[:-1]\n",
    "sigma_mle = result.x[-1]\n",
    "\n",
    "print(\"True parameters: w0={:.2f}, w1={:.2f}, sigma={:.2f}\".format(*w_true, sigma_true))\n",
    "print(\"MLE parameters:  w0={:.2f}, w1={:.2f}, sigma={:.2f}\".format(*w_mle, sigma_mle))\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.scatter(X, y, alpha=0.6, label='Data')\n",
    "X_line = np.linspace(X.min(), X.max(), 100).reshape(-1, 1)\n",
    "X_line_bias = np.hstack([np.ones((100, 1)), X_line])\n",
    "y_pred_mle = X_line_bias @ w_mle\n",
    "y_pred_true = X_line_bias @ w_true\n",
    "\n",
    "plt.plot(X_line, y_pred_true, 'g--', linewidth=2, label='True model')\n",
    "plt.plot(X_line, y_pred_mle, 'r-', linewidth=2, label='MLE fit')\n",
    "plt.fill_between(X_line.ravel(), \n",
    "                 y_pred_mle - 2*sigma_mle, \n",
    "                 y_pred_mle + 2*sigma_mle, \n",
    "                 alpha=0.2, color='red', label='±2σ')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Maximum Likelihood Estimation')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413123a0",
   "metadata": {},
   "source": [
    "## 2. Connection: NLL and MSE\n",
    "\n",
    "Show that minimizing negative log-likelihood with Gaussian noise is equivalent to minimizing MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a18a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Gaussian noise with fixed sigma, NLL ∝ MSE\n",
    "# Let's verify this by computing both\n",
    "\n",
    "def mse_loss(w, X, y):\n",
    "    \"\"\"Mean Squared Error\"\"\"\n",
    "    mu = X @ w\n",
    "    return np.mean((y - mu)**2)\n",
    "\n",
    "# Grid search over w1 (fixing w0 and sigma)\n",
    "w0_fixed = w_mle[0]\n",
    "sigma_fixed = sigma_mle\n",
    "w1_range = np.linspace(0, 6, 100)\n",
    "\n",
    "nll_values = []\n",
    "mse_values = []\n",
    "\n",
    "for w1 in w1_range:\n",
    "    params = np.array([w0_fixed, w1, sigma_fixed])\n",
    "    nll = neg_log_likelihood_gaussian(params, X_with_bias, y)\n",
    "    nll_values.append(nll)\n",
    "    \n",
    "    mse = mse_loss(np.array([w0_fixed, w1]), X_with_bias, y)\n",
    "    mse_values.append(mse)\n",
    "\n",
    "# Normalize for comparison\n",
    "nll_normalized = (nll_values - np.min(nll_values)) / (np.max(nll_values) - np.min(nll_values))\n",
    "mse_normalized = (mse_values - np.min(mse_values)) / (np.max(mse_values) - np.min(mse_values))\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(w1_range, nll_normalized, 'b-', linewidth=2, label='Normalized NLL')\n",
    "plt.plot(w1_range, mse_normalized, 'r--', linewidth=2, label='Normalized MSE')\n",
    "plt.axvline(w_true[1], color='g', linestyle=':', linewidth=2, label='True w1')\n",
    "plt.axvline(w_mle[1], color='orange', linestyle=':', linewidth=2, label='MLE w1')\n",
    "plt.xlabel('w1 (slope parameter)')\n",
    "plt.ylabel('Normalized Loss')\n",
    "plt.title('Negative Log-Likelihood vs MSE\\n(Curves overlap when noise is Gaussian!)')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"Correlation between NLL and MSE:\", np.corrcoef(nll_values, mse_values)[0, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040d9602",
   "metadata": {},
   "source": [
    "## 3. Robust Regression: Laplace and Student-t Noise\n",
    "\n",
    "Explore robust alternatives to Gaussian noise that are less sensitive to outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4bdebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data with outliers\n",
    "np.random.seed(42)\n",
    "X_robust = np.random.randn(80, 1) * 2\n",
    "X_robust_bias = np.hstack([np.ones((80, 1)), X_robust])\n",
    "y_robust = X_robust_bias @ w_true + np.random.randn(80) * 0.5\n",
    "\n",
    "# Add outliers\n",
    "outlier_idx = [10, 25, 40, 55, 70]\n",
    "y_robust[outlier_idx] += np.random.choice([-5, 5], size=len(outlier_idx))\n",
    "\n",
    "# MLE with Laplace noise (L1 loss)\n",
    "def neg_log_likelihood_laplace(params, X, y):\n",
    "    \"\"\"NLL for Laplace noise: L1 loss\"\"\"\n",
    "    w = params[:-1]\n",
    "    b = params[-1]  # scale parameter\n",
    "    if b <= 0:\n",
    "        return 1e10\n",
    "    \n",
    "    mu = X @ w\n",
    "    residuals = np.abs(y - mu)\n",
    "    return len(y) * np.log(2 * b) + np.sum(residuals) / b\n",
    "\n",
    "# MLE with Student-t noise (robust to outliers)\n",
    "def neg_log_likelihood_t(params, X, y, df=3):\n",
    "    \"\"\"NLL for Student-t noise\"\"\"\n",
    "    w = params[:-1]\n",
    "    scale = params[-1]\n",
    "    if scale <= 0:\n",
    "        return 1e10\n",
    "    \n",
    "    mu = X @ w\n",
    "    nll = -np.sum(stats.t.logpdf((y - mu) / scale, df=df)) + len(y) * np.log(scale)\n",
    "    return nll\n",
    "\n",
    "# Fit all three models\n",
    "init = np.array([0.0, 0.0, 1.0])\n",
    "\n",
    "result_gaussian = minimize(neg_log_likelihood_gaussian, init, args=(X_robust_bias, y_robust), method='L-BFGS-B')\n",
    "result_laplace = minimize(neg_log_likelihood_laplace, init, args=(X_robust_bias, y_robust), method='L-BFGS-B')\n",
    "result_t = minimize(neg_log_likelihood_t, init, args=(X_robust_bias, y_robust), method='L-BFGS-B')\n",
    "\n",
    "# Plot comparison\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.scatter(X_robust, y_robust, alpha=0.6, s=50, label='Data')\n",
    "plt.scatter(X_robust[outlier_idx], y_robust[outlier_idx], color='red', s=100, \n",
    "            marker='x', linewidths=3, label='Outliers')\n",
    "\n",
    "X_line = np.linspace(X_robust.min(), X_robust.max(), 100).reshape(-1, 1)\n",
    "X_line_bias = np.hstack([np.ones((100, 1)), X_line])\n",
    "\n",
    "plt.plot(X_line, X_line_bias @ w_true, 'g--', linewidth=2, label='True model', alpha=0.7)\n",
    "plt.plot(X_line, X_line_bias @ result_gaussian.x[:-1], 'b-', linewidth=2, label='Gaussian MLE')\n",
    "plt.plot(X_line, X_line_bias @ result_laplace.x[:-1], 'orange', linewidth=2, label='Laplace MLE (L1)')\n",
    "plt.plot(X_line, X_line_bias @ result_t.x[:-1], 'purple', linewidth=2, label='Student-t MLE')\n",
    "\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Robust Regression: Gaussian vs Laplace vs Student-t Noise Models')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nModel Comparison:\")\n",
    "print(f\"Gaussian: w0={result_gaussian.x[0]:.2f}, w1={result_gaussian.x[1]:.2f}\")\n",
    "print(f\"Laplace:  w0={result_laplace.x[0]:.2f}, w1={result_laplace.x[1]:.2f}\")\n",
    "print(f\"Student-t: w0={result_t.x[0]:.2f}, w1={result_t.x[1]:.2f}\")\n",
    "print(f\"\\nTrue:     w0={w_true[0]:.2f}, w1={w_true[1]:.2f}\")\n",
    "print(\"\\n→ Laplace and Student-t are more robust to outliers!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60a00d7",
   "metadata": {},
   "source": [
    "## 4. Likelihood Surfaces and Multimodality\n",
    "\n",
    "Visualize likelihood as a function of parameters to understand optimization landscapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b302411d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a grid over w0 and w1\n",
    "w0_range = np.linspace(0, 4, 50)\n",
    "w1_range = np.linspace(1, 6, 50)\n",
    "W0, W1 = np.meshgrid(w0_range, w1_range)\n",
    "\n",
    "# Compute NLL for each combination\n",
    "NLL = np.zeros_like(W0)\n",
    "sigma_fixed = 1.0\n",
    "\n",
    "for i in range(len(w0_range)):\n",
    "    for j in range(len(w1_range)):\n",
    "        params = np.array([W0[j, i], W1[j, i], sigma_fixed])\n",
    "        NLL[j, i] = neg_log_likelihood_gaussian(params, X_with_bias, y)\n",
    "\n",
    "# Plot likelihood surface\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "contour = plt.contour(W0, W1, NLL, levels=30, cmap='viridis')\n",
    "plt.colorbar(contour, label='Negative Log-Likelihood')\n",
    "plt.plot(w_mle[0], w_mle[1], 'r*', markersize=20, label='MLE')\n",
    "plt.plot(w_true[0], w_true[1], 'g*', markersize=20, label='True')\n",
    "plt.xlabel('w0 (intercept)')\n",
    "plt.ylabel('w1 (slope)')\n",
    "plt.title('Likelihood Surface (Contours)')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "# 3D surface\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "ax = plt.subplot(1, 2, 2, projection='3d')\n",
    "surf = ax.plot_surface(W0, W1, NLL, cmap='viridis', alpha=0.6, edgecolor='none')\n",
    "ax.scatter([w_mle[0]], [w_mle[1]], [neg_log_likelihood_gaussian(np.concatenate([w_mle, [sigma_fixed]]), X_with_bias, y)],\n",
    "           color='red', s=100, label='MLE')\n",
    "ax.set_xlabel('w0')\n",
    "ax.set_ylabel('w1')\n",
    "ax.set_zlabel('NLL')\n",
    "ax.set_title('Likelihood Surface (3D)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"The likelihood surface is convex for Gaussian linear regression.\")\n",
    "print(\"This guarantees a unique global maximum (minimum NLL).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6deb8b",
   "metadata": {},
   "source": [
    "## Exercises for Further Practice\n",
    "\n",
    "1. **Poisson Regression**: Implement MLE for count data using Poisson likelihood\n",
    "2. **Bayesian Linear Regression**: Add priors and compute MAP estimates\n",
    "3. **Heteroscedastic Noise**: Model variance as a function of inputs\n",
    "4. **AIC/BIC**: Implement model selection using information criteria\n",
    "5. **Real Data**: Apply different noise models to UCI regression datasets\n",
    "\n",
    "## Deliverables Checklist\n",
    "\n",
    "- [ ] MLE derivation and implementation for Gaussian linear regression\n",
    "- [ ] Demonstration of NLL ≈ MSE connection\n",
    "- [ ] Robust regression comparison (Gaussian vs Laplace vs Student-t)\n",
    "- [ ] Likelihood surface visualization with interpretation\n",
    "\n",
    "## Recommended Resources\n",
    "\n",
    "- Murphy, \"Machine Learning: A Probabilistic Perspective\" (MLE chapter)\n",
    "- Bishop, \"Pattern Recognition and Machine Learning\" (Probability chapter)\n",
    "- SciPy documentation on probability distributions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
