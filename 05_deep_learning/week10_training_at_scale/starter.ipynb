{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "645fcc9f",
   "metadata": {},
   "source": [
    "# Week 10 — Efficient Training (Training at Scale)\n",
    "\n",
    "This notebook focuses on building efficient training pipelines. You'll:\n",
    "- Implement efficient DataLoaders with prefetching\n",
    "- Profile and optimize data loading bottlenecks\n",
    "- Use learning rate schedulers\n",
    "- Build robust checkpoint systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4350925f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from pathlib import Path\n",
    "%matplotlib inline\n",
    "\n",
    "torch.manual_seed(42)\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5acab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, pickle\n",
    "\n",
    "CACHE_DIR = \"cache_week10\"\n",
    "os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "\n",
    "def save_result(key, obj):\n",
    "    with open(os.path.join(CACHE_DIR, f\"{key}.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(obj, f)\n",
    "\n",
    "def load_result(key):\n",
    "    path = os.path.join(CACHE_DIR, f\"{key}.pkl\")\n",
    "    if os.path.exists(path):\n",
    "        with open(path, \"rb\") as f:\n",
    "            return pickle.load(f)\n",
    "    return None\n",
    "\n",
    "def cached(key, compute_fn):\n",
    "    result = load_result(key)\n",
    "    if result is not None:\n",
    "        print(f\"[cache] loaded '{key}'\")\n",
    "        return result\n",
    "    print(f\"[cache] computing '{key}'...\")\n",
    "    result = compute_fn()\n",
    "    save_result(key, result)\n",
    "    return result\n",
    "\n",
    "print(\"Cache utilities ready. Results will be stored in:\", CACHE_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b12c1d",
   "metadata": {},
   "source": [
    "## 1. Custom Dataset with Efficient Loading\n",
    "\n",
    "Implement a custom dataset with proper data loading patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8be3320",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom dataset with simulated I/O delay\n",
    "class SyntheticDataset(Dataset):\n",
    "    def __init__(self, n_samples=1000, n_features=100, delay_ms=0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            n_samples: Number of samples\n",
    "            n_features: Number of features\n",
    "            delay_ms: Simulated I/O delay in milliseconds\n",
    "        \"\"\"\n",
    "        self.n_samples = n_samples\n",
    "        self.n_features = n_features\n",
    "        self.delay_ms = delay_ms\n",
    "        \n",
    "        # Pre-generate data (in practice, you'd load from disk)\n",
    "        self.data = torch.randn(n_samples, n_features)\n",
    "        self.labels = torch.randint(0, 10, (n_samples,))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Simulate I/O delay\n",
    "        if self.delay_ms > 0:\n",
    "            time.sleep(self.delay_ms / 1000.0)\n",
    "        \n",
    "        return self.data[idx], self.labels[idx]\n",
    "\n",
    "# Create datasets with different delays\n",
    "dataset_fast = SyntheticDataset(n_samples=1000, delay_ms=0)\n",
    "dataset_slow = SyntheticDataset(n_samples=1000, delay_ms=1)\n",
    "\n",
    "print(f\"Dataset size: {len(dataset_fast)}\")\n",
    "sample_x, sample_y = dataset_fast[0]\n",
    "print(f\"Sample shape: {sample_x.shape}, label: {sample_y}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6026fae",
   "metadata": {},
   "source": [
    "## 2. DataLoader Performance Comparison\n",
    "\n",
    "Compare different DataLoader configurations and measure throughput."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720a085f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def benchmark_dataloader(dataset, batch_size=32, num_workers=0, pin_memory=False, n_batches=50):\n",
    "    import time\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, num_workers=num_workers,\n",
    "                        pin_memory=pin_memory, shuffle=True)\n",
    "    start = time.time()\n",
    "    for i, (batch_x, _) in enumerate(loader):\n",
    "        if i >= n_batches: break\n",
    "        _ = batch_x.mean()\n",
    "    elapsed = time.time() - start\n",
    "    return elapsed, n_batches / elapsed\n",
    "\n",
    "configs = [\n",
    "    {'num_workers': 0, 'pin_memory': False, 'name': '0 workers'},\n",
    "    {'num_workers': 2, 'pin_memory': False, 'name': '2 workers'},\n",
    "    {'num_workers': 4, 'pin_memory': False, 'name': '4 workers'},\n",
    "    {'num_workers': 4, 'pin_memory': True,  'name': '4 workers + pin_memory'},\n",
    "]\n",
    "\n",
    "def _run_benchmarks():\n",
    "    results = []\n",
    "    for cfg in configs:\n",
    "        elapsed, tput = benchmark_dataloader(dataset_slow, 32, cfg['num_workers'], cfg['pin_memory'])\n",
    "        results.append({'name': cfg['name'], 'elapsed': elapsed, 'throughput': tput})\n",
    "        print(f\"{cfg['name']:28s}: {elapsed:.2f}s ({tput:.1f} batches/s)\")\n",
    "    return results\n",
    "\n",
    "dataloader_results = cached(\"dataloader_benchmark_4configs\", _run_benchmarks)\n",
    "\n",
    "names      = [r['name']       for r in dataloader_results]\n",
    "throughput = [r['throughput'] for r in dataloader_results]\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.barh(names, throughput, color='skyblue')\n",
    "plt.xlabel('Throughput (batches/s)'); plt.title('DataLoader Performance Comparison')\n",
    "plt.grid(axis='x', alpha=0.3); plt.tight_layout(); plt.show()\n",
    "print(\"\\n→ Using multiple workers significantly improves throughput!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee4d822",
   "metadata": {},
   "source": [
    "## 3. Learning Rate Schedulers\n",
    "\n",
    "Experiment with different LR scheduling strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b9bbe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple model for demonstration\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(100, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 10)\n",
    ")\n",
    "\n",
    "# Create different schedulers\n",
    "def visualize_schedulers(initial_lr=0.1, n_epochs=100):\n",
    "    schedulers_to_test = {\n",
    "        'StepLR': optim.lr_scheduler.StepLR(\n",
    "            optim.SGD(model.parameters(), lr=initial_lr),\n",
    "            step_size=30,\n",
    "            gamma=0.1\n",
    "        ),\n",
    "        'ExponentialLR': optim.lr_scheduler.ExponentialLR(\n",
    "            optim.SGD(model.parameters(), lr=initial_lr),\n",
    "            gamma=0.95\n",
    "        ),\n",
    "        'CosineAnnealingLR': optim.lr_scheduler.CosineAnnealingLR(\n",
    "            optim.SGD(model.parameters(), lr=initial_lr),\n",
    "            T_max=n_epochs\n",
    "        ),\n",
    "        'ReduceLROnPlateau': optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optim.SGD(model.parameters(), lr=initial_lr),\n",
    "            mode='min',\n",
    "            factor=0.5,\n",
    "            patience=10\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    lr_histories = {name: [] for name in schedulers_to_test.keys()}\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        for name, scheduler in schedulers_to_test.items():\n",
    "            current_lr = scheduler.optimizer.param_groups[0]['lr']\n",
    "            lr_histories[name].append(current_lr)\n",
    "            \n",
    "            # Step scheduler\n",
    "            if name == 'ReduceLROnPlateau':\n",
    "                # Simulate a metric (e.g., loss)\n",
    "                fake_loss = 1.0 / (epoch + 1) + np.random.randn() * 0.01\n",
    "                scheduler.step(fake_loss)\n",
    "            else:\n",
    "                scheduler.step()\n",
    "    \n",
    "    return lr_histories\n",
    "\n",
    "# Visualize\n",
    "lr_histories = visualize_schedulers(initial_lr=0.1, n_epochs=100)\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "for name, lrs in lr_histories.items():\n",
    "    plt.plot(lrs, label=name, linewidth=2)\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.title('Learning Rate Schedulers Comparison')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.yscale('log')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b97322",
   "metadata": {},
   "source": [
    "## 4. Robust Checkpointing System\n",
    "\n",
    "Build a comprehensive checkpoint system with metadata and resumption support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f593333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpoint manager\n",
    "class CheckpointManager:\n",
    "    def __init__(self, checkpoint_dir='checkpoints', keep_last_n=3):\n",
    "        self.checkpoint_dir = Path(checkpoint_dir)\n",
    "        self.checkpoint_dir.mkdir(exist_ok=True)\n",
    "        self.keep_last_n = keep_last_n\n",
    "    \n",
    "    def save(self, model, optimizer, scheduler, epoch, metrics, is_best=False):\n",
    "        \"\"\"\n",
    "        Save checkpoint with metadata\n",
    "        \n",
    "        Args:\n",
    "            model: PyTorch model\n",
    "            optimizer: Optimizer state\n",
    "            scheduler: LR scheduler state\n",
    "            epoch: Current epoch\n",
    "            metrics: Dict of metrics (e.g., {'train_loss': 0.5, 'val_acc': 0.9})\n",
    "            is_best: Whether this is the best model so far\n",
    "        \"\"\"\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict() if scheduler else None,\n",
    "            'metrics': metrics,\n",
    "        }\n",
    "        \n",
    "        # Save regular checkpoint\n",
    "        checkpoint_path = self.checkpoint_dir / f'checkpoint_epoch_{epoch}.pth'\n",
    "        torch.save(checkpoint, checkpoint_path)\n",
    "        print(f\"Checkpoint saved: {checkpoint_path}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if is_best:\n",
    "            best_path = self.checkpoint_dir / 'best_model.pth'\n",
    "            torch.save(checkpoint, best_path)\n",
    "            print(f\"Best model saved: {best_path}\")\n",
    "        \n",
    "        # Clean up old checkpoints\n",
    "        self._cleanup_old_checkpoints()\n",
    "    \n",
    "    def load(self, checkpoint_path, model, optimizer=None, scheduler=None):\n",
    "        \"\"\"\n",
    "        Load checkpoint and restore state\n",
    "        \"\"\"\n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    "        \n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        if optimizer:\n",
    "            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        if scheduler and checkpoint['scheduler_state_dict']:\n",
    "            scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "        \n",
    "        return checkpoint['epoch'], checkpoint['metrics']\n",
    "    \n",
    "    def _cleanup_old_checkpoints(self):\n",
    "        \"\"\"Keep only the last N checkpoints\"\"\"\n",
    "        checkpoints = sorted(self.checkpoint_dir.glob('checkpoint_epoch_*.pth'))\n",
    "        if len(checkpoints) > self.keep_last_n:\n",
    "            for ckpt in checkpoints[:-self.keep_last_n]:\n",
    "                ckpt.unlink()\n",
    "                print(f\"Removed old checkpoint: {ckpt}\")\n",
    "\n",
    "# Example usage\n",
    "ckpt_manager = CheckpointManager(checkpoint_dir='./checkpoints', keep_last_n=3)\n",
    "\n",
    "# Simulate training and saving checkpoints\n",
    "model = nn.Linear(10, 2)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10)\n",
    "\n",
    "best_val_acc = 0.0\n",
    "for epoch in range(1, 6):\n",
    "    # Simulate training\n",
    "    train_loss = 1.0 / epoch + np.random.rand() * 0.1\n",
    "    val_acc = min(0.95, 0.5 + epoch * 0.1 + np.random.rand() * 0.05)\n",
    "    \n",
    "    metrics = {'train_loss': train_loss, 'val_acc': val_acc}\n",
    "    is_best = val_acc > best_val_acc\n",
    "    \n",
    "    if is_best:\n",
    "        best_val_acc = val_acc\n",
    "    \n",
    "    ckpt_manager.save(model, optimizer, scheduler, epoch, metrics, is_best=is_best)\n",
    "    scheduler.step()\n",
    "    print()\n",
    "\n",
    "# Load best model\n",
    "print(\"\\nLoading best model...\")\n",
    "best_epoch, best_metrics = ckpt_manager.load('./checkpoints/best_model.pth', model, optimizer, scheduler)\n",
    "print(f\"Loaded best model from epoch {best_epoch}\")\n",
    "print(f\"Metrics: {best_metrics}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab42cc69",
   "metadata": {},
   "source": [
    "## Exercises for Further Practice\n",
    "\n",
    "1. **Mixed Precision Training**: Use `torch.cuda.amp` for faster training\n",
    "2. **Gradient Accumulation**: Implement gradient accumulation for larger effective batch sizes\n",
    "3. **Profiling**: Use PyTorch profiler to identify bottlenecks\n",
    "4. **Distributed Training**: Explore `torch.nn.DataParallel` or `DistributedDataParallel`\n",
    "5. **Real Dataset**: Apply these techniques to a large dataset (e.g., ImageNet subset)\n",
    "\n",
    "## Deliverables Checklist\n",
    "\n",
    "- [ ] Custom dataset with efficient data loading\n",
    "- [ ] DataLoader performance benchmarks\n",
    "- [ ] Learning rate scheduler experiments\n",
    "- [ ] Robust checkpoint system implementation\n",
    "- [ ] Short report on throughput optimizations\n",
    "\n",
    "## Recommended Resources\n",
    "\n",
    "- PyTorch documentation on data loading and DataLoader\n",
    "- PyTorch performance tuning guide\n",
    "- Papers on distributed training and mixed precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083dae03",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# EXERCISE 1 — Gradient Accumulation\n",
    "# Goal: implement gradient accumulation over K=4 mini-batches, then call optimizer.step().\n",
    "# This simulates a batch size 4× larger without increasing memory.\n",
    "# Compare training curves with true batch_size=128 vs accum(batch=32, K=4).\n",
    "# Use cached(\"grad_accum_vs_large_batch_50epochs\", ...) to store both histories.\n",
    "# Expected insight: accumulation mimics large batches at the cost of throughput.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6dfc080",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# EXERCISE 2 — Warmup + Cosine Schedule from Scratch\n",
    "# Goal: implement a custom LR scheduler function (not using PyTorch schedulers):\n",
    "#   - Linear warmup for 10 epochs from lr=0 to lr=0.1\n",
    "#   - Cosine decay from epoch 10 to 100 back to lr=0\n",
    "# Plot the LR curve and use it in a training loop.\n",
    "# Expected insight: warmup avoids early instability when LR starts too high.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2824a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# EXERCISE 4 — Resume-from-Checkpoint Training\n",
    "# Goal: train a SimpleMLP for 25 epochs and save a checkpoint with CheckpointManager.\n",
    "# Simulate a \"crash\" by reinitialising the model. Load the checkpoint and resume\n",
    "# training for another 25 epochs. Confirm that the full 50-epoch loss curve is smooth.\n",
    "# Use cached(\"resume_training_50epochs\", ...) for the combined history.\n",
    "# Expected insight: robust checkpointing is essential for long training runs.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
