{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44e829cc",
   "metadata": {},
   "source": [
    "# Week 04 â€” Regularization & Validation\n",
    "\n",
    "This notebook covers techniques to control overfitting and select models robustly. You'll:\n",
    "- Understand and implement L1/L2 regularization\n",
    "- Master cross-validation strategies\n",
    "- Apply time-series aware validation techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8959935c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import Ridge, Lasso, RidgeCV, LassoCV, ElasticNet\n",
    "from sklearn.model_selection import cross_val_score, KFold, TimeSeriesSplit\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(42)\n",
    "print(\"Libraries imported!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06444be",
   "metadata": {},
   "source": [
    "## 1. Ridge vs Lasso Regularization\n",
    "\n",
    "Compare L2 (Ridge) and L1 (Lasso) regularization on synthetic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272ef31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data with correlated features\n",
    "n_samples = 100\n",
    "n_features = 50\n",
    "X, y = make_regression(n_samples=n_samples, n_features=n_features, \n",
    "                       n_informative=10, noise=10, random_state=42)\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Fit Ridge and Lasso\n",
    "alpha = 1.0\n",
    "ridge = Ridge(alpha=alpha)\n",
    "lasso = Lasso(alpha=alpha)\n",
    "\n",
    "ridge.fit(X_scaled, y)\n",
    "lasso.fit(X_scaled, y)\n",
    "\n",
    "# Compare coefficients\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.stem(ridge.coef_, linefmt='b-', markerfmt='bo', basefmt=' ')\n",
    "plt.title(f'Ridge Coefficients (L2)\\nNon-zero: {np.sum(np.abs(ridge.coef_) > 1e-3)}')\n",
    "plt.xlabel('Feature Index')\n",
    "plt.ylabel('Coefficient Value')\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.stem(lasso.coef_, linefmt='r-', markerfmt='ro', basefmt=' ')\n",
    "plt.title(f'Lasso Coefficients (L1)\\nNon-zero: {np.sum(np.abs(lasso.coef_) > 1e-3)}')\n",
    "plt.xlabel('Feature Index')\n",
    "plt.ylabel('Coefficient Value')\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.scatter(ridge.coef_, lasso.coef_, alpha=0.6)\n",
    "plt.xlabel('Ridge Coefficients')\n",
    "plt.ylabel('Lasso Coefficients')\n",
    "plt.title('Ridge vs Lasso')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.axhline(0, color='k', linestyle='--', linewidth=0.5)\n",
    "plt.axvline(0, color='k', linestyle='--', linewidth=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Ridge: {np.sum(np.abs(ridge.coef_) > 1e-3)} non-zero coefficients\")\n",
    "print(f\"Lasso: {np.sum(np.abs(lasso.coef_) > 1e-3)} non-zero coefficients (feature selection!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abab5ce4",
   "metadata": {},
   "source": [
    "## 2. Cross-Validation for Hyperparameter Selection\n",
    "\n",
    "Use k-fold cross-validation to select the regularization strength (alpha)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50dfdc44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define alpha range\n",
    "alphas = np.logspace(-4, 2, 50)\n",
    "\n",
    "# Use RidgeCV and LassoCV (built-in CV)\n",
    "ridge_cv = RidgeCV(alphas=alphas, cv=5)\n",
    "lasso_cv = LassoCV(alphas=alphas, cv=5, max_iter=10000)\n",
    "\n",
    "ridge_cv.fit(X_scaled, y)\n",
    "lasso_cv.fit(X_scaled, y)\n",
    "\n",
    "print(f\"Ridge optimal alpha: {ridge_cv.alpha_:.4f}\")\n",
    "print(f\"Lasso optimal alpha: {lasso_cv.alpha_:.4f}\")\n",
    "\n",
    "# Manual CV to visualize validation curves\n",
    "ridge_scores = []\n",
    "lasso_scores = []\n",
    "\n",
    "for alpha in alphas:\n",
    "    ridge = Ridge(alpha=alpha)\n",
    "    lasso = Lasso(alpha=alpha, max_iter=10000)\n",
    "    \n",
    "    ridge_score = cross_val_score(ridge, X_scaled, y, cv=5, scoring='neg_mean_squared_error').mean()\n",
    "    lasso_score = cross_val_score(lasso, X_scaled, y, cv=5, scoring='neg_mean_squared_error').mean()\n",
    "    \n",
    "    ridge_scores.append(-ridge_score)\n",
    "    lasso_scores.append(-lasso_score)\n",
    "\n",
    "# Plot validation curves\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.semilogx(alphas, ridge_scores, 'b-', linewidth=2, label='Ridge')\n",
    "plt.axvline(ridge_cv.alpha_, color='b', linestyle='--', label=f'Optimal: {ridge_cv.alpha_:.4f}')\n",
    "plt.xlabel('Alpha (regularization strength)')\n",
    "plt.ylabel('CV Mean Squared Error')\n",
    "plt.title('Ridge Validation Curve')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.semilogx(alphas, lasso_scores, 'r-', linewidth=2, label='Lasso')\n",
    "plt.axvline(lasso_cv.alpha_, color='r', linestyle='--', label=f'Optimal: {lasso_cv.alpha_:.4f}')\n",
    "plt.xlabel('Alpha (regularization strength)')\n",
    "plt.ylabel('CV Mean Squared Error')\n",
    "plt.title('Lasso Validation Curve')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c6dbf3",
   "metadata": {},
   "source": [
    "## 3. Time-Series Cross-Validation\n",
    "\n",
    "Implement walk-forward validation for time-series data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7142084c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic time-series data\n",
    "n_samples_ts = 200\n",
    "time = np.linspace(0, 10, n_samples_ts)\n",
    "trend = 0.5 * time\n",
    "seasonal = 2 * np.sin(2 * np.pi * time / 2)\n",
    "noise = np.random.randn(n_samples_ts) * 0.5\n",
    "y_ts = trend + seasonal + noise\n",
    "\n",
    "# Create lag features\n",
    "def create_lag_features(y, n_lags=5):\n",
    "    X = np.zeros((len(y) - n_lags, n_lags))\n",
    "    for i in range(n_lags):\n",
    "        X[:, i] = y[n_lags-i-1:len(y)-i-1]\n",
    "    return X, y[n_lags:]\n",
    "\n",
    "X_ts, y_ts_target = create_lag_features(y_ts, n_lags=5)\n",
    "\n",
    "# Time series CV\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "print(\"Time Series Cross-Validation Splits:\")\n",
    "for i, (train_idx, test_idx) in enumerate(tscv.split(X_ts)):\n",
    "    print(f\"Split {i+1}: Train size={len(train_idx)}, Test size={len(test_idx)}\")\n",
    "\n",
    "# Evaluate model with time-series CV\n",
    "model = Ridge(alpha=1.0)\n",
    "scores = cross_val_score(model, X_ts, y_ts_target, cv=tscv, scoring='neg_mean_squared_error')\n",
    "print(f\"\\nTime-series CV MSE scores: {-scores}\")\n",
    "print(f\"Mean CV MSE: {-scores.mean():.4f} +/- {scores.std():.4f}\")\n",
    "\n",
    "# Visualize splits\n",
    "plt.figure(figsize=(12, 6))\n",
    "for i, (train_idx, test_idx) in enumerate(tscv.split(X_ts)):\n",
    "    plt.subplot(tscv.n_splits, 1, i+1)\n",
    "    plt.plot(train_idx, np.ones(len(train_idx)) * i, 'b-', linewidth=10, label='Train' if i==0 else '')\n",
    "    plt.plot(test_idx, np.ones(len(test_idx)) * i, 'r-', linewidth=10, label='Test' if i==0 else '')\n",
    "    plt.ylabel(f'Split {i+1}')\n",
    "    plt.yticks([])\n",
    "    if i == 0:\n",
    "        plt.legend(loc='upper right')\n",
    "    if i < tscv.n_splits - 1:\n",
    "        plt.xticks([])\n",
    "\n",
    "plt.xlabel('Sample Index')\n",
    "plt.suptitle('Time Series Cross-Validation Splits (Walk-Forward)', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06a69ce",
   "metadata": {},
   "source": [
    "## 4. Elastic Net: Combining L1 and L2\n",
    "\n",
    "Explore Elastic Net which combines Ridge and Lasso regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3bbae0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elastic Net: combines L1 and L2\n",
    "# Loss = MSE + alpha * (l1_ratio * L1 + (1 - l1_ratio) * L2)\n",
    "\n",
    "l1_ratios = [0.1, 0.5, 0.9]  # 0 = Ridge, 1 = Lasso\n",
    "alpha = 0.1\n",
    "\n",
    "plt.figure(figsize=(15, 4))\n",
    "for i, l1_ratio in enumerate(l1_ratios):\n",
    "    elastic = ElasticNet(alpha=alpha, l1_ratio=l1_ratio, max_iter=10000)\n",
    "    elastic.fit(X_scaled, y)\n",
    "    \n",
    "    plt.subplot(1, 3, i+1)\n",
    "    plt.stem(elastic.coef_, linefmt='g-', markerfmt='go', basefmt=' ')\n",
    "    plt.title(f'Elastic Net (l1_ratio={l1_ratio})\\nNon-zero: {np.sum(np.abs(elastic.coef_) > 1e-3)}')\n",
    "    plt.xlabel('Feature Index')\n",
    "    plt.ylabel('Coefficient Value')\n",
    "    plt.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa82576",
   "metadata": {},
   "source": [
    "## Exercises for Further Practice\n",
    "\n",
    "1. **Implement Coordinate Descent**: Code Lasso using coordinate descent algorithm\n",
    "2. **Regularization Path**: Plot the full regularization path showing how coefficients change with alpha\n",
    "3. **Early Stopping**: Implement early stopping as a form of regularization\n",
    "4. **Dropout Simulation**: Simulate dropout on linear models\n",
    "5. **Real Financial Data**: Apply time-series CV to stock price prediction\n",
    "\n",
    "## Deliverables Checklist\n",
    "\n",
    "- [ ] Ridge vs Lasso comparison with feature selection demonstration\n",
    "- [ ] Cross-validation experiments with validation curves\n",
    "- [ ] Time-series CV implementation and visualization\n",
    "- [ ] Notebook with clear recommendations for choosing regularization methods\n",
    "\n",
    "## Recommended Resources\n",
    "\n",
    "- Hastie, Tibshirani, Friedman: \"Elements of Statistical Learning\" (Regularization chapter)\n",
    "- scikit-learn documentation: linear models and cross-validation\n",
    "- \"An Introduction to Statistical Learning\" (ISLR) Chapter on Ridge/Lasso"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
