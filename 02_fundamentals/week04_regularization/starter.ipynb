{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44e829cc",
   "metadata": {},
   "source": [
    "# Week 04 — Regularization & Validation\n",
    "\n",
    "This notebook covers techniques to control overfitting and select models robustly. You'll:\n",
    "- Understand and implement L1/L2 regularization\n",
    "- Master cross-validation strategies\n",
    "- Apply time-series aware validation techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8959935c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import Ridge, Lasso, RidgeCV, LassoCV, ElasticNet\n",
    "from sklearn.model_selection import cross_val_score, KFold, TimeSeriesSplit\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(42)\n",
    "print(\"Libraries imported!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cccf9d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, pickle\n",
    "\n",
    "CACHE_DIR = \"cache_week04\"\n",
    "os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "\n",
    "def save_result(key, obj):\n",
    "    with open(os.path.join(CACHE_DIR, f\"{key}.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(obj, f)\n",
    "\n",
    "def load_result(key):\n",
    "    path = os.path.join(CACHE_DIR, f\"{key}.pkl\")\n",
    "    if os.path.exists(path):\n",
    "        with open(path, \"rb\") as f:\n",
    "            return pickle.load(f)\n",
    "    return None\n",
    "\n",
    "def cached(key, compute_fn):\n",
    "    result = load_result(key)\n",
    "    if result is not None:\n",
    "        print(f\"[cache] loaded '{key}'\")\n",
    "        return result\n",
    "    print(f\"[cache] computing '{key}'...\")\n",
    "    result = compute_fn()\n",
    "    save_result(key, result)\n",
    "    return result\n",
    "\n",
    "print(\"Cache utilities ready. Results will be stored in:\", CACHE_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06444be",
   "metadata": {},
   "source": [
    "## 1. Ridge vs Lasso Regularization\n",
    "\n",
    "Compare L2 (Ridge) and L1 (Lasso) regularization on synthetic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272ef31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n_samples = 100\n",
    "n_features = 50\n",
    "X, y = make_regression(n_samples=n_samples, n_features=n_features,\n",
    "                       n_informative=10, noise=10, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "alpha = 1.0\n",
    "\n",
    "def _fit_ridge():\n",
    "    m = Ridge(alpha=alpha); m.fit(X_scaled, y); return m\n",
    "def _fit_lasso():\n",
    "    m = Lasso(alpha=alpha); m.fit(X_scaled, y); return m\n",
    "\n",
    "ridge = cached(\"ridge_alpha1.0\", _fit_ridge)\n",
    "lasso = cached(\"lasso_alpha1.0\", _fit_lasso)\n",
    "\n",
    "plt.figure(figsize=(14, 5))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.stem(ridge.coef_, linefmt='b-', markerfmt='bo', basefmt=' ')\n",
    "plt.title(f'Ridge Coefficients (L2)\\nNon-zero: {np.sum(np.abs(ridge.coef_) > 1e-3)}')\n",
    "plt.xlabel('Feature Index'); plt.ylabel('Coefficient Value'); plt.grid(alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.stem(lasso.coef_, linefmt='r-', markerfmt='ro', basefmt=' ')\n",
    "plt.title(f'Lasso Coefficients (L1)\\nNon-zero: {np.sum(np.abs(lasso.coef_) > 1e-3)}')\n",
    "plt.xlabel('Feature Index'); plt.ylabel('Coefficient Value'); plt.grid(alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.scatter(ridge.coef_, lasso.coef_, alpha=0.6)\n",
    "plt.xlabel('Ridge Coefficients'); plt.ylabel('Lasso Coefficients')\n",
    "plt.title('Ridge vs Lasso'); plt.grid(alpha=0.3)\n",
    "plt.axhline(0, color='k', linestyle='--', linewidth=0.5)\n",
    "plt.axvline(0, color='k', linestyle='--', linewidth=0.5)\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "print(f\"Ridge: {np.sum(np.abs(ridge.coef_) > 1e-3)} non-zero coefficients\")\n",
    "print(f\"Lasso: {np.sum(np.abs(lasso.coef_) > 1e-3)} non-zero coefficients (feature selection!)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abab5ce4",
   "metadata": {},
   "source": [
    "## 2. Cross-Validation for Hyperparameter Selection\n",
    "\n",
    "Use k-fold cross-validation to select the regularization strength (alpha)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50dfdc44",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "alphas = np.logspace(-4, 2, 50)\n",
    "\n",
    "def _cv_scores():\n",
    "    ridge_cv = RidgeCV(alphas=alphas, cv=5)\n",
    "    lasso_cv = LassoCV(alphas=alphas, cv=5, max_iter=10000)\n",
    "    ridge_cv.fit(X_scaled, y)\n",
    "    lasso_cv.fit(X_scaled, y)\n",
    "\n",
    "    r_scores = [-cross_val_score(Ridge(alpha=a), X_scaled, y, cv=5,\n",
    "                  scoring='neg_mean_squared_error').mean() for a in alphas]\n",
    "    l_scores = [-cross_val_score(Lasso(alpha=a, max_iter=10000), X_scaled, y, cv=5,\n",
    "                  scoring='neg_mean_squared_error').mean() for a in alphas]\n",
    "    return ridge_cv.alpha_, lasso_cv.alpha_, r_scores, l_scores\n",
    "\n",
    "ridge_alpha, lasso_alpha, ridge_scores, lasso_scores = cached(\n",
    "    \"ridge_lasso_cv_scores_50alphas\", _cv_scores)\n",
    "\n",
    "print(f\"Ridge optimal alpha: {ridge_alpha:.4f}\")\n",
    "print(f\"Lasso optimal alpha: {lasso_alpha:.4f}\")\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.semilogx(alphas, ridge_scores, 'b-', linewidth=2, label='Ridge')\n",
    "plt.axvline(ridge_alpha, color='b', linestyle='--', label=f'Optimal: {ridge_alpha:.4f}')\n",
    "plt.xlabel('Alpha'); plt.ylabel('CV MSE'); plt.title('Ridge Validation Curve')\n",
    "plt.legend(); plt.grid(alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.semilogx(alphas, lasso_scores, 'r-', linewidth=2, label='Lasso')\n",
    "plt.axvline(lasso_alpha, color='r', linestyle='--', label=f'Optimal: {lasso_alpha:.4f}')\n",
    "plt.xlabel('Alpha'); plt.ylabel('CV MSE'); plt.title('Lasso Validation Curve')\n",
    "plt.legend(); plt.grid(alpha=0.3)\n",
    "plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c6dbf3",
   "metadata": {},
   "source": [
    "## 3. Time-Series Cross-Validation\n",
    "\n",
    "Implement walk-forward validation for time-series data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7142084c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n_samples_ts = 200\n",
    "time = np.linspace(0, 10, n_samples_ts)\n",
    "y_ts = 0.5 * time + 2 * np.sin(2 * np.pi * time / 2) + np.random.randn(n_samples_ts) * 0.5\n",
    "\n",
    "def create_lag_features(y, n_lags=5):\n",
    "    X = np.zeros((len(y) - n_lags, n_lags))\n",
    "    for i in range(n_lags):\n",
    "        X[:, i] = y[n_lags-i-1:len(y)-i-1]\n",
    "    return X, y[n_lags:]\n",
    "\n",
    "X_ts, y_ts_target = create_lag_features(y_ts, n_lags=5)\n",
    "\n",
    "def _timeseries_cv():\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "    scores = cross_val_score(Ridge(alpha=1.0), X_ts, y_ts_target, cv=tscv,\n",
    "                             scoring='neg_mean_squared_error')\n",
    "    return scores\n",
    "\n",
    "scores = cached(\"timeseries_cv_ridge\", _timeseries_cv)\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "print(\"Time Series CV MSE scores:\", -scores)\n",
    "print(f\"Mean CV MSE: {-scores.mean():.4f} +/- {scores.std():.4f}\")\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "for i, (train_idx, test_idx) in enumerate(tscv.split(X_ts)):\n",
    "    plt.subplot(tscv.n_splits, 1, i+1)\n",
    "    plt.plot(train_idx, np.ones(len(train_idx)) * i, 'b-', linewidth=10,\n",
    "             label='Train' if i == 0 else '')\n",
    "    plt.plot(test_idx,  np.ones(len(test_idx))  * i, 'r-', linewidth=10,\n",
    "             label='Test'  if i == 0 else '')\n",
    "    plt.ylabel(f'Split {i+1}'); plt.yticks([])\n",
    "    if i == 0: plt.legend(loc='upper right')\n",
    "    if i < tscv.n_splits - 1: plt.xticks([])\n",
    "plt.xlabel('Sample Index')\n",
    "plt.suptitle('Time Series Cross-Validation Splits (Walk-Forward)', y=1.02)\n",
    "plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06a69ce",
   "metadata": {},
   "source": [
    "## 4. Elastic Net: Combining L1 and L2\n",
    "\n",
    "Explore Elastic Net which combines Ridge and Lasso regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3bbae0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "l1_ratios = [0.1, 0.5, 0.9]\n",
    "\n",
    "def _fit_elastic_nets():\n",
    "    models = {}\n",
    "    for l1_ratio in l1_ratios:\n",
    "        m = ElasticNet(alpha=0.1, l1_ratio=l1_ratio, max_iter=10000)\n",
    "        m.fit(X_scaled, y)\n",
    "        models[l1_ratio] = m.coef_.copy()\n",
    "    return models\n",
    "\n",
    "elastic_coefs = cached(\"elasticnet_l1ratios_0.1_0.5_0.9\", _fit_elastic_nets)\n",
    "\n",
    "plt.figure(figsize=(15, 4))\n",
    "for i, l1_ratio in enumerate(l1_ratios):\n",
    "    coef = elastic_coefs[l1_ratio]\n",
    "    plt.subplot(1, 3, i+1)\n",
    "    plt.stem(coef, linefmt='g-', markerfmt='go', basefmt=' ')\n",
    "    plt.title(f'Elastic Net (l1_ratio={l1_ratio})\\nNon-zero: {np.sum(np.abs(coef) > 1e-3)}')\n",
    "    plt.xlabel('Feature Index'); plt.ylabel('Coefficient Value'); plt.grid(alpha=0.3)\n",
    "plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa82576",
   "metadata": {},
   "source": [
    "## Exercises for Further Practice\n",
    "\n",
    "1. **Implement Coordinate Descent**: Code Lasso using coordinate descent algorithm\n",
    "2. **Regularization Path**: Plot the full regularization path showing how coefficients change with alpha\n",
    "3. **Early Stopping**: Implement early stopping as a form of regularization\n",
    "4. **Dropout Simulation**: Simulate dropout on linear models\n",
    "5. **Real Financial Data**: Apply time-series CV to stock price prediction\n",
    "\n",
    "## Deliverables Checklist\n",
    "\n",
    "- [ ] Ridge vs Lasso comparison with feature selection demonstration\n",
    "- [ ] Cross-validation experiments with validation curves\n",
    "- [ ] Time-series CV implementation and visualization\n",
    "- [ ] Notebook with clear recommendations for choosing regularization methods\n",
    "\n",
    "## Recommended Resources\n",
    "\n",
    "- Hastie, Tibshirani, Friedman: \"Elements of Statistical Learning\" (Regularization chapter)\n",
    "- scikit-learn documentation: linear models and cross-validation\n",
    "- \"An Introduction to Statistical Learning\" (ISLR) Chapter on Ridge/Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28347798",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# EXERCISE 1 — Regularization Path Visualization\n",
    "# Goal: sweep alpha from 1e-4 to 1e3 for both Ridge and Lasso.\n",
    "# For each alpha, record *all* 50 coefficients. Plot them as paths (α on x, coef value on y).\n",
    "# Use cached(\"ridge_path_50alphas\", ...) and cached(\"lasso_path_50alphas\", ...).\n",
    "# Expected insight: Ridge shrinks all, Lasso zeroes them out one by one as α grows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0ac6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# EXERCISE 3 — Early Stopping\n",
    "# Goal: implement a GD loop for linear regression that saves the model every 10 steps.\n",
    "# Split data 60/20/20 (train/val/test). Monitor validation MSE and stop when it starts\n",
    "# increasing. Plot train vs val MSE over steps.\n",
    "# Deliverable: report the step at which early stopping fires and the test MSE achieved.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
